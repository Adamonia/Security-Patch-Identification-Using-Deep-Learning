{"cells":[{"cell_type":"markdown","metadata":{"id":"5wxnat0TUADv"},"source":["\n","\n","```\n","# Article: https://arxiv.org/pdf/2108.03358\n","# Source Code (original authors): https://github.com/shuwang127/PatchRNN\n","# Demo Code (original authors): https://github.com/shuwang127/PatchRNN-demo\n","# Run Initialisation once, after starting a new Environment in Colab\n","# Initialisation requires 16 GB RAM and can take a few minutes.\n","```\n","\n","# Initialisation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NARLlgMQHPpw"},"source":["## Helpers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_TpeJltOLoFy","executionInfo":{"status":"ok","timestamp":1750533027172,"user_tz":-120,"elapsed":12,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["#STARTIGNORE - helper functions\n","import os\n","import requests\n","from pathlib import Path\n","\n","def CreateDirectory(path):\n","  if os.path.exists(path):\n","    return\n","  else:\n","    os.mkdir(path)\n","\n","def CreateFile(path, content, binary = False):\n","  if binary == True:\n","    with open(path, \"wb\") as f:\n","      f.write(content)\n","  else:\n","    with open(path, \"w\") as f:\n","      f.write(content)\n","\n","def ReadFile(path):\n","    with open(path, \"r\", errors=\"replace\") as f:\n","      return f.read()\n","\n","def Download(url, dest):\n","  # Download helper functions from Learn PyTorch repo (if not already downloaded)\n","  if Path(f\"{dest}\").is_file():\n","    print(f\"{dest} already exists, skipping download\")\n","  else:\n","    print(f\"Downloading {dest}\")\n","    request = requests.get(url)\n","    CreateFile(dest,request.content,True)\n","\n","def GetJsonFromURL(url,verbose=False):\n","  resp = requests.get(url)\n","  if verbose==True:\n","    print(resp)\n","    print(resp.text)\n","  return resp.json()\n","\n","def GetTextFromURL(url,verbose=False):\n","  resp = requests.get(url)\n","  if verbose==True:\n","    print(resp)\n","    print(resp.text)\n","  return resp.text\n","\n","def ReleaseGPUMemory():\n","  import gc\n","  import torch\n","  for i in range(5):\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    try:\n","      del model\n","    except:\n","      pass\n","\n","def pretty_size(size):\n","    \"\"\"Pretty prints a torch.Size object\"\"\"\n","    assert(isinstance(size, torch.Size))\n","    return \" × \".join(map(str, size))\n","\n","def dump_tensors(gpu_only=True):\n","    \"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n","    import gc\n","    total_size = 0\n","    for obj in gc.get_objects():\n","        try:\n","            if torch.is_tensor(obj):\n","                if not gpu_only or obj.is_cuda:\n","                    print(\"%s:%s%s %s\" % (type(obj).__name__,\n","                                          \" GPU\" if obj.is_cuda else \"\",\n","                                          \" pinned\" if obj.is_pinned() else \"\",\n","                                          pretty_size(obj.size())))\n","                    total_size += obj.numel()\n","            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n","                if not gpu_only or obj.is_cuda:\n","                    print(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__,\n","                                                   type(obj.data).__name__,\n","                                                   \" GPU\" if obj.is_cuda else \"\",\n","                                                   \" pinned\" if obj.data.is_pinned() else \"\",\n","                                                   \" grad\" if obj.requires_grad else \"\",\n","                                                   \" volatile\" if obj.volatile else \"\",\n","                                                   pretty_size(obj.data.size())))\n","                    total_size += obj.data.numel()\n","        except Exception as e:\n","            pass\n","    print(\"Total size:\", total_size)\n","#ENDIGNORE"]},{"cell_type":"markdown","metadata":{"id":"0p1f0cF1UhD0"},"source":["\n","\n","```\n","# Run this once, after starting a new Environment\n","# https://github.com/shuwang127/PatchRNN\n","```\n","\n","## Data Fetch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6z0vSq7Uoay","outputId":"4e83975d-8d04-4f69-9a2a-67f274b608ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'PatchRNN'...\n","remote: Enumerating objects: 37054, done.\u001b[K\n","remote: Counting objects: 100% (14/14), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 37054 (delta 10), reused 9 (delta 9), pack-reused 37040 (from 1)\u001b[K\n","Receiving objects: 100% (37054/37054), 202.05 MiB | 40.93 MiB/s, done.\n","Resolving deltas: 100% (425/425), done.\n","mv: cannot move 'PatchRNN/analysis' to './analysis': Directory not empty\n","mv: cannot move 'PatchRNN/data' to './data': Directory not empty\n","mv: cannot move 'PatchRNN/temp' to './temp': Directory not empty\n","unzip:  cannot find or open data/negatives/20200401N.zip, data/negatives/20200401N.zip.zip or data/negatives/20200401N.zip.ZIP.\n","unzip:  cannot find or open data/negatives/20200512N.zip, data/negatives/20200512N.zip.zip or data/negatives/20200512N.zip.ZIP.\n","unzip:  cannot find or open data/negatives/20200528N.zip, data/negatives/20200528N.zip.zip or data/negatives/20200528N.zip.ZIP.\n","unzip:  cannot find or open data/negatives/20200603N.zip, data/negatives/20200603N.zip.zip or data/negatives/20200603N.zip.ZIP.\n","unzip:  cannot find or open data/negatives/20200618N.zip, data/negatives/20200618N.zip.zip or data/negatives/20200618N.zip.ZIP.\n","unzip:  cannot find or open data/negatives/20200707N.zip, data/negatives/20200707N.zip.zip or data/negatives/20200707N.zip.ZIP.\n","unzip:  cannot find or open data/positives/20200401P.zip, data/positives/20200401P.zip.zip or data/positives/20200401P.zip.ZIP.\n","unzip:  cannot find or open data/positives/20200512P.zip, data/positives/20200512P.zip.zip or data/positives/20200512P.zip.ZIP.\n","unzip:  cannot find or open data/positives/20200528P.zip, data/positives/20200528P.zip.zip or data/positives/20200528P.zip.ZIP.\n","unzip:  cannot find or open data/positives/20200603P.zip, data/positives/20200603P.zip.zip or data/positives/20200603P.zip.ZIP.\n","unzip:  cannot find or open data/positives/20200618P.zip, data/positives/20200618P.zip.zip or data/positives/20200618P.zip.ZIP.\n","unzip:  cannot find or open data/positives/20200707P.zip, data/positives/20200707P.zip.zip or data/positives/20200707P.zip.ZIP.\n","unzip:  cannot find or open temp/data.zip, temp/data.zip.zip or temp/data.zip.ZIP.\n","unzip:  cannot find or open temp/msgs.zip, temp/msgs.zip.zip or temp/msgs.zip.ZIP.\n","unzip:  cannot find or open temp/props.zip, temp/props.zip.zip or temp/props.zip.ZIP.\n"]}],"source":["# Get the numpy arrays from repository\n","!git clone https://github.com/shuwang127/PatchRNN\n","!mv PatchRNN/analysis .\n","!mv PatchRNN/data .\n","!mv PatchRNN/temp .\n","!rm -r PatchRNN\n","!unzip -d data/negatives/ data/negatives/20200401N.zip && rm data/negatives/20200401N.zip\n","!unzip -d data/negatives/ data/negatives/20200512N.zip && rm data/negatives/20200512N.zip\n","!unzip -d data/negatives/ data/negatives/20200528N.zip && rm data/negatives/20200528N.zip\n","!unzip -d data/negatives/ data/negatives/20200603N.zip && rm data/negatives/20200603N.zip\n","!unzip -d data/negatives/ data/negatives/20200618N.zip && rm data/negatives/20200618N.zip\n","!unzip -d data/negatives/ data/negatives/20200707N.zip && rm data/negatives/20200707N.zip\n","!unzip -d data/positives/ data/positives/20200401P.zip && rm data/positives/20200401P.zip\n","!unzip -d data/positives/ data/positives/20200512P.zip && rm data/positives/20200512P.zip\n","!unzip -d data/positives/ data/positives/20200528P.zip && rm data/positives/20200528P.zip\n","!unzip -d data/positives/ data/positives/20200603P.zip && rm data/positives/20200603P.zip\n","!unzip -d data/positives/ data/positives/20200618P.zip && rm data/positives/20200618P.zip\n","!unzip -d data/positives/ data/positives/20200707P.zip && rm data/positives/20200707P.zip\n","!unzip -d temp temp/data.zip && rm temp/data.zip\n","!unzip -d temp temp/msgs.zip && rm temp/msgs.zip\n","!unzip -d temp temp/props.zip && rm temp/props.zip"]},{"cell_type":"markdown","metadata":{"id":"K-6d2b5aWJ5i"},"source":["## Default Hyperparameters (global variables)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Ir5E0E9KV-Sh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"47f8f82a-6053-4b29-c6d8-e6d3b7c8fed5","executionInfo":{"status":"ok","timestamp":1750533033870,"user_tz":-120,"elapsed":4376,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# @title  {\"display-mode\":\"code\"}\n","# This section contains mostly unchanged code created by:\n","# Xinda Wang∗, Shu Wang∗, Pengbin Feng∗, Kun Sun∗, Sushil Jajodia∗, Sanae Benchaaboun†, and Frank Geck†∗Center for Secure Information Systems, George Mason University, Fairfax, VA, US† CSIA Division, C5ISR Center, Space and Terrestrial Communications Directorate,U.S. Army Combat Capabilities Development Command (DEVCOM)\n","# PatchRNN: A Deep Learning-Based System for Security Patch Identification\n","# Original code can be found here: https://github.com/shuwang127/PatchRNN-demo/tree/main/model\n","# Shared under Apache 2.0 license: https://github.com/shuwang127/PatchRNN-demo/tree/main?tab=Apache-2.0-1-ov-file#readme\n","# There might be minor changes that were necessary to port the original code to Transformer-based setup in later sections.\n","\n","os.system('pip install clang')\n","import re\n","import gc\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","from nltk.tokenize import TweetTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import clang.cindex\n","import clang.enumerations\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as torchdata\n","from sklearn.metrics import accuracy_score\n","\n","# environment settings.\n","_COLAB_ = 0 if (os.getenv('COLAB_GPU', 'NONE') == 'NONE') else 1 # 0 : Local environment, 1 : Google Colaboratory.\n","_COLAB_ = 0\n","# file paths.\n","rootPath = './drive/My Drive/Colab Notebooks/' if (_COLAB_) else './'\n","dataPath = rootPath + '/data/'\n","sDatPath = dataPath + '/security_patch/'\n","pDatPath = dataPath + '/positives/'\n","nDatPath = dataPath + '/negatives/'\n","tempPath = rootPath + '/temp/'\n","\n","# hyper-parameters. (affect GPU memory size)\n","_DiffEmbedDim_  = 128       # 128\n","_DiffMaxLen_    = 600       # 200(0.7), 314(0.8), 609(0.9), 1100(0.95), 2200(0.98), 3289(0.99), 5000(0.995), 10000(0.9997)\n","_DRnnHidSiz_    = 16        # 16\n","_MsgEmbedDim_   = 128       # 128\n","_MsgMaxLen_     = 200       # 54(0.9), 78(0.95), 130(0.98), 187(0.99), 268(0.995), 356(0.998), 516(0.999), 1434(1)\n","_MRnnHidSiz_    = 32        # 16\n","_TwinEmbedDim_  = 128       # 128\n","_TwinMaxLen_    = 800       # 224(0.8), 425(0.9), 755(0.95), 1448(0.98), 2270(0.99)\n","_TRnnHidSiz_    = 32        # 16\n","# hyper-parameters. (affect training speed)\n","_DRnnBatchSz_   = 128       # 128\n","_DRnnLearnRt_   = 0.0001    # 0.0001\n","_MRnnBatchSz_   = 128       # 128\n","_MRnnLearnRt_   = 0.0001    # 0.0001\n","_PRnnBatchSz_   = 256       # 256\n","_PRnnLearnRt_   = 0.0005    # 0.0005\n","_TRnnBatchSz_   = 256       # 256\n","_TRnnLearnRt_   = 0.0005    # 0.0005\n","# hyper-parameters. (trivial network parameters, unnecessary to modify)\n","_DiffExtraDim_  = 2         # 2\n","_TwinExtraDim_  = 1         # 1\n","_DRnnHidLay_    = 1         # 1\n","_MRnnHidLay_    = 1         # 1\n","_TRnnHidLay_    = 1         # 1\n","# hyper-parameters. (epoch related parameters, unnecessary to modify)\n","_DRnnMaxEpoch_  = 1000      # 1000\n","_DRnnPerEpoch_  = 1         # 1\n","_DRnnJudEpoch_  = 10        # 10\n","_MRnnMaxEpoch_  = 1000      # 1000\n","_MRnnPerEpoch_  = 1         # 1\n","_MRnnJudEpoch_  = 10        # 10\n","_PRnnMaxEpoch_  = 1000      # 1000\n","_PRnnPerEpoch_  = 1         # 1\n","_PRnnJudEpoch_  = 10        # 10\n","_TRnnMaxEpoch_  = 1000      # 1000\n","_TRnnPerEpoch_  = 1         # 1\n","_TRnnJudEpoch_  = 10        # 10\n","# hyper-parameters. (flow control)\n","_DEBUG_ = 0 #  0 : release\n","            #  1 : debug\n","_LOCK_  = 0 #  0 : unlocked - create random split sets.\n","            #  1 : locked   - use the saved split sets.\n","_MODEL_ = 0 #  0 : unlocked - train a new model.\n","            #  1 : locked   - load the saved model.\n","_DTYP_  = 1 #  0 : maintain both diff code and context code.\n","            #  1 : only maintain diff code.\n","_CTYP_  = 0 #  0 : maintain both the code and comments.\n","            #  1 : only maintain code and delete comments.\n","_NIND_ =  1 # -1 : not abstract tokens. (and will disable _NLIT_)\n","            #  0 : abstract identifiers with VAR/FUNC.\n","            #  1 : abstract identifiers with VARn/FUNCn.\n","_NLIT_  = 1 #  0 : abstract literals with LITERAL.\n","            #  1 : abstract literals with LITERAL/n.\n","_TWIN_  = 1 #  0 : only twin neural network.\n","            #  1 : twins + msg neural network.\n","\n","# print setting.\n","pd.options.display.max_columns = None\n","pd.options.display.max_rows = None\n","np.set_printoptions(threshold=np.inf)"]},{"cell_type":"markdown","metadata":{"id":"3tY0VEerWlxT"},"source":["## Codebase from original article"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ykw75_zlW5vo","cellView":"form","executionInfo":{"status":"ok","timestamp":1750533033875,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# Copied from shuwang127/PatchRNN-demo\n","# @title SplitData\n","def SplitData(data, labels, setType, rate=0.2):\n","    '''\n","    Split the data and labels into two sets with a specific rate.\n","    :param data: feature data.\n","    [[[n, {0~5}, {-1~1}], ...], ...]\n","    [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n","    :param labels: labels. [[0/1], ...]\n","    :param setType: the splited dataset type.\n","    :param rate: the split rate. 0 ~ 1\n","    :return: dsetRest - the rest dataset.\n","             lsetRest - the rest labels.\n","             dset - the splited dataset.\n","             lset - the splited labels.\n","    '''\n","\n","    # set parameters.\n","    setType = setType.upper()\n","    numData = len(data)\n","    num = math.floor(numData * rate)\n","\n","    # get the random data list.\n","    if (os.path.exists(tempPath + '/split_' + setType + '.npy')) & (_LOCK_):\n","        dataList = np.load(tempPath + '/split_' + setType + '.npy')\n","    else:\n","        dataList = list(range(numData))\n","        random.seed(10)\n","        random.shuffle(dataList)\n","        np.save(tempPath + '/split_' + setType + '.npy', dataList, allow_pickle=True)\n","\n","    # split data.\n","    dset = data[dataList[0:num]]\n","    lset = labels[dataList[0:num]]\n","    dsetRest = data[dataList[num:]]\n","    lsetRest = labels[dataList[num:]]\n","\n","    # print.\n","    setTypeRest = 'TRAIN' if (setType == 'VALID') else 'REST'\n","    print('[INFO] <SplitData> Split data into ' + str(len(dsetRest)) + ' ' + setTypeRest\n","          + ' dataset and ' + str(len(dset)) + ' ' + setType + ' dataset. (Total: '\n","          + str(len(dsetRest) + len(dset)) + ', Rate: ' + str(int(rate * 100)) + '%)')\n","\n","    return dsetRest, lsetRest, dset, lset"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","id":"BMK-iDFPYLDj","executionInfo":{"status":"ok","timestamp":1750533033880,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title TwinRNNTrain\n","def TwinRNNTrain(dTrain, lTrain, dValid, lValid, preWTwin, preWMsg, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n","    '''\n","    Train the TwinRNN model.\n","    :param dTrain: training data. [[n, ...], ...]\n","    :param lTrain: training label. [[n, ...], ...]\n","    :param dValid: validation data. [[n, ...], ...]\n","    :param lValid: validation label. [[n, ...], ...]\n","    :param preWDiff: pre-trained weights for diff embedding layer.\n","    :param preWMsg: pre-trained weights for msg embedding layer.\n","    :param batchsize: number of samples in a batch.\n","    :param learnRate: learning rate.\n","    :param dTest: test data. [[n, ...], ...]\n","    :param lTest: test label. [[n, ...], ...]\n","    :return: model - the TwinRNN model.\n","    '''\n","\n","    # get the mark of the test dataset.\n","    if dTest is None: dTest = []\n","    if lTest is None: lTest = []\n","    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n","\n","    # tensor data processing.\n","    xTrain = torch.from_numpy(dTrain).long().cuda()\n","    yTrain = torch.from_numpy(lTrain).long().cuda()\n","    xValid = torch.from_numpy(dValid).long().cuda()\n","    yValid = torch.from_numpy(lValid).long().cuda()\n","    if (markTest):\n","        xTest = torch.from_numpy(dTest).long().cuda()\n","        yTest = torch.from_numpy(lTest).long().cuda()\n","\n","    # batch size processing.\n","    train = torchdata.TensorDataset(xTrain, yTrain)\n","    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n","    valid = torchdata.TensorDataset(xValid, yValid)\n","    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n","    if (markTest):\n","        test = torchdata.TensorDataset(xTest, yTest)\n","        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n","\n","    # get training weights.\n","    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n","    weights = []\n","    for lb in range(2):\n","        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n","    lbWeights = torch.FloatTensor(weights).cuda()\n","\n","    # build the model of recurrent neural network.\n","    preWTwin = torch.from_numpy(preWTwin)\n","    preWMsg = torch.from_numpy(preWMsg)\n","    model = TwinRNN(preWTwin, preWMsg, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    print('[INFO] <TwinRNNTrain> ModelType: TwinRNN.')\n","    print('[INFO] <TwinRNNTrain> Code Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_TwinEmbedDim_, _TwinMaxLen_, _TRnnHidSiz_, _TRnnHidLay_))\n","    print('[INFO] <TwinRNNTrain> Msg  Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_MsgEmbedDim_, _MsgMaxLen_, _MRnnHidSiz_, _MRnnHidLay_))\n","    print('[INFO] <TwinRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d, JudEpoch: %d.' % (batchsize, learnRate, _TRnnMaxEpoch_, _TRnnPerEpoch_, _TRnnJudEpoch_))\n","    # optimizing with stochastic gradient descent.\n","    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n","    # seting loss function as mean squared error.\n","    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n","    # memory\n","    torch.backends.cudnn.benchmark = True\n","    torch.backends.cudnn.enabled = True\n","\n","    # run on each epoch.\n","    accList = [0]\n","    for epoch in range(_TRnnMaxEpoch_):\n","        # training phase.\n","        model.train()\n","        lossTrain = 0\n","        predictions = []\n","        labels = []\n","        for iter, (data, label) in enumerate(trainloader):\n","            # data conversion.\n","            data = data.to(device)\n","            label = label.contiguous().view(-1)\n","            label = label.to(device)\n","            # back propagation.\n","            optimizer.zero_grad()  # set the gradients to zero.\n","            yhat = model.forward(data)  # get output\n","            loss = criterion(yhat, label)\n","            loss.backward()\n","            optimizer.step()\n","            # statistic\n","            lossTrain += loss.item() * len(label)\n","            preds = yhat.max(1)[1]\n","            predictions.extend(preds.int().tolist())\n","            labels.extend(label.int().tolist())\n","            torch.cuda.empty_cache()\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        lossTrain /= len(dTrain)\n","        # train accuracy.\n","        accTrain = accuracy_score(labels, predictions) * 100\n","\n","        # validation phase.\n","        model.eval()\n","        predictions = []\n","        labels = []\n","        with torch.no_grad():\n","            for iter, (data, label) in enumerate(validloader):\n","                # data conversion.\n","                data = data.to(device)\n","                label = label.contiguous().view(-1)\n","                label = label.to(device)\n","                # forward propagation.\n","                yhat = model.forward(data)  # get output\n","                # statistic\n","                preds = yhat.max(1)[1]\n","                predictions.extend(preds.int().tolist())\n","                labels.extend(label.int().tolist())\n","                torch.cuda.empty_cache()\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # valid accuracy.\n","        accValid = accuracy_score(labels, predictions) * 100\n","        accList.append(accValid)\n","\n","        # testing phase.\n","        if (markTest):\n","            model.eval()\n","            predictions = []\n","            labels = []\n","            with torch.no_grad():\n","                for iter, (data, label) in enumerate(testloader):\n","                    # data conversion.\n","                    data = data.to(device)\n","                    label = label.contiguous().view(-1)\n","                    label = label.to(device)\n","                    # forward propagation.\n","                    yhat = model.forward(data)  # get output\n","                    # statistic\n","                    preds = yhat.max(1)[1]\n","                    predictions.extend(preds.int().tolist())\n","                    labels.extend(label.int().tolist())\n","                    torch.cuda.empty_cache()\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            # test accuracy.\n","            accTest = accuracy_score(labels, predictions) * 100\n","\n","        # output information.\n","        if (0 == (epoch + 1) % _TRnnPerEpoch_):\n","            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n","            if (markTest):\n","                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n","            print(strAcc)\n","        # save the best model.\n","        if (accList[-1] > max(accList[0:-1])):\n","            torch.save(model.state_dict(), tempPath + '/model_TwinRNN.pth')\n","        # stop judgement.\n","        if (epoch >= _TRnnJudEpoch_) and (accList[-1] < min(accList[-1-_TRnnJudEpoch_:-1])):\n","            break\n","\n","    # load best model.\n","    model.load_state_dict(torch.load(tempPath + '/model_TwinRNN.pth'))\n","    print('[INFO] <TwinRNNTrain> Finish training TwinRNN model. (Best model: ' + tempPath + '/model_TwinRNN.pth)')\n","\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"cellView":"form","id":"-C2D1PEWa_ui","executionInfo":{"status":"ok","timestamp":1750533033889,"user_tz":-120,"elapsed":6,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title ReadData\n","def ReadData():\n","    '''\n","    Read data from the files.\n","    :return: data - a set of commit message, diff code, and labels.\n","    [[['', ...], [['', ...], ['', ...], ...], 0/1], ...]\n","    '''\n","\n","    def ReadCommitMsg(filename):\n","        '''\n","        Read commit message from a file.\n","        :param filename: file name (string).\n","        :return: commitMsg - commit message.\n","        ['line', 'line', ...]\n","        '''\n","\n","        fp = open(filename, encoding='utf-8', errors='ignore')  # get file point.\n","        lines = fp.readlines()  # read all lines.\n","        #numLines = len(lines)   # get the line number.\n","        #print(lines)\n","\n","        # initialize commit message.\n","        commitMsg = []\n","        # get the wide range of commit message.\n","        for line in lines:\n","            if line.startswith('diff --git'):\n","                break\n","            else:\n","                commitMsg.append(line)\n","        #print(commitMsg)\n","        # process the head of commit message.\n","        while (1):\n","            headMsg = commitMsg[0]\n","            if (headMsg.startswith('From') or headMsg.startswith('Date:') or headMsg.startswith('Subject:')\n","                    or headMsg.startswith('commit') or headMsg.startswith('Author:')):\n","                commitMsg.pop(0)\n","            else:\n","                break\n","        #print(commitMsg)\n","        # process the tail of commit message.\n","        dashLines = [i for i in range(len(commitMsg))\n","                     if commitMsg[i].startswith('---')]  # finds all lines start with ---.\n","        if (len(dashLines)):\n","            lnum = dashLines[-1]  # last line number of ---\n","            marks = [1 if (' file changed, ' in commitMsg[i] or ' files changed, ' in commitMsg[i]) else 0\n","                     for i in range(lnum, len(commitMsg))]\n","            if (sum(marks)):\n","                for i in reversed(range(lnum, len(commitMsg))):\n","                    commitMsg.pop(i)\n","        #print(commitMsg)\n","\n","        #msgShow = ''\n","        #for i in range(len(commitMsg)):\n","        #    msgShow += commitMsg[i]\n","        #print(msgShow)\n","\n","        return commitMsg\n","\n","    def ReadDiffLines(filename):\n","        '''\n","        Read diff code from a file.\n","        :param filename:  file name (string).\n","        :return: diffLines - diff code.\n","        [['line', ...], ['line', ...], ...]\n","        '''\n","\n","        fp = open(filename, encoding='utf-8', errors='ignore')  # get file point.\n","        lines = fp.readlines()  # read all lines.\n","        numLines = len(lines)  # get the line number.\n","        # print(filename)\n","\n","        atLines = [i for i in range(numLines) if lines[i].startswith('@@ ')]  # find all lines start with @@.\n","        atLines.append(numLines)\n","        # print(atLines)\n","\n","        diffLines = []\n","        for nh in range(len(atLines) - 1):  # find all hunks.\n","            # print(atLines[nh], atLines[nh + 1])\n","            hunk = []\n","            for nl in range(atLines[nh] + 1, atLines[nh + 1]):\n","                # print(lines[nl], end='')\n","                if lines[nl].startswith('diff --git '):\n","                    break\n","                else:\n","                    hunk.append(lines[nl])\n","            diffLines.append(hunk)\n","            # print(hunk)\n","        # print(diffLines)\n","        # print(len(diffLines))\n","\n","        # process the last hunk.\n","        lastHunk = diffLines[-1]\n","        numLastHunk = len(lastHunk)\n","        dashLines = [i for i in range(numLastHunk) if lastHunk[i].startswith('--')]\n","        if (len(dashLines)):\n","            lnum = dashLines[-1]\n","            for i in reversed(range(lnum, numLastHunk)):\n","                lastHunk.pop(i)\n","        # print(diffLines)\n","        # print(len(diffLines))\n","\n","        return diffLines\n","\n","    # create temp folder.\n","    if not os.path.exists(tempPath):\n","        os.mkdir(tempPath)\n","    fp = open(tempPath + 'filelist.txt', 'w')\n","\n","    # initialize data.\n","    data = []\n","    # read security patch data.\n","    for root, ds, fs in os.walk(sDatPath):\n","        for file in fs:\n","            filename = os.path.join(root, file).replace('\\\\', '/')\n","            fp.write(filename + '\\n')\n","            commitMsg = ReadCommitMsg(filename)\n","            diffLines = ReadDiffLines(filename)\n","            data.append([commitMsg, diffLines, 1])\n","\n","    # read positive data.\n","    for root, ds, fs in os.walk(pDatPath):\n","        for file in fs:\n","            filename = os.path.join(root, file).replace('\\\\', '/')\n","            fp.write(filename + '\\n')\n","            commitMsg = ReadCommitMsg(filename)\n","            diffLines = ReadDiffLines(filename)\n","            data.append([commitMsg, diffLines, 1])\n","\n","    # read negative data.\n","    for root, ds, fs in os.walk(nDatPath):\n","        for file in fs:\n","            filename = os.path.join(root, file).replace('\\\\', '/')\n","            fp.write(filename + '\\n')\n","            commitMsg = ReadCommitMsg(filename)\n","            diffLines = ReadDiffLines(filename)\n","            data.append([commitMsg, diffLines, 0])\n","    fp.close()\n","\n","    #print(len(dataLoaded))\n","    #print(len(dataLoaded[0]))\n","    #print(dataLoaded)\n","    # [[['a', 'b', 'c', ], [['', '', '', ], ['', '', '', ], ], 0/1], ]\n","    # sample = dataLoaded[i]\n","    # commitMsg = dataLoaded[i][0]\n","    # diffLines = dataLoaded[i][1]\n","    # label = dataLoaded[i][2]\n","    # diffHunk = dataLoaded[i][1][j]\n","\n","    # save dataLoaded.\n","    if not os.path.exists(tempPath + '/data.npy'):\n","        np.save(tempPath + '/data.npy', data, allow_pickle=True)\n","        print('[INFO] <ReadData> Save ' + str(len(data)) + ' raw data to ' + tempPath + '/data.npy.')\n","\n","    return data"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","id":"ISz-4dsJbrr_","executionInfo":{"status":"ok","timestamp":1750533033917,"user_tz":-120,"elapsed":25,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetDiffProps\n","def GetDiffProps(data):\n","    '''\n","    Get the properties of the code in diff files.\n","    :param data: [[[line, , ], [[line, , ], [line, , ], ...], 0/1], ...]\n","    :return: props - [[[tokens], [nums], [nums], 0/1], ...]\n","    '''\n","\n","    def RemoveSign(line):\n","        '''\n","        Remove the sign (+/-) in the first character.\n","        :param line: a code line.\n","        :return: process line.\n","        '''\n","\n","        return ' ' + line[1:] if (line[0] == '+') or (line[0] == '-') else line\n","\n","    def GetClangTokens(line):\n","        '''\n","        Get the tokens of a line with the Clang tool.\n","        :param line: a code line.\n","        :return: tokens - ['tk', 'tk', ...] ('tk': string)\n","                 tokenTypes - [tkt, tkt, ...] (tkt: 1, 2, 3, 4, 5)\n","                 diffTypes - [dft, dft, ...] (dft: -1, 0, 1)\n","        '''\n","\n","        # remove non-ascii\n","        line = line.encode(\"ascii\", \"ignore\").decode()\n","\n","        # defination.\n","        tokenClass = [clang.cindex.TokenKind.KEYWORD,      # 1\n","                      clang.cindex.TokenKind.IDENTIFIER,   # 2\n","                      clang.cindex.TokenKind.LITERAL,      # 3\n","                      clang.cindex.TokenKind.PUNCTUATION,  # 4\n","                      clang.cindex.TokenKind.COMMENT]      # 5\n","        tokenDict = {cls: index + 1 for index, cls in enumerate(tokenClass)}\n","        #print(tokenDict)\n","\n","        # initialize.\n","        tokens = []\n","        tokenTypes = []\n","        diffTypes = []\n","\n","        # clang sparser.\n","        idx = clang.cindex.Index.create()\n","        tu = idx.parse('tmp.cpp', args=['-std=c++11'], unsaved_files=[('tmp.cpp', RemoveSign(line))], options=0)\n","        for t in tu.get_tokens(extent=tu.cursor.extent):\n","            #print(t.kind, t.spelling, t.location)\n","            tokens.append(t.spelling)\n","            tokenTypes.append(tokenDict[t.kind])\n","            diffTypes.append(1 if (line[0] == '+') else -1 if (line[0] == '-') else 0)\n","        #print(tokens)\n","        #print(tokenTypes)\n","        #print(diffTypes)\n","\n","        return tokens, tokenTypes, diffTypes\n","\n","    def GetWordTokens(line):\n","        '''\n","        Get the word tokens from a code line.\n","        :param line: a code line.\n","        :return: tokens - ['tk', 'tk', ...] ('tk': string)\n","        '''\n","\n","        tknzr = TweetTokenizer()\n","        tokens = tknzr.tokenize(RemoveSign(line))\n","        return tokens\n","\n","    def GetString(lines):\n","        '''\n","        Get the strings from the diff code\n","        :param lines: diff code.\n","        :return: lineStr - All the diff lines.\n","                 lineStrB - The before-version code lines.\n","                 lineStrA - The after-version code lines.\n","        '''\n","\n","        lineStr = ''\n","        lineStrB = ''\n","        lineStrA = ''\n","        for hunk in lines:\n","            for line in hunk:\n","                # all lines.\n","                lineStr += RemoveSign(line)\n","                # all Before lines.\n","                lineStrB += RemoveSign(line) if line[0] != '+' else ''\n","                # all After lines.\n","                lineStrA += RemoveSign(line) if line[0] != '-' else ''\n","\n","        return lineStr, lineStrB, lineStrA\n","\n","    def GetDiffTokens(lines):\n","        '''\n","        Get the tokens for the diff lines.\n","        :param lines: the diff code.\n","        :return: tokens - tokens ['tk', 'tk', ...] ('tk': string)\n","                 tokenTypes - token types [tkt, tkt, ...] (tkt: 1, 2, 3, 4, 5)\n","                 diffTypes - diff types [dft, dft, ...] (dft: -1, 0, 1)\n","        '''\n","\n","        # initialize.\n","        tokens = []\n","        tokenTypes = []\n","        diffTypes = []\n","\n","        # for each line of lines.\n","        for hunk in lines:\n","            for line in hunk:\n","                #print(line, end='')\n","                tk, tkT, dfT = GetClangTokens(line)\n","                tokens.extend(tk)\n","                tokenTypes.extend(tkT)\n","                diffTypes.extend(dfT)\n","                #print('-----------------------------------------------------------------------')\n","        #print(tokens)\n","        #print(tokenTypes)\n","        #print(diffTypes)\n","\n","        return tokens, tokenTypes, diffTypes\n","\n","    #lines = data[0][1]\n","    #print(lines)\n","    #hunk = data[0][1][0]\n","    #print(hunk)\n","    #line = data[0][1][0][0]\n","    #print(line)\n","\n","    # for each sample data[n].\n","    numData = len(data)\n","    props = []\n","    for n in range(numData):\n","        # get the lines of the diff file.\n","        diffLines = data[n][1]\n","        # properties.\n","        tk, tkT, dfT = GetDiffTokens(diffLines)\n","        label = data[n][2]\n","        prop = [tk, tkT, dfT, label]\n","        #print(prop)\n","        props.append(prop)\n","        print(n)\n","\n","    # save props.\n","    if not os.path.exists(tempPath):\n","        os.mkdir(tempPath)\n","    if not os.path.exists(tempPath + '/props.npy'):\n","        np.save(tempPath + '/props.npy', props, allow_pickle=True)\n","        print('[INFO] <GetDiffProps> Save ' + str(len(props)) + ' diff property data to ' + tempPath + '/props.npy.')\n","\n","    return props"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","id":"TaQXUyRXbvL1","executionInfo":{"status":"ok","timestamp":1750533033946,"user_tz":-120,"elapsed":28,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title ProcessTokens\n","def ProcessTokens(props, dType=1, cType=1):\n","    '''\n","    only maintain the diff parts of the code.\n","    :param props: the features of diff code.\n","    [[[tokens], [nums], [nums], 0/1], ...]\n","    :param dType: 0 - maintain both diff code and context code.\n","                  1 - only maintain diff code.\n","    :param cType: 0 - maintain both the code and comments.\n","                  1 - only maintain code and delete comments.\n","    :return: props - the normalized features of diff code.\n","    [[[tokens], [nums], [nums], 0/1], ...]\n","    '''\n","\n","    # process diff code.\n","    if (1 == dType):\n","        propsNew = []\n","        for item in props:\n","            # the number of tokens.\n","            numTokens = len(item[1])\n","            # item[0]: tokens, item[1]: tokenTypes, item[2]: diffTypes, item[3]: label.\n","            tokens = [item[0][n] for n in range(numTokens) if (item[2][n])]\n","            tokenTypes = [item[1][n] for n in range(numTokens) if (item[2][n])]\n","            diffTypes = [item[2][n] for n in range(numTokens) if (item[2][n])]\n","            label = item[3]\n","            # reconstruct sample.\n","            sample = [tokens, tokenTypes, diffTypes, label]\n","            propsNew.append(sample)\n","        props = propsNew\n","        print('[INFO] <ProcessTokens> Only maintain the diff parts of the code.')\n","\n","    # process comments.\n","    if (1 == cType):\n","        propsNew = []\n","        for item in props:\n","            # the number of tokens.\n","            numTokens = len(item[1])\n","            # item[0]: tokens, item[1]: tokenTypes, item[2]: diffTypes, item[3]: label.\n","            tokens = [item[0][n] for n in range(numTokens) if (item[1][n] < 5)]\n","            tokenTypes = [item[1][n] for n in range(numTokens) if (item[1][n] < 5)]\n","            diffTypes = [item[2][n] for n in range(numTokens) if (item[1][n] < 5)]\n","            label = item[3]\n","            # reconstruct sample.\n","            sample = [tokens, tokenTypes, diffTypes, label]\n","            propsNew.append(sample)\n","        props = propsNew\n","        print('[INFO] <ProcessTokens> Delete the comment parts of the diff code.')\n","\n","    #print(props[0])\n","\n","    return props"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","id":"_vzOdioNb28G","executionInfo":{"status":"ok","timestamp":1750533033950,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title AbstractTokens\n","def AbstractTokens(props, iType=1, lType=1):\n","    '''\n","    abstract the tokens of identifiers, literals, and comments.\n","    :param props: the features of diff code.\n","    [[[tokens], [nums], [nums], 0/1], ...]\n","    :param iType:   -1 - not abstract tokens.\n","                     0 - only abstract variable type and function type. VAR / FUNC\n","                     1 - abstract the identical variable names and function names.  VAR0, VAR1, ... / FUNC0, FUNC1, ...\n","    :param lType:   -1 - not abstract tokens.\n","                     0 - abstract literals with LITERAL.\n","                     1 - abstract literals with LITERAL/n.\n","    :return: props - the abstracted features of diff code.\n","    [[[tokens], [nums], [nums], 0/1], ...]\n","    '''\n","\n","    if (iType not in [0, 1]) or (lType not in [0, 1]):\n","        print('[INFO] <AbstractTokens> Not abstract the tokens of identifiers, literals, and comments.')\n","        return props\n","\n","    for item in props:\n","        # get tokens and token types.\n","        tokens = item[0]\n","        tokenTypes = item[1]\n","        numTokens = len(tokenTypes)\n","        #print(tokens)\n","        #print(tokenTypes)\n","        #print(numTokens)\n","\n","        # abstract literals and comments, and separate identifiers into variables and functions.\n","        markVar = list(np.zeros(numTokens, dtype=int))\n","        markFuc = list(np.zeros(numTokens, dtype=int))\n","        for n in range(numTokens):\n","            # 2: IDENTIFIER, 3: LITERAL, 5: COMMENT\n","            if 5 == tokenTypes[n]:\n","                tokens[n] = 'COMMENT'\n","            elif 3 == tokenTypes[n]:\n","                if (0 == lType):\n","                    tokens[n] = 'LITERAL'\n","                elif (1 == lType):\n","                    if (not tokens[n].isdigit()):\n","                        tokens[n] = 'LITERAL'\n","            elif 2 == tokenTypes[n]:\n","                # separate variable name and function name.\n","                if (n < numTokens-1):\n","                    if (tokens[n+1] == '('):\n","                        markFuc[n] = 1\n","                    else:\n","                        markVar[n] = 1\n","                else:\n","                    markVar[n] = 1\n","        #print(tokens)\n","        #print(markVar)\n","        #print(markFuc)\n","\n","        # abstract variables and functions.\n","        if (0 == iType):\n","            for n in range(numTokens):\n","                if 1 == markVar[n]:\n","                    tokens[n] = 'VAR'\n","                elif 1 == markFuc[n]:\n","                    tokens[n] = 'FUNC'\n","        elif (1 == iType):\n","            # get variable dictionary.\n","            varList = [tokens[idx] for idx, mark in enumerate(markVar) if mark == 1]\n","            varVoc  = {}.fromkeys(varList)\n","            varVoc  = list(varVoc.keys())\n","            varDict = {tk: 'VAR' + str(idx) for idx, tk in enumerate(varVoc)}\n","            # get function dictionary.\n","            fucList = [tokens[idx] for idx, mark in enumerate(markFuc) if mark == 1]\n","            fucVoc  = {}.fromkeys(fucList)\n","            fucVoc  = list(fucVoc.keys())\n","            fucDict = {tk: 'FUNC' + str(idx) for idx, tk in enumerate(fucVoc)}\n","            #print(varDict)\n","            #print(fucDict)\n","            for n in range(numTokens):\n","                if 1 == markVar[n]:\n","                    tokens[n] = varDict[tokens[n]]\n","                elif 1 == markFuc[n]:\n","                    tokens[n] = fucDict[tokens[n]]\n","    #print(tokens)\n","    print('[INFO] <AbstractTokens> Abstract the tokens of identifiers with iType ' + str(iType), end='')\n","    print(' (VAR/FUNC).') if (0 == iType) else print(' (VARn/FUNCn).')\n","    print('[INFO] <AbstractTokens> Abstract the tokens of literals, and comments with iType ' + str(lType), end='')\n","    print(' (LITERAL/COMMENT).') if (0 == lType) else print(' (LITERAL/n/COMMENT).')\n","\n","    return props"]},{"cell_type":"code","execution_count":10,"metadata":{"cellView":"form","id":"D3j9mqRscExB","executionInfo":{"status":"ok","timestamp":1750533033954,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetDiffVocab\n","def GetDiffVocab(props):\n","    '''\n","    Get the vocabulary of diff tokens\n","    :param props - the features of diff code.\n","    [[[tokens], [nums], [nums], 0/1], ...]\n","    :return: vocab - the vocabulary of diff tokens. ['tk', 'tk', ...]\n","             maxLen - the max length of a diff code.\n","    '''\n","\n","    # create temp folder.\n","    if not os.path.exists(tempPath):\n","        os.mkdir(tempPath)\n","    fp = open(tempPath + 'difflen.csv', 'w')\n","\n","    # get the whole tokens and the max diff length.\n","    tokens = []\n","    maxLen = 0\n","\n","    # for each sample.\n","    for item in props:\n","        tokens.extend(item[0])\n","        maxLen = len(item[0]) if (len(item[0]) > maxLen) else maxLen\n","        fp.write(str(len(item[0])) + '\\n')\n","    fp.close()\n","\n","    # remove duplicates and get vocabulary.\n","    vocab = {}.fromkeys(tokens)\n","    vocab = list(vocab.keys())\n","\n","    # print.\n","    print('[INFO] <GetDiffVocab> There are ' + str(len(vocab)) + ' diff vocabulary tokens. (except \\'<pad>\\')')\n","    print('[INFO] <GetDiffVocab> The max diff length is ' + str(maxLen) + ' tokens. (hyperparameter: _DiffMaxLen_ = ' + str(_DiffMaxLen_) + ')')\n","\n","    return vocab, maxLen"]},{"cell_type":"code","execution_count":11,"metadata":{"cellView":"form","id":"1bMPLAuCcHUh","executionInfo":{"status":"ok","timestamp":1750533033958,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetDiffDict\n","def GetDiffDict(vocab):\n","    '''\n","    Get the dictionary of diff vocabulary.\n","    :param vocab: the vocabulary of diff tokens. ['tk', 'tk', ...]\n","    :return: tokenDict - the dictionary of diff vocabulary.\n","    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n","    '''\n","\n","    # get token dict from vocabulary.\n","    tokenDict = {token: (index+1) for index, token in enumerate(vocab)}\n","    tokenDict['<pad>'] = 0\n","\n","    # print.\n","    print('[INFO] <GetDiffDict> Create dictionary for ' + str(len(tokenDict)) + ' diff vocabulary tokens. (with \\'<pad>\\')')\n","\n","    return tokenDict"]},{"cell_type":"code","execution_count":12,"metadata":{"cellView":"form","id":"f7_gKJiPcLtS","executionInfo":{"status":"ok","timestamp":1750533033961,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetDiffEmbed\n","def GetDiffEmbed(tokenDict, embedSize):\n","    '''\n","    Get the pre-trained weights for embedding layer from the dictionary of diff vocabulary.\n","    :param tokenDict: the dictionary of diff vocabulary.\n","    {'tk': 0, 'tk': 1, ..., '<pad>': N}\n","    :param embedSize: the dimension of the embedding vector.\n","    :return: preWeights - the pre-trained weights for embedding layer.\n","    [[n, ...], [n, ...], ...]\n","    '''\n","\n","    # number of the vocabulary tokens.\n","    numTokens = len(tokenDict)\n","\n","    # initialize the pre-trained weights for embedding layer.\n","    preWeights = np.zeros((numTokens, embedSize))\n","    for index in range(numTokens):\n","        preWeights[index] = np.random.normal(size=(embedSize,))\n","    print('[INFO] <GetDiffEmbed> Create pre-trained embedding weights with ' + str(len(preWeights)) + ' * ' + str(len(preWeights[0])) + ' matrix.')\n","\n","    # save preWeights.\n","    if not os.path.exists(tempPath + '/preWeights.npy'):\n","        np.save(tempPath + '/preWeights.npy', preWeights, allow_pickle=True)\n","        print('[INFO] <GetDiffEmbed> Save the pre-trained weights of embedding layer to ' + tempPath + '/preWeights.npy.')\n","\n","    return preWeights"]},{"cell_type":"code","execution_count":13,"metadata":{"cellView":"form","id":"fCjLn5cpcQDW","executionInfo":{"status":"ok","timestamp":1750533033964,"user_tz":-120,"elapsed":1,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title DivideBeforeAfter\n","def DivideBeforeAfter(diffProps):\n","\n","    # create temp folder.\n","    if not os.path.exists(tempPath):\n","        os.mkdir(tempPath)\n","    fp = open(tempPath + 'twinlen.csv', 'w')\n","\n","    twinProps = []\n","    maxLen = 0\n","    # for each sample in diffProps.\n","    for item in diffProps:\n","        # get the tk, tkT, dfT, lb.\n","        tokens = item[0]\n","        tokenTypes = item[1]\n","        diffTypes = item[2]\n","        label = item[3]\n","        numTokens = len(diffTypes)\n","        # reconstruct tkB, tkTB, tkA, tkTA.\n","        tokensB = [tokens[i] for i in range(numTokens) if (diffTypes[i] <= 0)]\n","        tokenTypesB = [tokenTypes[i] for i in range(numTokens) if (diffTypes[i] <= 0)]\n","        tokensA = [tokens[i] for i in range(numTokens) if (diffTypes[i] >= 0)]\n","        tokenTypesA = [tokenTypes[i] for i in range(numTokens) if (diffTypes[i] >= 0)]\n","        # reconstruct new sample.\n","        sample = [tokensB, tokenTypesB, tokensA, tokenTypesA, label]\n","        twinProps.append(sample)\n","        # get max length.\n","        maxLenAB = max(len(tokenTypesB), len(tokenTypesA))\n","        maxLen = maxLenAB if (maxLen < maxLenAB) else maxLen\n","        fp.write(str(len(tokenTypesB)) + '\\n')\n","        fp.write(str(len(tokenTypesA)) + '\\n')\n","    fp.close()\n","\n","    #print(twinProps[0])\n","    #print(maxLen)\n","\n","    # print.\n","    print('[INFO] <DivideBeforeAfter> Divide diff code into BEFORE-version and AFTER-version code.')\n","    print('[INFO] <DivideBeforeAfter> The max length in BEFORE/AFTER-version code is ' + str(maxLen) + ' tokens. (hyperparameter: _TwinMaxLen_ = ' + str(_TwinMaxLen_) + ')')\n","\n","    return twinProps, maxLen"]},{"cell_type":"code","execution_count":14,"metadata":{"cellView":"form","id":"qgm61TAxc0CO","executionInfo":{"status":"ok","timestamp":1750533033968,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetTwinMapping\n","def GetTwinMapping(props, maxLen, tokenDict):\n","    '''\n","    Map the feature data into indexed data.\n","    :param props: the features of diff code.\n","    [[[tokens], [nums], [tokens], [nums], 0/1], ...]\n","    :param maxLen: the max length of a twin code.\n","    :param tokenDict: the dictionary of diff vocabulary.\n","    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n","    :return: np.array(data) - feature data.\n","             [[[n, {0~5}, n, {0~5}], ...], ...]\n","             np.array(labels) - labels.\n","             [[0/1], ...]\n","    '''\n","\n","    def PadList(dList, pad, length):\n","        '''\n","        Pad the list data to a fixed length.\n","        :param dList: the list data - [ , , ...]\n","        :param pad: the variable used to pad.\n","        :param length: the fixed length.\n","        :return: dList - padded list data. [ , , ...]\n","        '''\n","\n","        if len(dList) <= length:\n","            dList.extend(pad for i in range(length - len(dList)))\n","        elif len(dList) > length:\n","            dList = dList[0:length]\n","\n","        return dList\n","\n","    # initialize the data and labels.\n","    data = []\n","    labels = []\n","\n","    # for each sample.\n","    for item in props:\n","        # initialize sample.\n","        sample = []\n","\n","        # process tokensB.\n","        tokens = item[0]\n","        tokens = PadList(tokens, '<pad>', maxLen)\n","        tokens2index = []\n","        for tk in tokens:\n","            tokens2index.append(tokenDict[tk])\n","        sample.append(tokens2index)\n","        # process tokenTypesB.\n","        tokenTypes = item[1]\n","        tokenTypes = PadList(tokenTypes, 0, maxLen)\n","        sample.append(tokenTypes)\n","        # process tokensA.\n","        tokens = item[2]\n","        tokens = PadList(tokens, '<pad>', maxLen)\n","        tokens2index = []\n","        for tk in tokens:\n","            tokens2index.append(tokenDict[tk])\n","        sample.append(tokens2index)\n","        # process tokenTypesA.\n","        tokenTypes = item[3]\n","        tokenTypes = PadList(tokenTypes, 0, maxLen)\n","        sample.append(tokenTypes)\n","\n","        # process sample.\n","        sample = np.array(sample).T\n","        data.append(sample)\n","        # process label.\n","        label = item[4]\n","        labels.append([label])\n","\n","    if _DEBUG_:\n","        print('[DEBUG] data:')\n","        print(data[0:3])\n","        print('[DEBUG] labels:')\n","        print(labels[0:3])\n","\n","    # print.\n","    print('[INFO] <GetTwinMapping> Create ' + str(len(data)) + ' feature data with ' + str(len(data[0])) + ' * ' + str(len(data[0][0])) + ' matrix.')\n","    print('[INFO] <GetTwinMapping> Create ' + str(len(labels)) + ' labels with 1 * 1 matrix.')\n","\n","    # save files.\n","    if (not os.path.exists(tempPath + '/tdata_' + str(maxLen) + '.npy')) \\\n","            | (not os.path.exists(tempPath + '/tlabels_' + str(maxLen) + '.npy')):\n","        np.save(tempPath + '/tdata_' + str(maxLen) + '.npy', data, allow_pickle=True)\n","        print('[INFO] <GetTwinMapping> Save the mapped numpy data to ' + tempPath + '/tdata_' + str(maxLen) + '.npy.')\n","        np.save(tempPath + '/tlabels_' + str(maxLen) + '.npy', labels, allow_pickle=True)\n","        print('[INFO] <GetTwinMapping> Save the mapped numpy labels to ' + tempPath + '/tlabels_' + str(maxLen) + '.npy.')\n","\n","    return np.array(data), np.array(labels)"]},{"cell_type":"code","execution_count":15,"metadata":{"cellView":"form","id":"1EijQp48c2ba","executionInfo":{"status":"ok","timestamp":1750533033971,"user_tz":-120,"elapsed":1,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title UpdateTwinTokenTypes\n","def UpdateTwinTokenTypes(data):\n","    '''\n","    Update the token type in the feature data into one-hot vector.\n","    :param data: feature data. [[[n, {0~5}, n, {0~5},], ...], ...]\n","    :return: np.array(newData). [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, n, 0/1, 0/1, 0/1, 0/1, 0/1], ...], ...]\n","    '''\n","\n","    newData = []\n","    # for each sample.\n","    for item in data:\n","        # get the transpose of props.\n","        itemT = item.T\n","        # initialize new sample.\n","        newItem = []\n","        newItem.append(itemT[0])\n","        newItem.extend(np.zeros((5, len(item)), dtype=int))\n","        newItem.append(itemT[2])\n","        newItem.extend(np.zeros((5, len(item)), dtype=int))\n","        # assign the new sample.\n","        for i in range(len(item)):\n","            tokenType = itemT[1][i]\n","            if (tokenType):\n","                newItem[tokenType][i] = 1\n","            tokenType = itemT[3][i]\n","            if (tokenType):\n","                newItem[tokenType+6][i] = 1\n","        # get the transpose of new sample.\n","        newItem = np.array(newItem).T\n","        # append new sample.\n","        newData.append(newItem)\n","\n","    if _DEBUG_:\n","        print('[DEBUG] newData:')\n","        print(newData[0:3])\n","\n","    # print.\n","    print('[INFO] <UpdateTwinTokenTypes> Update ' + str(len(newData)) + ' feature data with ' + str(len(newData[0])) + ' * ' + str(len(newData[0][0])) + ' matrix.')\n","\n","    # save files.\n","    if (not os.path.exists(tempPath + '/newtdata_' + str(len(newData[0])) + '.npy')):\n","        np.save(tempPath + '/newtdata_' + str(len(newData[0])) + '.npy', newData, allow_pickle=True)\n","        print('[INFO] <UpdateTwinTokenTypes> Save the mapped numpy data to ' + tempPath + '/newtdata_' + str(len(newData[0])) + '.npy.')\n","\n","    # change marco.\n","    global _TwinExtraDim_\n","    _TwinExtraDim_ = 5\n","\n","    return np.array(newData)"]},{"cell_type":"code","execution_count":16,"metadata":{"cellView":"form","id":"yIsxU_uKc7Oh","executionInfo":{"status":"ok","timestamp":1750533033993,"user_tz":-120,"elapsed":20,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetCommitMsgs\n","def GetCommitMsgs(data):\n","    '''\n","    Get the commit messages in diff files.\n","    :param data: [[[line, , ], [[line, , ], [line, , ], ...], 0/1], ...]\n","    :return: msgs - [[[tokens], 0/1], ...]\n","    '''\n","\n","    def GetMsgTokens(lines):\n","        '''\n","        Get the tokens from a commit message.\n","        :param lines: commit message. [line, , ]\n","        :return: tokensStem ['tk', , ]\n","        '''\n","\n","        # concatenate lines.\n","        # get the string of commit message.\n","        msg = ''\n","        for line in lines:\n","            msg += line[:-1] + ' '\n","        #print(msg)\n","\n","        # pre-process.\n","        # remove url.\n","        pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n","        msg = re.sub(pattern, ' ', msg)\n","        # remove independent numbers.\n","        pattern = r' \\d+ '\n","        msg = re.sub(pattern, ' ', msg)\n","        # lower case capitalized words.\n","        pattern = r'([A-Z][a-z]+)'\n","        def LowerFunc(matched):\n","            return matched.group(1).lower()\n","        msg = re.sub(pattern, LowerFunc, msg)\n","        # remove footnote.\n","        patterns = ['signed-off-by:', 'reported-by:', 'reviewed-by:', 'acked-by:', 'found-by:', 'tested-by:', 'cc:']\n","        for pattern in patterns:\n","            index = msg.find(pattern)\n","            if (index > 0):\n","                msg = msg[:index]\n","        #print(msg)\n","\n","        # clearance.\n","        # get the tokens.\n","        tknzr = TweetTokenizer()\n","        tokens = tknzr.tokenize(msg)\n","        # clear tokens that don't contain any english letter.\n","        for i in reversed(range(len(tokens))):\n","            if not (re.search('[a-z]', tokens[i])):\n","                tokens.pop(i)\n","        # clear tokens that are stopwords.\n","        for i in reversed(range(len(tokens))):\n","            if (tokens[i] in stopwords.words('english')):\n","                tokens.pop(i)\n","        pattern = re.compile(\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\")\n","        for i in reversed(range(len(tokens))):\n","            if (pattern.findall(tokens[i])):\n","                tokens.pop(i)\n","        #print(tokens)\n","\n","        # process tokens with stemming.\n","        porter = PorterStemmer()\n","        tokensStem = []\n","        for item in tokens:\n","            tokensStem.append(porter.stem(item))\n","        #print(tokensStem)\n","\n","        return tokensStem\n","\n","    # for each sample data[n].\n","    numData = len(data)\n","    msgs = []\n","    for n in range(numData):\n","        # get the lines of the commit message.\n","        commitMsg = data[n][0]\n","        mtk = GetMsgTokens(commitMsg)\n","        # get the label.\n","        label = data[n][2]\n","        #print([mtk, label])\n","        # append the message tokens.\n","        msgs.append([mtk, label])\n","        print(n)\n","\n","    # save commit messages.\n","    if not os.path.exists(tempPath):\n","        os.mkdir(tempPath)\n","    if not os.path.exists(tempPath + '/msgs.npy'):\n","        np.save(tempPath + '/msgs.npy', msgs, allow_pickle=True)\n","        print('[INFO] <GetCommitMsg> Save ' + str(len(msgs)) + ' commit messages to ' + tempPath + '/msgs.npy.')\n","\n","    return msgs"]},{"cell_type":"code","execution_count":17,"metadata":{"cellView":"form","id":"qSaTh-4KdDyZ","executionInfo":{"status":"ok","timestamp":1750533033997,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetMsgVocab\n","def GetMsgVocab(msgs):\n","    '''\n","    Get the vocabulary of message tokens\n","    :param msgs - [[[tokens], 0/1], ...]\n","    :return: vocab - the vocabulary of message tokens. ['tk', 'tk', ...]\n","             maxLen - the max length of a commit message.\n","    '''\n","\n","    # create temp folder.\n","    if not os.path.exists(tempPath):\n","        os.mkdir(tempPath)\n","    fp = open(tempPath + 'msglen.csv', 'w')\n","\n","    # get the whole tokens and the max msg length.\n","    tokens = []\n","    maxLen = 0\n","\n","    # for each sample.\n","    for item in msgs:\n","        tokens.extend(item[0])\n","        maxLen = len(item[0]) if (len(item[0]) > maxLen) else maxLen\n","        fp.write(str(len(item[0])) + '\\n')\n","    fp.close()\n","\n","    # remove duplicates and get vocabulary.\n","    vocab = {}.fromkeys(tokens)\n","    vocab = list(vocab.keys())\n","\n","    # print.\n","    print('[INFO] <GetMsgVocab> There are ' + str(len(vocab)) + ' commit message vocabulary tokens. (except \\'<pad>\\')')\n","    print('[INFO] <GetMsgVocab> The max msg length is ' + str(maxLen) + ' tokens. (hyperparameter: _MsgMaxLen_ = ' + str(_MsgMaxLen_) + ')')\n","\n","    return vocab, maxLen"]},{"cell_type":"code","execution_count":18,"metadata":{"cellView":"form","id":"Pwi0PWyOdRYy","executionInfo":{"status":"ok","timestamp":1750533034002,"user_tz":-120,"elapsed":3,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetMsgDict\n","def GetMsgDict(vocab):\n","    '''\n","    Get the dictionary of msg vocabulary.\n","    :param vocab: the vocabulary of msg tokens. ['tk', 'tk', ...]\n","    :return: tokenDict - the dictionary of msg vocabulary.\n","    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n","    '''\n","\n","    # get token dict from vocabulary.\n","    tokenDict = {token: (index+1) for index, token in enumerate(vocab)}\n","    tokenDict['<pad>'] = 0\n","\n","    # print.\n","    print('[INFO] <GetMsgDict> Create dictionary for ' + str(len(tokenDict)) + ' msg vocabulary tokens. (with \\'<pad>\\')')\n","\n","    return tokenDict"]},{"cell_type":"code","execution_count":19,"metadata":{"cellView":"form","id":"mJ86EKQvdVmF","executionInfo":{"status":"ok","timestamp":1750533034008,"user_tz":-120,"elapsed":4,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetMsgEmbed\n","def GetMsgEmbed(tokenDict, embedSize):\n","    '''\n","    Get the pre-trained weights for embedding layer from the dictionary of msg vocabulary.\n","    :param tokenDict: the dictionary of msg vocabulary.\n","    {'tk': 0, 'tk': 1, ..., '<pad>': N}\n","    :param embedSize: the dimension of the embedding vector.\n","    :return: preWeights - the pre-trained weights for embedding layer.\n","    [[n, ...], [n, ...], ...]\n","    '''\n","\n","    # number of the vocabulary tokens.\n","    numTokens = len(tokenDict)\n","\n","    # initialize the pre-trained weights for embedding layer.\n","    preWeights = np.zeros((numTokens, embedSize))\n","    for index in range(numTokens):\n","        preWeights[index] = np.random.normal(size=(embedSize,))\n","    print('[INFO] <GetMsgEmbed> Create pre-trained embedding weights with ' + str(len(preWeights)) + ' * ' + str(len(preWeights[0])) + ' matrix.')\n","\n","    # save preWeights.\n","    if not os.path.exists(tempPath + '/msgPreWeights.npy'):\n","        np.save(tempPath + '/msgPreWeights.npy', preWeights, allow_pickle=True)\n","        print('[INFO] <GetMsgEmbed> Save the pre-trained weights of embedding layer to ' + tempPath + '/msgPreWeights.npy.')\n","\n","    return preWeights"]},{"cell_type":"code","execution_count":20,"metadata":{"cellView":"form","id":"__R_dpiGdcsm","executionInfo":{"status":"ok","timestamp":1750533034011,"user_tz":-120,"elapsed":4,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title GetMsgMapping\n","def GetMsgMapping(msgs, maxLen, tokenDict):\n","    '''\n","    Map the feature data into indexed data.\n","    :param props: the features of commit messages.\n","    [[[tokens], 0/1], ...]\n","    :param maxLen: the max length of the commit message.\n","    :param tokenDict: the dictionary of commit message vocabulary.\n","    {'tk': 1, 'tk': 2, ..., 'tk': N, '<pad>': 0}\n","    :return: np.array(data) - feature data.\n","             [[n, ...], ...]\n","             np.array(labels) - labels.\n","             [[0/1], ...]\n","    '''\n","\n","    def PadList(dList, pad, length):\n","        '''\n","        Pad the list data to a fixed length.\n","        :param dList: the list data - [ , , ...]\n","        :param pad: the variable used to pad.\n","        :param length: the fixed length.\n","        :return: dList - padded list data. [ , , ...]\n","        '''\n","\n","        if len(dList) <= length:\n","            dList.extend(pad for i in range(length - len(dList)))\n","        elif len(dList) > length:\n","            dList = dList[0:length]\n","\n","        return dList\n","\n","    # initialize the data and labels.\n","    data = []\n","    labels = []\n","\n","    # for each sample.\n","    for item in msgs:\n","        # process tokens.\n","        tokens = item[0]\n","        tokens = PadList(tokens, '<pad>', maxLen)\n","        # convert tokens into numbers.\n","        tokens2index = []\n","        for tk in tokens:\n","            tokens2index.append(tokenDict[tk])\n","        data.append(tokens2index)\n","        # process label.\n","        label = item[1]\n","        labels.append([label])\n","\n","    if _DEBUG_:\n","        print('[DEBUG] data:')\n","        print(data[0:3])\n","        print('[DEBUG] labels:')\n","        print(labels[0:3])\n","\n","    # print.\n","    print('[INFO] <GetMsgMapping> Create ' + str(len(data)) + ' feature data with 1 * ' + str(len(data[0])) + ' vector.')\n","    print('[INFO] <GetMsgMapping> Create ' + str(len(labels)) + ' labels with 1 * 1 matrix.')\n","\n","    # save files.\n","    if (not os.path.exists(tempPath + '/mdata_' + str(maxLen) + '.npy')) \\\n","            | (not os.path.exists(tempPath + '/mlabels_' + str(maxLen) + '.npy')):\n","        np.save(tempPath + '/mdata_' + str(maxLen) + '.npy', data, allow_pickle=True)\n","        print('[INFO] <GetMsgMapping> Save the mapped numpy data to ' + tempPath + '/mdata_' + str(maxLen) + '.npy.')\n","        np.save(tempPath + '/mlabels_' + str(maxLen) + '.npy', labels, allow_pickle=True)\n","        print('[INFO] <GetMsgMapping> Save the mapped numpy labels to ' + tempPath + '/mlabels_' + str(maxLen) + '.npy.')\n","\n","    return np.array(data), np.array(labels)"]},{"cell_type":"code","execution_count":21,"metadata":{"cellView":"form","id":"44oeeWDrdeqP","executionInfo":{"status":"ok","timestamp":1750533034015,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title CombineTwinMsgs\n","def CombineTwinMsgs(props, msgs, plabels, mlabels):\n","    '''\n","    Combine the twin props with the commit messages.\n","    :param props: twin data. [[[n, {0~5}, n, {0~5}], ...], ...] or [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, n, 0/1, 0/1, 0/1, 0/1, 0/1], ...], ...]\n","    :param msgs: message data. [[n, ...], ...]\n","    :param plabels: twin labels. [[0/1], ...]\n","    :param mlabels: message labels. [[0/1], ...]\n","    :return: np.array(data) - combined data. [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, n, 0/1, 0/1, 0/1, 0/1, 0/1, n], ...], ...]\n","             np.array(plabels) - combined labels. [[0/1], ...]\n","    '''\n","\n","    # check the lengths.\n","    if (len(plabels) != len(mlabels)):\n","        print('[ERROR] <CombineTwinMsgs> the data lengths are mismatch.')\n","        return [[]], [[]]\n","\n","    # check the labels.\n","    cntMatch = 0\n","    for n in range(len(plabels)):\n","        if (plabels[n][0] == mlabels[n][0]):\n","            cntMatch += 1\n","    if (cntMatch != len(plabels)):\n","        print('[ERROR] <CombineTwinMsgs> the labels are mismatch. ' + str(cntMatch) + '/' + str(len(plabels)) + '.')\n","        return [[]], [[]]\n","\n","    #print(props[1], len(props[1]))\n","    #print(msgs[1], len(msgs[1]))\n","\n","    data = []\n","    for n in range(len(plabels)):\n","        # get the twin prop and message.\n","        prop = props[n]\n","        msg = msgs[n]\n","        # pad data.\n","        if (_TwinMaxLen_ >= _MsgMaxLen_):\n","            msg = np.pad(msg, (0, _TwinMaxLen_ - _MsgMaxLen_), 'constant')\n","        else:\n","            prop = np.pad(prop, ((0, _MsgMaxLen_ - _TwinMaxLen_), (0, 0)), 'constant')\n","        #print(msg, len(msg))\n","        #print(prop, len(prop))\n","        # reconstruct sample.\n","        sample = np.vstack((prop.T, msg))\n","        # append the sample to data.\n","        data.append(sample.T)\n","\n","    if _DEBUG_:\n","        print(np.array(data[0:3]))\n","\n","    print('[INFO] <CombineTwinMsgs> Combine the twin props with the commit messages.')\n","\n","    return np.array(data), np.array(plabels)"]},{"cell_type":"code","execution_count":22,"metadata":{"cellView":"form","id":"FXvz1-YMpcVG","executionInfo":{"status":"ok","timestamp":1750533034018,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title OutputEval\n","def OutputEval(predictions, labels, method=''):\n","    '''\n","    Output the evaluation results.\n","    :param predictions: predicted labels. [[0], [1], ...]\n","    :param labels: ground truth labels. [[1], [1], ...]\n","    :param method: method name. string\n","    :return: accuracy - the total accuracy. numeric\n","             confusion - confusion matrix [[1000, 23], [12, 500]]\n","    '''\n","\n","    # evaluate the predictions with gold labels, and get accuracy and confusion matrix.\n","    def Evaluation(predictions, labels):\n","\n","        # parameter settings.\n","        D = len(labels)\n","        cls = 2\n","\n","        # get confusion matrix.\n","        confusion = np.zeros((cls, cls))\n","        for ind in range(D):\n","            nRow = int(predictions[ind][0])\n","            nCol = int(labels[ind][0])\n","            confusion[nRow][nCol] += 1\n","\n","        # get accuracy.\n","        accuracy = 0\n","        for ind in range(cls):\n","            accuracy += confusion[ind][ind]\n","        accuracy /= D\n","\n","        return accuracy, confusion\n","\n","    # get accuracy and confusion matrix.\n","    accuracy, confusion = Evaluation(predictions, labels)\n","    precision = confusion[1][1] / (confusion[1][0] + confusion[1][1]) if (confusion[1][0] + confusion[1][1]) else 0\n","    recall = confusion[1][1] / (confusion[0][1] + confusion[1][1]) if (confusion[0][1] + confusion[1][1]) else 0\n","    F1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n","\n","    # output on screen and to file.\n","    print('       -------------------------------------------')\n","    print('       method           :  ' +  method) if len(method) else print('', end='')\n","    print('       accuracy  (ACC)  :  %.3f%%' % (accuracy * 100))\n","    print('       precision (P)    :  %.3f%%' % (precision * 100))\n","    print('       recall    (R)    :  %.3f%%' % (recall * 100))\n","    print('       F1 score  (F1)   :  %.3f' % (F1))\n","    print('       fall-out  (FPR)  :  %.3f%%' % (confusion[1][0] * 100 / (confusion[1][0] + confusion[0][0])))\n","    print('       miss rate (FNR)  :  %.3f%%' % (confusion[0][1] * 100 / (confusion[0][1] + confusion[1][1])))\n","    print('       confusion matrix :      (actual)')\n","    print('                           Neg         Pos')\n","    print('       (predicted) Neg     %-5d(TN)   %-5d(FN)' % (confusion[0][0], confusion[0][1]))\n","    print('                   Pos     %-5d(FP)   %-5d(TP)' % (confusion[1][0], confusion[1][1]))\n","    print('       -------------------------------------------')\n","\n","    return accuracy, confusion"]},{"cell_type":"markdown","metadata":{"id":"rgLsa_DuZ3-k"},"source":["## Loading data to memory"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgOcEXvfZp76","outputId":"0d0e691b-7f3d-41d3-ff1d-d97e48b86227","executionInfo":{"status":"ok","timestamp":1750533095530,"user_tz":-120,"elapsed":61510,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] <ReadData> Load 38041 raw data from .//temp//data.npy.\n","[INFO] <GetDiffProps> Load 38041 diff property data from .//temp//props.npy.\n","[INFO] <AbstractTokens> Abstract the tokens of identifiers with iType 1 (VARn/FUNCn).\n","[INFO] <AbstractTokens> Abstract the tokens of literals, and comments with iType 1 (LITERAL/n/COMMENT).\n","[INFO] <GetDiffVocab> There are 35575 diff vocabulary tokens. (except '<pad>')\n","[INFO] <GetDiffVocab> The max diff length is 2706522 tokens. (hyperparameter: _DiffMaxLen_ = 600)\n","[INFO] <GetDiffDict> Create dictionary for 35576 diff vocabulary tokens. (with '<pad>')\n","[INFO] <GetDiffEmbed> Create pre-trained embedding weights with 35576 * 128 matrix.\n","[INFO] <DivideBeforeAfter> Divide diff code into BEFORE-version and AFTER-version code.\n","[INFO] <DivideBeforeAfter> The max length in BEFORE/AFTER-version code is 1938015 tokens. (hyperparameter: _TwinMaxLen_ = 800)\n","[INFO] <GetTwinMapping> Create 38041 feature data with 800 * 4 matrix.\n","[INFO] <GetTwinMapping> Create 38041 labels with 1 * 1 matrix.\n","[INFO] <UpdateTwinTokenTypes> Update 38041 feature data with 800 * 12 matrix.\n","[INFO] <GetCommitMsg> Load 38041 commit messages from .//temp//msgs.npy.\n","[INFO] <GetMsgVocab> There are 80830 commit message vocabulary tokens. (except '<pad>')\n","[INFO] <GetMsgVocab> The max msg length is 1434 tokens. (hyperparameter: _MsgMaxLen_ = 200)\n","[INFO] <GetMsgDict> Create dictionary for 80831 msg vocabulary tokens. (with '<pad>')\n","[INFO] <GetMsgEmbed> Create pre-trained embedding weights with 80831 * 128 matrix.\n","[INFO] <GetMsgMapping> Create 38041 feature data with 1 * 200 vector.\n","[INFO] <GetMsgMapping> Create 38041 labels with 1 * 1 matrix.\n","[INFO] <CombineTwinMsgs> Combine the twin props with the commit messages.\n"]}],"source":["# load data.\n","if (not os.path.exists(tempPath + '/data.npy')):  # | (not _DEBUG_)\n","    dataLoaded = ReadData()\n","else:\n","    dataLoaded = np.load(tempPath + '/data.npy', allow_pickle=True)\n","    print('[INFO] <ReadData> Load ' + str(len(dataLoaded)) + ' raw data from ' + tempPath + '/data.npy.')\n","\n","# get the diff file properties.\n","if (not os.path.exists(tempPath + '/props.npy')):\n","    diffProps = GetDiffProps(dataLoaded)\n","else:\n","    diffProps = np.load(tempPath + '/props.npy', allow_pickle=True)\n","    print('[INFO] <GetDiffProps> Load ' + str(len(diffProps)) + ' diff property data from ' + tempPath + '/props.npy.')\n","# maintain both the context and diff parts. Delete comments.\n","diffProps = ProcessTokens(diffProps, dType=0, cType=_CTYP_)\n","# normalize the tokens of identifiers (VARn/FUNCn), literals (LITERAL/n), and comments (none).\n","diffProps = AbstractTokens(diffProps, iType=_NIND_, lType=_NLIT_)\n","# get the diff token vocabulary.\n","diffVocab, _ = GetDiffVocab(diffProps)\n","# TS: Noting exceptional up to this point. :TS\n","# get the diff token dictionary.\n","diffDict = GetDiffDict(diffVocab)\n","# TS:\n","# diffDict contains encodings based on the input code. It's not extensible for new keyword tokens at this point. E.g  while is encoded as 1\n","# List of standard key-value pairs where k:str and v:int\n","# :TS\n","# get pre-trained weights for embedding layer.\n","twinPreWeights = GetDiffEmbed(diffDict, _TwinEmbedDim_)\n","# divide diff code into before/after-version code.\n","twinProps, twinMaxLen = DivideBeforeAfter(diffProps)\n","# TS: At this point twinProps are in the following format\n","# every item in twinProps is a list of 4 items\n","# [[beforestr1,...,beforestrN] , [beforestrType1,...,beforestrType1] , [afterstr1,...,afterstrN] , [afterstrType1,...,afterstrTypeN]]\n","# where beforestr1 can be e.g while and thus beforestrType1 will be 1 (1 == keyword)\n","# :TS\n","# get the max twin length.\n","twinMaxLen = _TwinMaxLen_ if (twinMaxLen > _DiffMaxLen_) else twinMaxLen\n","# get the mapping for feature data and labels.\n","twinData, twinLabels = GetTwinMapping(twinProps, twinMaxLen, diffDict)\n","# TS:\n","# At this point every item in twinData is a 2-d array:\n","# [[encodedtoken0B, encodedtokenType0B, encodedtoken0A, encodedTokenType0A], ..., [encodedtokenNB, encodedtokenTypeNB, encodedtokenNA, encodedTokenTypeNA]\n","# The lenghts are 'padded'\n","# :TS\n","# change the tokentypes into one-hot vector.\n","twinData = UpdateTwinTokenTypes(twinData)\n","# TS: Note that this encodes only token-types, the tokenID remains an integer value, which is fine, because then we have embeddings:\n","# e.g: [[ 1  1  0  0  0  0  1  1  0  0  0  0],\n","# [ 2  0  0  0  1  0  2  0  0  0  1  0],\n","# [ 3  0  0  0  1  0  3  0  0  0  1  0]....\n","# Each row is [tokenB 5x tokenTypeB.onehot tokenA 5x tokenTypeA.onehot]\n","# get the commit messages from data.\n","if (not os.path.exists(tempPath + '/msgs.npy')):\n","    commitMsgs = GetCommitMsgs(dataLoaded)\n","else:\n","    commitMsgs = np.load(tempPath + '/msgs.npy', allow_pickle=True)\n","    print('[INFO] <GetCommitMsg> Load ' + str(len(commitMsgs)) + ' commit messages from ' + tempPath + '/msgs.npy.')\n","# TS: Loaded messages are only key words + label, they are no longer structured like a normal message.\n","# e.g: [list(['dv', 'fix', 'null', 'pointer', 'derefer', 'due', 'ach', 'fix', 'part']) 1]\n","# :TS\n","# get the message token vocabulary.\n","msgVocab, msgMaxLen = GetMsgVocab(commitMsgs)\n","# get the max msg length.\n","msgMaxLen = _MsgMaxLen_ if (msgMaxLen > _MsgMaxLen_) else msgMaxLen\n","# get the msg token dictionary.\n","# TS: Every token found in messages is converted to dict (it's not just words, they are detected in more elaborate fashion, for example word 'architecture' becomes 'architectur')\n","# :TS\n","msgDict = GetMsgDict(msgVocab)\n","# TS: Straightforward \"token\" to \"int\"\n","# :TS\n","# get pre-trained weights for embedding layer.\n","msgPreWeights = GetMsgEmbed(msgDict, _MsgEmbedDim_)\n","# get the mapping for feature data and labels.\n","msgData, msgLabels = GetMsgMapping(commitMsgs, msgMaxLen, msgDict)\n","# TS:\n","# msgData items are just lists of tokens. (padded and not one-hot encoded, which is fine, because then we we have embeddings)\n","# [ 1  2  3  4  5  6  7  8  3  4  9 10 11 12 13 14 15  3  4  0  0  0 ... padding .... 0]\n","# :TS\n","# combine the twin data with the message data.\n","data, label = CombineTwinMsgs(twinData, msgData, twinLabels, msgLabels)\n","# TS:\n","# Every data item is the following:\n","#[[ 4  0  1  0  0  0  4  0  1  0  0  0  1]\n","# [21  0  0  0  1  0 21  0  0  0  1  0  2]\n","# ....\n","# [[tokenB, 5x one-hotTypeB, tokenA, 5x one-hotTypeA, msgToken]....\n","# Last column is simply message tokens encoded\n","# :TS"]},{"cell_type":"markdown","metadata":{"id":"iyixzxKZns3s"},"source":["# Network Training - LSTM (original)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hyFgMI7P2FaE","executionInfo":{"status":"aborted","timestamp":1750532034296,"user_tz":-120,"elapsed":167,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title Loading Dataset (Test and Train)\n","dataTrain, labelTrain, dataTest, labelTest = SplitData(data, label, 'test', rate=0.2)\n","print('[INFO] <demoTwin> Get ' + str(len(dataTrain)) + ' TRAIN data, ' + str(len(dataTest))\n","      + ' TEST data. (Total: ' + str(len(dataTrain) + len(dataTest)) + ')')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysKdqCacoZtt","executionInfo":{"status":"aborted","timestamp":1750532034299,"user_tz":-120,"elapsed":111,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title HyperParameters Overrides\n","# Overrides default hyperparameters\n","#Predefined variables\n","_TTransformerMaxEpoch_ = 500\n","_TTransformerPerEpoch_ = 1\n","_TTransformerJudEpoch_ = 1\n","_TTransformerBatchSz_ = 64\n","_TTransformerLearnRt_ = 0.0005\n","_TwinMaxLen_ = 800"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"v_8FAKkLXpZV","executionInfo":{"status":"aborted","timestamp":1750532034301,"user_tz":-120,"elapsed":3,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title class TwinRNN (nn.Module)\n","class TwinRNN(nn.Module):\n","    '''\n","    TwinRNN : convert a patch data into a predicted label.\n","    '''\n","\n","    def __init__(self, preWTwin, preWMsg, hidSizTwin=32, hidSizMsg=32, hidLayTwin=1, hidLayMsg=1):\n","        '''\n","        define each layer in the network model.\n","        :param preWTwin: tensor pre-trained weights for embedding layer for twin.\n","        :param preWMsg: tensor pre-trained weights for embedding layer for msg.\n","        :param hidSizTwin: node number in the hidden layer for twin.\n","        :param hidSizMsg: node number in the hidden layer for msg.\n","        :param hidLayTwin: number of hidden layer for twin.\n","        :param hidLayMsg: number of hidden layer for msg.\n","        '''\n","\n","        super(TwinRNN, self).__init__()\n","        # parameters.\n","        class_num = 2\n","    # twin.\n","        vSizTwin, emDimTwin = preWTwin.size()\n","        # Embedding Layer for twin.\n","        self.embedTwin = nn.Embedding(num_embeddings=vSizTwin, embedding_dim=emDimTwin)\n","        self.embedTwin.load_state_dict({'weight': preWTwin})\n","        self.embedTwin.weight.requires_grad = True\n","        # LSTM Layer for twin.\n","        if _DEBUG_: print(_TwinExtraDim_)\n","        self.lstmTwin = nn.LSTM(input_size=emDimTwin+_TwinExtraDim_, hidden_size=hidSizTwin, num_layers=hidLayTwin, bidirectional=True)\n","    # msg.\n","        vSizMsg, emDimMsg = preWMsg.size()\n","        # Embedding Layer for msg.\n","        self.embedMsg = nn.Embedding(num_embeddings=vSizMsg, embedding_dim=emDimMsg)\n","        self.embedMsg.load_state_dict({'weight': preWMsg})\n","        self.embedMsg.weight.requires_grad = True\n","        # LSTM Layer for msg.\n","        self.lstmMsg = nn.LSTM(input_size=emDimMsg, hidden_size=hidSizMsg, num_layers=hidLayMsg, bidirectional=True)\n","    # common.\n","        # Fully-Connected Layer.\n","        self.fc1 = nn.Linear(hidSizTwin * hidLayTwin * 4, hidSizTwin * hidLayTwin * 2)\n","        self.fc2 = nn.Linear(hidSizTwin * hidLayTwin * 2, class_num)\n","        self.fc3 = nn.Linear((hidSizTwin * hidLayTwin + hidSizMsg * hidLayMsg) * 2, hidSizTwin * hidLayTwin + hidSizMsg * hidLayMsg)\n","        self.fc4 = nn.Linear(hidSizTwin * hidLayTwin + hidSizMsg * hidLayMsg, class_num)\n","        # Softmax non-linearity.\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        '''\n","        convert inputs to predictions.\n","        :param x: input tensor. dimension: batch_size * twin_length * feature_dim.\n","        :return: self.softmax(final_out) - predictions.\n","        [[0.3, 0.7], [0.2, 0.8], ...]\n","        '''\n","\n","    # twin 1.\n","        xTwin = x[:, :_TwinMaxLen_, :6]\n","        # xTwin         batch_size * twin_length * feature_dim\n","        #print(xTwin.size())\n","        embedsTwin = self.embedTwin(xTwin[:, :, 0])\n","        # embedsTwin    batch_size * twin_length * embed_dim_twin\n","        features = xTwin[:, :, 1:]\n","        # features      batch_size * twin_length * _TwinExtraDim_\n","        inputsTwin = torch.cat((embedsTwin.float(), features.float()), 2)\n","        #print(inputsTwin.size())\n","        # inputsTwin    batch_size * twin_length * (embed_dim_twin + _TwinExtraDim_)\n","        inputsTwin = inputsTwin.permute(1, 0, 2)\n","        # inputsTwin    twin_length * batch_size * (embed_dim_twin + _TwinExtraDim_)\n","        lstm_out, (h_n, c_n) = self.lstmTwin(inputsTwin)\n","        # lstm_out      twin_length * batch_size * (hidden_size * direction_num)\n","        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n","        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n","        featMapTwin1 = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n","        # featMapTwin1   batch_size * (hidden_size * num_layers * direction_num)\n","        #print(featMapTwin1)\n","    # twin 2.\n","        xTwin = x[:, :_TwinMaxLen_, 6:-1]\n","        # xTwin         batch_size * twin_length * feature_dim\n","        #print(xTwin.size())\n","        embedsTwin = self.embedTwin(xTwin[:, :, 0])\n","        # embedsTwin    batch_size * twin_length * embed_dim_twin\n","        features = xTwin[:, :, 1:]\n","        # features      batch_size * twin_length * _TwinExtraDim_\n","        inputsTwin = torch.cat((embedsTwin.float(), features.float()), 2)\n","        #print(inputsTwin.size())\n","        # inputsTwin    batch_size * twin_length * (embed_dim_twin + _TwinExtraDim_)\n","        inputsTwin = inputsTwin.permute(1, 0, 2)\n","        # inputsTwin    twin_length * batch_size * (embed_dim_twin + _TwinExtraDim_)\n","        lstm_out, (h_n, c_n) = self.lstmTwin(inputsTwin)\n","        # lstm_out      twin_length * batch_size * (hidden_size * direction_num)\n","        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n","        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n","        featMapTwin2 = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n","        # featMapTwin2   batch_size * (hidden_size * num_layers * direction_num)\n","        #print(featMapTwin2)\n","    # msg.\n","        xMsg = x[:, :_MsgMaxLen_, -1]\n","        # xMsg          batch_size * msg_length * 1\n","        # print(xMsg.size())\n","        embedsMsg = self.embedMsg(xMsg)\n","        # embedsMsg     batch_size * msg_length * embed_dim_msg\n","        inputsMsg = embedsMsg.permute(1, 0, 2)\n","        # inputsMsg     msg_length * batch_size * (embed_dim_msg)\n","        lstm_out, (h_n, c_n) = self.lstmMsg(inputsMsg)\n","        # lstm_out      msg_length * batch_size * (hidden_size * direction_num)\n","        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n","        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n","        featMapMsg = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n","        # featMapMsg    batch_size * (hidden_size * num_layers * direction_num)\n","        #print(featMapMsg.size())\n","    # common.\n","        # combine twins.\n","        featMap = torch.cat((featMapTwin1, featMapTwin2), dim=1)\n","        # fc layers.\n","        featMap = self.fc1(featMap)\n","        if (0 == _TWIN_): # (only twins).\n","            final_out = self.fc2(featMap)\n","        elif (1 == _TWIN_): # (twins + msg).\n","            # combine twins + msg.\n","            featMap = torch.cat((featMap, featMapMsg), dim=1)\n","            # fc 2 layers.\n","            featMap = self.fc3(featMap)\n","            final_out = self.fc4(featMap)\n","        #print(final_out.size())\n","        return self.softmax(final_out)      # batch_size * class_num"]},{"cell_type":"markdown","metadata":{"id":"xH-Smy1_q8lS"},"source":["### Test/Train Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EkH0LBbhoKV-","executionInfo":{"status":"aborted","timestamp":1750532034303,"user_tz":-120,"elapsed":26978,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title TwinRNN Train Definition\n","def TwinRNNTrain(dTrain, lTrain, dValid, lValid, preWTwin, preWMsg, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n","    '''\n","    Train the TwinRNN model.\n","    :param dTrain: training data. [[n, ...], ...]\n","    :param lTrain: training label. [[n, ...], ...]\n","    :param dValid: validation data. [[n, ...], ...]\n","    :param lValid: validation label. [[n, ...], ...]\n","    :param preWDiff: pre-trained weights for diff embedding layer.\n","    :param preWMsg: pre-trained weights for msg embedding layer.\n","    :param batchsize: number of samples in a batch.\n","    :param learnRate: learning rate.\n","    :param dTest: test data. [[n, ...], ...]\n","    :param lTest: test label. [[n, ...], ...]\n","    :return: model - the TwinRNN model.\n","    '''\n","\n","    # get the mark of the test dataset.\n","    if dTest is None: dTest = []\n","    if lTest is None: lTest = []\n","    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n","\n","    # tensor data processing.\n","    xTrain = torch.from_numpy(dTrain).long().cuda()\n","    yTrain = torch.from_numpy(lTrain).long().cuda()\n","    xValid = torch.from_numpy(dValid).long().cuda()\n","    yValid = torch.from_numpy(lValid).long().cuda()\n","    if (markTest):\n","        xTest = torch.from_numpy(dTest).long().cuda()\n","        yTest = torch.from_numpy(lTest).long().cuda()\n","\n","    # batch size processing.\n","    train = torchdata.TensorDataset(xTrain, yTrain)\n","    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n","    valid = torchdata.TensorDataset(xValid, yValid)\n","    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n","    if (markTest):\n","        test = torchdata.TensorDataset(xTest, yTest)\n","        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n","\n","    # get training weights.\n","    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n","    weights = []\n","    for lb in range(2):\n","        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n","    lbWeights = torch.FloatTensor(weights).cuda()\n","\n","    # build the model of recurrent neural network.\n","    preWTwin = torch.from_numpy(preWTwin)\n","    preWMsg = torch.from_numpy(preWMsg)\n","    model = TwinRNN(preWTwin, preWMsg, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    print('[INFO] <TwinRNNTrain> ModelType: TwinRNN.')\n","    print('[INFO] <TwinRNNTrain> Code Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_TwinEmbedDim_, _TwinMaxLen_, _TRnnHidSiz_, _TRnnHidLay_))\n","    print('[INFO] <TwinRNNTrain> Msg  Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_MsgEmbedDim_, _MsgMaxLen_, _MRnnHidSiz_, _MRnnHidLay_))\n","    print('[INFO] <TwinRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d, JudEpoch: %d.' % (batchsize, learnRate, _TRnnMaxEpoch_, _TRnnPerEpoch_, _TRnnJudEpoch_))\n","    # optimizing with stochastic gradient descent.\n","    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n","    # seting loss function as mean squared error.\n","    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n","    # memory\n","    torch.backends.cudnn.benchmark = True\n","    torch.backends.cudnn.enabled = True\n","\n","    # run on each epoch.\n","    accList = [0]\n","    for epoch in range(_TRnnMaxEpoch_):\n","        # training phase.\n","        model.train()\n","        lossTrain = 0\n","        predictions = []\n","        labels = []\n","        for iter, (data, label) in enumerate(trainloader):\n","            # data conversion.\n","            data = data.to(device)\n","            label = label.contiguous().view(-1)\n","            label = label.to(device)\n","            # back propagation.\n","            optimizer.zero_grad()  # set the gradients to zero.\n","            yhat = model.forward(data)  # get output\n","            loss = criterion(yhat, label)\n","            loss.backward()\n","            optimizer.step()\n","            # statistic\n","            lossTrain += loss.item() * len(label)\n","            preds = yhat.max(1)[1]\n","            predictions.extend(preds.int().tolist())\n","            labels.extend(label.int().tolist())\n","            torch.cuda.empty_cache()\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        lossTrain /= len(dTrain)\n","        # train accuracy.\n","        accTrain = accuracy_score(labels, predictions) * 100\n","\n","        # validation phase.\n","        model.eval()\n","        predictions = []\n","        labels = []\n","        with torch.no_grad():\n","            for iter, (data, label) in enumerate(validloader):\n","                # data conversion.\n","                data = data.to(device)\n","                label = label.contiguous().view(-1)\n","                label = label.to(device)\n","                # forward propagation.\n","                yhat = model.forward(data)  # get output\n","                # statistic\n","                preds = yhat.max(1)[1]\n","                predictions.extend(preds.int().tolist())\n","                labels.extend(label.int().tolist())\n","                torch.cuda.empty_cache()\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # valid accuracy.\n","        accValid = accuracy_score(labels, predictions) * 100\n","        accList.append(accValid)\n","\n","        # testing phase.\n","        if (markTest):\n","            model.eval()\n","            predictions = []\n","            labels = []\n","            with torch.no_grad():\n","                for iter, (data, label) in enumerate(testloader):\n","                    # data conversion.\n","                    data = data.to(device)\n","                    label = label.contiguous().view(-1)\n","                    label = label.to(device)\n","                    # forward propagation.\n","                    yhat = model.forward(data)  # get output\n","                    # statistic\n","                    preds = yhat.max(1)[1]\n","                    predictions.extend(preds.int().tolist())\n","                    labels.extend(label.int().tolist())\n","                    torch.cuda.empty_cache()\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            # test accuracy.\n","            accTest = accuracy_score(labels, predictions) * 100\n","\n","        # output information.\n","        if (0 == (epoch + 1) % _TRnnPerEpoch_):\n","            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n","            if (markTest):\n","                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n","            print(strAcc)\n","        # save the best model.\n","        if (accList[-1] > max(accList[0:-1])):\n","            torch.save(model.state_dict(), tempPath + '/model_TwinRNN.pth')\n","        # stop judgement.\n","        if (epoch >= _TRnnJudEpoch_) and (accList[-1] < min(accList[-1-_TRnnJudEpoch_:-1])):\n","            break\n","\n","    # load best model.\n","    model.load_state_dict(torch.load(tempPath + '/model_TwinRNN.pth'))\n","    print('[INFO] <TwinRNNTrain> Finish training TwinRNN model. (Best model: ' + tempPath + '/model_TwinRNN.pth)')\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_eMUktlFpHWo","executionInfo":{"status":"aborted","timestamp":1750532034304,"user_tz":-120,"elapsed":26978,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title Test TwinRNN Definition\n","def TwinRNNTest(model, dTest, lTest, batchsize=64):\n","    '''\n","    Test the TwinRNN model.\n","    :param model: deep learning model.\n","    :param dTest: test data.\n","    :param lTest: test label.\n","    :param batchsize: number of samples in a batch\n","    :return: predictions - predicted labels. [[0], [1], ...]\n","             accuracy - the total test accuracy. numeric\n","    '''\n","\n","    # tensor data processing.\n","    xTest = torch.from_numpy(dTest).long().cuda()\n","    yTest = torch.from_numpy(lTest).long().cuda()\n","\n","    # batch size processing.\n","    test = torchdata.TensorDataset(xTest, yTest)\n","    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n","\n","    # load the model of recurrent neural network.\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    # testing phase.\n","    model.eval()\n","    predictions = []\n","    labels = []\n","    with torch.no_grad():\n","        for iter, (data, label) in enumerate(testloader):\n","            # data conversion.\n","            data = data.to(device)\n","            label = label.contiguous().view(-1)\n","            label = label.to(device)\n","            # forward propagation.\n","            yhat = model.forward(data)  # get output\n","            # statistic\n","            preds = yhat.max(1)[1]\n","            predictions.extend(preds.int().tolist())\n","            labels.extend(label.int().tolist())\n","            torch.cuda.empty_cache()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # testing accuracy.\n","    accuracy = accuracy_score(labels, predictions) * 100\n","    predictions = [[item] for item in predictions]\n","\n","    return predictions, accuracy"]},{"cell_type":"markdown","metadata":{"id":"rZOA2VzCoHZU"},"source":["## Run"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zFDxrR4Io4ms","executionInfo":{"status":"aborted","timestamp":1750532034332,"user_tz":-120,"elapsed":27004,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title RunRNN\n","def RunRNN():\n","  # TwinRNNTrain\n","  if (_MODEL_) & (os.path.exists(tempPath + '/model_TwinRNN.pth')):\n","      preWTwin = torch.from_numpy(twinPreWeights)\n","      preWMsg = torch.from_numpy(msgPreWeights)\n","      model = TwinRNN(preWTwin, preWMsg, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n","      model.load_state_dict(torch.load(tempPath + '/model_TwinRNN.pth'))\n","  else:\n","      model = TwinRNNTrain(dataTrain, labelTrain, dataTest, labelTest, preWTwin=twinPreWeights, preWMsg=msgPreWeights,\n","                            batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n","\n","  # TwinRNNTest\n","  predictions, accuracy = TwinRNNTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n","  _, confusion = OutputEval(predictions, labelTest, 'TwinRNN')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kJbOPnVadeJ","executionInfo":{"status":"aborted","timestamp":1750532034335,"user_tz":-120,"elapsed":26996,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# This code runs default setup by original authors of the article (with hyperparameter changes)\n","# The result is that we can easily achieve 82% accuracy on test dataset in <10 epochs.\n","# [Epoch 010] loss: 0.399, train acc: 91.910%, valid acc: 83.899%, test acc: 83.899%.\n","# This model uses: 1. Code before, 2. Code after, 3. Commit Messages to\n","RunRNN()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yf02gu3et-d6","executionInfo":{"status":"aborted","timestamp":1750532034337,"user_tz":-120,"elapsed":26990,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["# @title Cleanup\n","# Due to Python memory management this might be needed to clean gpu cache (even a few times)\n","# Please ignore Error 'model' is not defined\n","\n","\n","torch.cuda.empty_cache()\n","gc.collect()\n","# !nvidia-smi # Prints nvidia stats\n","\n","del model"]},{"cell_type":"markdown","source":["# Dataprep (both DS / BERT)\n"],"metadata":{"id":"XB96TvLu_FZF"}},{"cell_type":"code","source":["#@title Switch depending on deepseek or bert\n","from transformers import AutoTokenizer, AutoConfig, AutoModel, PreTrainedModel\n","from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n","tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n","#tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V2-Lite\", trust_remote_code=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blDfVjjO_PRz","outputId":"846546b1-5580-4b81-8ddd-302e868c3449","executionInfo":{"status":"ok","timestamp":1750533106282,"user_tz":-120,"elapsed":10748,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"id":"H8Hao-kd_JB_","outputId":"d36f3073-d5cc-4c7c-f2a2-222218f71fac","executionInfo":{"status":"ok","timestamp":1750532992553,"user_tz":-120,"elapsed":3954,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Using cached pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Using cached pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n","Installing collected packages: pyarrow\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 20.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pyarrow-20.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pyarrow"]},"id":"edc745be24654bce89509edddcc10c41"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install pyarrow==14.0.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"id":"x766RzgfENJ-","outputId":"2ca921ae-e465-45df-b425-54fa2e55dadc","executionInfo":{"status":"ok","timestamp":1750533007949,"user_tz":-120,"elapsed":3850,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyarrow==14.0.2\n","  Using cached pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.2) (1.26.4)\n","Using cached pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n","Installing collected packages: pyarrow\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 20.0.0\n","    Uninstalling pyarrow-20.0.0:\n","      Successfully uninstalled pyarrow-20.0.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datasets 3.6.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pyarrow-14.0.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pyarrow"]},"id":"51d5d99f755e4accb2545f592914accd"}},"metadata":{}}]},{"cell_type":"code","source":["from datasets import Dataset\n"],"metadata":{"id":"G2vC9pz0_Qs3","executionInfo":{"status":"ok","timestamp":1750533021233,"user_tz":-120,"elapsed":870,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# @title SplitData\n","def SplitDataRaw(data, rate=0.2):\n","    numData = len(data)\n","    num = math.floor(numData * rate)\n","\n","    dataList = list(range(numData))\n","    random.seed(10)\n","    random.shuffle(dataList)\n","\n","\n","    # split data.\n","    dset = data[dataList[0:num]]\n","    dsetRest = data[dataList[num:]]\n","\n","    return dset,dsetRest"],"metadata":{"id":"_bS6hzhuK_rv","executionInfo":{"status":"ok","timestamp":1750533106410,"user_tz":-120,"elapsed":4,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["dataLSTest,dataLSTrain = SplitDataRaw(dataLoaded)"],"metadata":{"id":"a2lKEM4TLMDw","executionInfo":{"status":"ok","timestamp":1750533106413,"user_tz":-120,"elapsed":1,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["dsTrainR = Dataset.from_dict({\n","    \"commit\": dataLSTrain[:, 0],\n","    \"code\": dataLSTrain[:, 1],\n","    \"label\": dataLSTrain[:, 2]\n","})\n","\n","dsTestR = Dataset.from_dict({\n","    \"commit\": dataLSTest[:, 0],\n","    \"code\": dataLSTest[:, 1],\n","    \"label\": dataLSTest[:, 2]\n","})"],"metadata":{"id":"UlOZu051EaTW","executionInfo":{"status":"ok","timestamp":1750533107660,"user_tz":-120,"elapsed":1245,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n","from transformers import Trainer, TrainingArguments"],"metadata":{"id":"qKkXVkHXAXkt","executionInfo":{"status":"ok","timestamp":1750533111254,"user_tz":-120,"elapsed":3592,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# Deepseek"],"metadata":{"id":"6ilDxpn-Nl9h"}},{"cell_type":"code","source":["def join_and_tokenize(sample):\n","  joined_text = f\"COMMIT: {sample['commit']} CODE: {sample['code']}\"\n","  sample['text'] = joined_text\n","  return tokenizer(\n","        sample[\"text\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=800\n","    )"],"metadata":{"id":"sq25uWfBN3Xl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dsTrain = dsTrainR.map(join_and_tokenize, batched=False).remove_columns([\"commit\",\"code\"])\n","dsTest = dsTestR.map(join_and_tokenize, batched=False).remove_columns([\"commit\",\"code\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["91cb984d41f84ccf9dd429e596cc6a9e","040b5326b88748ea94e1547c72eac420","1b6d5271171f4b7d9e3116991a043f42","11ce2ddb93d94ec38cb4b3f4fb4ab229","951ab643f28b4c499e0c7dfc775b7d12","e89ece786e674efba80944e084107e26","2670b1bf75834664a5ac6bf3a03649ff","c3eaac7222d940518a74f9d2db4d8bc0","df33a24b77a444d39d00db2fd53afb4e","3da7c741cc1743edb75911d43d842993","fe522c7d515d4bd5a8f15c70fda477a4","aacad80d2e0b4e43b76553008ef90f2a","cdef000b56ba45f98ec4598201dd2d70","41a11a6bc8ce4df8a1584d00a50942e2","7f86ecfde6c24e64adac6a42e7d442a4","622d1b43cd544a1db413338be9cb3c72","6c181606614b45a2b9246dc71acc9d34","4d92bff7c5004d33bee84a5e63e3edf5","cb7a133f84fd46a88394ac329efaf023","240bc7a2b3b041bbb1541d1c671a7a5e","1468c2c529e749418c163fd02f9d4f70","f5584363917840b795f599a772fd12aa"]},"id":"ZLgbwRNJ_T5p","outputId":"d6ef53c6-e4e5-44ae-b51b-184a5e612795"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/30433 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91cb984d41f84ccf9dd429e596cc6a9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/7608 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aacad80d2e0b4e43b76553008ef90f2a"}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","\n","device = \"cpu\""],"metadata":{"id":"3-h0ygYjFq8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(example):\n","  encoded_example =  tokenizer(example['text'], truncation=True)\n","  # example['input_ids'] = encoded_example['input_ids']\n","  # example['attention_mask'] = encoded_example['attention_mask']\n","  return encoded_example"],"metadata":{"id":"R6spjXqOF6ou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install flash_attn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMAz95t1g8Us","outputId":"1f508a5d-46d4-41d9-d22b-1ede49219cce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flash_attn\n","  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash_attn) (2.4.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash_attn) (0.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash_attn) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash_attn) (1.3.0)\n","Building wheels for collected packages: flash_attn\n","  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash_attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187788763 sha256=a8719a86d0cd12a105739151f9e45ac8c2ecb840a83ed7a080a88d1645a98440\n","  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\n","Successfully built flash_attn\n","Installing collected packages: flash_attn\n","Successfully installed flash_attn-2.7.4.post1\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoConfig, AutoModel, PreTrainedModel\n","from transformers.models.llama.modeling_llama import LlamaModel  # DeepSeek is LLaMA-based\n","import torch.nn as nn\n","\n","class DeepseekV2LiteForSequenceClassification(PreTrainedModel):\n","    def __init__(self, config, num_labels=2):\n","        super().__init__(config)\n","        self.model = AutoModel.from_config(config)  # Load base model from config\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.last_hidden_state[:, -1]  # Take last token for classification\n","        logits = self.classifier(pooled_output)\n","        if labels is not None:\n","            loss = F.cross_entropy(logits, labels)\n","            return {\"loss\": loss, \"logits\": logits}\n","        return {\"logits\": logits}\n","\n","# Load DeepSeek config\n","config = AutoConfig.from_pretrained(\"deepseek-ai/DeepSeek-V2-Lite\", trust_remote_code=True)\n","\n","# Inject number of labels (if not already in config)\n","config.num_labels = 2\n","\n","# Create model\n","model = DeepseekV2LiteForSequenceClassification(config)\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V2-Lite\", trust_remote_code=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["896fa4fccc0142768abca36302fa4169","943cda20adae48dc949a72b48d1a4b45","3e9812f750db4a37b1746ecd0279bf98","d0dacf90928244ea81750a4978c1e769","6887caaea9eb4f9b83d90c9e3db597c8","2424dc68b4004341afdafcf59d462103","b3ba6359ad984b2b8c294f720fddc577","56d17bf1a43d48b2b53807046603a1d6","821fa077fb684f979d386c57d9bd318c","1b23ac6979d240eaa548086e80c2b3c4","412ba5ac39ce49fbad8c613de49bc17b","647c573096564b45a29530de28e93925","a94c72811c4c4b09b32fab230599e012","1a69e3e765a3494db3dd9fbcf21e55b9","4bd61096e7b045d7ac299bf13d230129","6c64834ca0914387b34c38dfd87c71dc","2213a6e586784ffd9b925f3e62302ea9","b0ce189bd063496a934377524ca6897a","556e3f5d20b74b65bd4601c03e15a034","fa49bbfbedcc47a99a5293b50211eedf","aaa247bb790048b2b7401117c3d99b38","1e496485cdc048df862fd50b130bef0e","9e52625c876d423fbb2a081a67dc4f90","e4cae2638ace422a91f814389dfbc2a8","8f5014f012654d29aa47b4983e4e09e0","5df681f467544406900146c830ffe038","d13893ad015d4be5a82a2065844fe875","d995ac22361d4ec9a09b430dd52fcb9e","11c44a07fae449fba3a73a4e72747f30","b0429f68beba4b2cb3cf352ce32d68b8","870aebdd5f674ab8a744c134fa259819","4a5bb0a58a6047d0a93cfa89d4e486d8","4e0235b2f8084137adcd41f914e27348"]},"id":"nSKBlTojP54q","outputId":"b9e46465-1efa-4288-dbbf-887aaf4af18a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"896fa4fccc0142768abca36302fa4169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["configuration_deepseek.py:   0%|          | 0.00/10.3k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647c573096564b45a29530de28e93925"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite:\n","- configuration_deepseek.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"]},{"output_type":"display_data","data":{"text/plain":["modeling_deepseek.py:   0%|          | 0.00/78.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e52625c876d423fbb2a081a67dc4f90"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite:\n","- modeling_deepseek.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"id":"0vWwOYdrF-nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"],"metadata":{"id":"0ymf60NWq-i1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    labels = np.array(labels)\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_score(labels, predictions)\n","    np.savetxt(\"reals.txt\", labels, fmt=\"%s\", delimiter=\"\\n\")\n","    np.savetxt(\"predicted.txt\", predictions, fmt=\"%s\", delimiter=\"\\n\")\n","    # Calculate F1 score\n","    f1 = f1_score(labels, predictions, average='weighted')  # Use 'weighted' for multiclass; change as needed for binary\n","\n","    metrics = {\n","        'accuracy': accuracy,\n","        'f1_score': f1\n","    }\n","\n","    return metrics\n","\n","# TrainingArguments setup\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    report_to=\"none\",\n","    eval_strategy=\"steps\",\n","    eval_steps=100,\n","    no_cuda=True,\n","    use_cpu=True\n",")\n","\n","# Trainer setup\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dsTrain,\n","    eval_dataset=dsTest,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics  # Add compute_metrics here\n",")\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", message=\"Can't initialize NVML\")\n","\n","# Start training\n","trainer.train()\n"],"metadata":{"id":"nO8QH_6dGHWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AKTjjqpxGHYf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSUhYbkN_o38"},"source":["# Pretrained bert"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"6yDzYLnSAAJ0","executionInfo":{"status":"ok","timestamp":1750533111267,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n","from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","source":["def join_and_tokenize(sample):\n","  joined_text = f\"COMMIT: {sample['commit']} CODE: {sample['code']}\"\n","  sample['text'] = joined_text\n","  return tokenizer(\n","        sample[\"text\"],\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=512\n","    )"],"metadata":{"id":"oXEGpbKpN8Ic","executionInfo":{"status":"ok","timestamp":1750533111273,"user_tz":-120,"elapsed":3,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["dsTrain = dsTrainR.map(join_and_tokenize, batched=False).remove_columns([\"commit\",\"code\"])\n","dsTest = dsTestR.map(join_and_tokenize, batched=False).remove_columns([\"commit\",\"code\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["e9f74d74999f48c2ab263d59a7a6c015","241ea13971f644279c89e552099e4b36","e06e003c078e4d04b2307ad0673d07ae","c3de7d5fbb0a4c3e9dbe61a0df5e769b","5c363d8085dd48c1a58304154ba98302","bf9cf381f9634f158e9cc44219e79628","a4b287fbab8941b6bd342a6a657c0f16","92be6250d59d4245b55eaeaa16ac283c","9413285dfb2b43c4b3525edfa8d73beb","b19aef4e4a2f468aacf8ec97c8129d3c","3f1fd2cd4daf49c4ae22a5da5153e9c7","b99bbb54f9da43238c6132e803d16e47","b5388668109f40b0b8ab9ef3e869e579","d9138abbae9d4d11abdd4ee10410f8d1","bd0d2fd645e247b2a8923e4c5fa0b441","a1ead30ee2b34740801f5014a69f8690","172a52ad37cc4f43995ccd8dbdb74332","b8fbacce4b9443a0bec6626b477a0b7b","f0a76983ccf9420d8d820331da7c98ed","9df10f746c2142eab01f804df3717c67","d10160f4e1b9418b8f9e87f13523add5","8a05563af6f24b7c80f69a29b5895cfd"]},"id":"gx2IkUOeN8vw","outputId":"f7ad25fb-6c52-4ee7-e051-045d6cdb473f","executionInfo":{"status":"ok","timestamp":1750533302022,"user_tz":-120,"elapsed":190707,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/30433 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f74d74999f48c2ab263d59a7a6c015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/7608 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b99bbb54f9da43238c6132e803d16e47"}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","\n","device = \"cpu\""],"metadata":{"id":"Blh6JpxQMSfQ","executionInfo":{"status":"ok","timestamp":1750533302037,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YYQroiLiABUF","outputId":"8415a51d-0a2a-4d6d-b8d0-b47b24c623bb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n","Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n","You are not authenticated with the Hugging Face Hub in this notebook.\n","If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# import torch\n","# from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n","# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQPAJ9JGACy2"},"outputs":[],"source":["# def preprocess_function(example):\n","#   encoded_example =  tokenizer(example['text'], truncation=True)\n","#   # example['input_ids'] = encoded_example['input_ids']\n","#   # example['attention_mask'] = encoded_example['attention_mask']\n","#   return encoded_example"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b595bdcd677d4f0ba91e15e30f6e1e6c","6fab03833cb84f179a5317d4481b86c4","e390001c9e374619b77fe15bb158d84a","6f4844186243477b8c39942d4839c51f","67ae91f73e6443bca1c55bc4ee559ad0","ae6488596b2a42639a5d4d833533dc75","aae749477f6f40b9938a1b9cdab166e7","b28dc90e3b8d4b76b4ccd5a67d7cbb5e","73abd90a326a4ff692d8a39eb57e595c","61c069436b5a44b1aadc950210c7f174","1b443fecd83f478e9de52cc4ae744393"]},"id":"572rBC3SAEHI","outputId":"11af3a5b-545c-46fc-987c-7d7d9561c12c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b595bdcd677d4f0ba91e15e30f6e1e6c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/38041 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# ds_dataset = ds.map(preprocess_function, batched=True, batch_size=64)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"FWK1rbk9BmI2","executionInfo":{"status":"ok","timestamp":1750533302043,"user_tz":-120,"elapsed":2,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["422308152f334ccca1c5b561001ee1f9","da9512195cd842d596066637e6fbfa71","850811e03d574f8ea15cd0d6167d8a32","ea92a5cacecf451b8a16485716f17f46","c5ca54eebcdf4bda8c01b608ac93b66a","e32142035c6741e4b9aa60075c726753","0bd16ce6239c4130a62fa8a458d7ec37","3d6182a638bd4d84b34efac2052cce5e","09c728f001624016b5e415ae5484e998","4bac6211d7534f23bf549e1e9c5591d8","255dd84f7f9f4b35844636840e8ec342"]},"id":"ugARR9FDAN7d","outputId":"8dd7907d-e6db-4f6b-89df-683fde724105"},"outputs":[{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"422308152f334ccca1c5b561001ee1f9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nwKrBQC-AP_F"},"outputs":[],"source":["#df_dataset_split = ds_dataset.train_test_split(test_size=0.1)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"pT6SORQtASLN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d0cc3700-983e-47e8-d21f-04484845524b","executionInfo":{"status":"ok","timestamp":1750533306276,"user_tz":-120,"elapsed":4183,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 20.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install evaluate -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"id":"XEdjJeEoATdN","outputId":"46cda1ac-152f-4232-88bf-7bf8b971b65e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='952' max='952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [952/952 19:00:04, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>No log</td>\n","      <td>0.440882</td>\n","      <td>0.797581</td>\n","      <td>0.785000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>No log</td>\n","      <td>0.294921</td>\n","      <td>0.878812</td>\n","      <td>0.876854</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>No log</td>\n","      <td>0.274253</td>\n","      <td>0.890510</td>\n","      <td>0.890020</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>No log</td>\n","      <td>0.271847</td>\n","      <td>0.895636</td>\n","      <td>0.893091</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.363000</td>\n","      <td>0.275881</td>\n","      <td>0.893139</td>\n","      <td>0.893446</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.363000</td>\n","      <td>0.250598</td>\n","      <td>0.897871</td>\n","      <td>0.897459</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.363000</td>\n","      <td>0.239236</td>\n","      <td>0.902340</td>\n","      <td>0.901306</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.363000</td>\n","      <td>0.245184</td>\n","      <td>0.898922</td>\n","      <td>0.898906</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.363000</td>\n","      <td>0.246024</td>\n","      <td>0.898791</td>\n","      <td>0.898966</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=952, training_loss=0.30656792937206623, metrics={'train_runtime': 68453.991, 'train_samples_per_second': 0.445, 'train_steps_per_second': 0.014, 'total_flos': 8007258747770880.0, 'train_loss': 0.30656792937206623, 'epoch': 1.0})"]},"metadata":{},"execution_count":45}],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    labels = np.array(labels)\n","    predictions = np.argmax(predictions, axis=1)\n","    accuracy = accuracy_score(labels, predictions)\n","    np.savetxt(\"reals.txt\", labels, fmt=\"%s\", delimiter=\"\\n\")\n","    np.savetxt(\"predicted.txt\", predictions, fmt=\"%s\", delimiter=\"\\n\")\n","    # Calculate F1 score\n","    f1 = f1_score(labels, predictions, average='weighted')  # Use 'weighted' for multiclass; change as needed for binary\n","\n","    metrics = {\n","        'accuracy': accuracy,\n","        'f1_score': f1\n","    }\n","\n","    return metrics\n","\n","# TrainingArguments setup\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    report_to=\"none\",\n","    eval_strategy=\"steps\",\n","    eval_steps=100,\n","    use_cpu=True\n",")\n","\n","# Trainer setup\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dsTrain,\n","    eval_dataset=dsTest,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,  # Add compute_metrics here\n",")\n","\n","# Start training\n","#trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FuUK1TZiOhU"},"outputs":[],"source":["# @title OutputEval\n","def OutputEval(predictions, labels, method=''):\n","    '''\n","    Output the evaluation results.\n","    :param predictions: predicted labels. [[0], [1], ...]\n","    :param labels: ground truth labels. [[1], [1], ...]\n","    :param method: method name. string\n","    :return: accuracy - the total accuracy. numeric\n","             confusion - confusion matrix [[1000, 23], [12, 500]]\n","    '''\n","\n","    # evaluate the predictions with gold labels, and get accuracy and confusion matrix.\n","    def Evaluation(predictions, labels):\n","\n","        # parameter settings.\n","        D = len(labels)\n","        cls = 2\n","\n","        # get confusion matrix.\n","        confusion = np.zeros((cls, cls))\n","        for ind in range(D):\n","            nRow = int(predictions[ind][0])\n","            nCol = int(labels[ind][0])\n","            confusion[nRow][nCol] += 1\n","\n","        # get accuracy.\n","        accuracy = 0\n","        for ind in range(cls):\n","            accuracy += confusion[ind][ind]\n","        accuracy /= D\n","\n","        return accuracy, confusion\n","\n","    # get accuracy and confusion matrix.\n","    accuracy, confusion = Evaluation(predictions, labels)\n","    precision = confusion[1][1] / (confusion[1][0] + confusion[1][1]) if (confusion[1][0] + confusion[1][1]) else 0\n","    recall = confusion[1][1] / (confusion[0][1] + confusion[1][1]) if (confusion[0][1] + confusion[1][1]) else 0\n","    F1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n","\n","    # output on screen and to file.\n","    print('       -------------------------------------------')\n","    print('       method           :  ' +  method) if len(method) else print('', end='')\n","    print('       accuracy  (ACC)  :  %.3f%%' % (accuracy * 100))\n","    print('       precision (P)    :  %.3f%%' % (precision * 100))\n","    print('       recall    (R)    :  %.3f%%' % (recall * 100))\n","    print('       F1 score  (F1)   :  %.3f' % (F1))\n","    print('       fall-out  (FPR)  :  %.3f%%' % (confusion[1][0] * 100 / (confusion[1][0] + confusion[0][0])))\n","    print('       miss rate (FNR)  :  %.3f%%' % (confusion[0][1] * 100 / (confusion[0][1] + confusion[1][1])))\n","    print('       confusion matrix :      (actual)')\n","    print('                           Neg         Pos')\n","    print('       (predicted) Neg     %-5d(TN)   %-5d(FN)' % (confusion[0][0], confusion[0][1]))\n","    print('                   Pos     %-5d(FP)   %-5d(TP)' % (confusion[1][0], confusion[1][1]))\n","    print('       -------------------------------------------')\n","\n","    return accuracy, confusion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"efZOFc7piQNK","outputId":"590c0be5-4c85-479b-ea50-6fcdae6f3aaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["       -------------------------------------------\n","       accuracy  (ACC)  :  89.879%\n","       precision (P)    :  84.041%\n","       recall    (R)    :  85.191%\n","       F1 score  (F1)   :  0.846\n","       fall-out  (FPR)  :  7.847%\n","       miss rate (FNR)  :  14.809%\n","       confusion matrix :      (actual)\n","                           Neg         Pos\n","       (predicted) Neg     4721 (TN)   368  (FN)\n","                   Pos     402  (FP)   2117 (TP)\n","       -------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.8987907465825447,\n"," array([[4721.,  368.],\n","        [ 402., 2117.]]))"]},"metadata":{},"execution_count":47}],"source":["reals = np.loadtxt(\"reals.txt\").reshape(-1,1)\n","preds = np.loadtxt(\"predicted.txt\").reshape(-1,1)\n","\n","OutputEval(preds,reals)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"bypTuRXGm2v9","outputId":"ac280b8e-9484-427a-ea63-9bd1a743ce2d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='31' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 31/238 06:49 < 47:05, 0.07 it/s]\n","    </div>\n","    "]},"metadata":{}}],"source":["import time\n","\n","start = time.perf_counter()  # Start time\n","test_preds = trainer.predict(dsTest)\n","end = time.perf_counter()  # End time\n","execution_time = (end - start)\n","\n","print(f\"Execution time: {execution_time:.3f} s\")\n"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xujeP9wruV5o","outputId":"4fbaf3fb-bc8b-48d8-f0ef-f8af86434369","executionInfo":{"status":"ok","timestamp":1750533529059,"user_tz":-120,"elapsed":77,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'label': 0,\n"," 'text': 'COMMIT: [\\'\\\\n\\', \\'    add --no-embedded-cert and --embed-leak-only\\\\n\\', \\'\\\\n\\'] CODE: [[\\' \\\\tif (ret)\\\\n\\', \\' \\\\t    hx509_err(context, 1, ret, \"hx509_certs_find\");\\\\n\\', \\'     }\\\\n\\', \\'+    if (!opt->embedded_certs_flag)\\\\n\\', \\'+\\\\tflags |= HX509_CMS_SIGNATURE_NO_CERTS;\\\\n\\', \\'+    if (opt->embed_leaf_only_flag)\\\\n\\', \\'+\\\\tflags |= HX509_CMS_SIGNATURE_LEAF_ONLY;\\\\n\\', \\' \\\\n\\', \\'     ret = rk_undumpdata(infile, &p, &sz);\\\\n\\', \\'     if (ret)\\\\n\\', \\'\\\\n\\']]',\n"," 'input_ids': [0,\n","  10370,\n","  36548,\n","  35,\n","  47052,\n","  37457,\n","  282,\n","  3934,\n","  128,\n","  1437,\n","  1437,\n","  1437,\n","  1606,\n","  480,\n","  2362,\n","  12,\n","  35804,\n","  17452,\n","  12,\n","  25782,\n","  8,\n","  480,\n","  35804,\n","  12,\n","  459,\n","  677,\n","  12,\n","  8338,\n","  37457,\n","  282,\n","  3934,\n","  128,\n","  37457,\n","  282,\n","  44403,\n","  37604,\n","  35,\n","  48395,\n","  108,\n","  44128,\n","  48609,\n","  36,\n","  4903,\n","  49394,\n","  282,\n","  3934,\n","  128,\n","  44128,\n","  90,\n","  1437,\n","  1437,\n","  1437,\n","  1368,\n","  1178,\n","  37046,\n","  1215,\n","  14385,\n","  1640,\n","  46796,\n","  6,\n","  112,\n","  6,\n","  5494,\n","  6,\n","  22,\n","  298,\n","  1178,\n","  37046,\n","  1215,\n","  25782,\n","  29,\n","  1215,\n","  26559,\n","  45751,\n","  37457,\n","  282,\n","  3934,\n","  128,\n","  1437,\n","  1437,\n","  1437,\n","  1437,\n","  35524,\n","  37457,\n","  282,\n","  3934,\n","  128,\n","  2744,\n","  1437,\n","  1437,\n","  1437,\n","  114,\n","  48209,\n","  19693,\n","  46613,\n","  35804,\n","  17452,\n","  1215,\n","  25782,\n","  29,\n","  1215,\n","  30160,\n","  49394,\n","  282,\n","  3934,\n","  128,\n","  2744,\n","  37457,\n","  90,\n","  46760,\n","  1721,\n","  5214,\n","  289,\n","  1000,\n","  37046,\n","  1215,\n","  347,\n","  6222,\n","  1215,\n","  42101,\n","  35576,\n","  1215,\n","  13449,\n","  1215,\n","  347,\n","  2076,\n","  2685,\n","  131,\n","  37457,\n","  282,\n","  3934,\n","  128,\n","  2744,\n","  1437,\n","  1437,\n","  1437,\n","  114,\n","  36,\n","  19693,\n","  46613,\n","  35804,\n","  1215,\n","  24999,\n","  1215,\n","  8338,\n","  1215,\n","  30160,\n","  49394,\n","  282,\n","  3934,\n","  128,\n","  2744,\n","  37457,\n","  90,\n","  46760,\n","  1721,\n","  5214,\n","  289,\n","  1000,\n","  37046,\n","  1215,\n","  347,\n","  6222,\n","  1215,\n","  42101,\n","  35576,\n","  1215,\n","  3850,\n","  8573,\n","  1215,\n","  2191,\n","  14079,\n","  131,\n","  37457,\n","  282,\n","  3934,\n","  128,\n","  44128,\n","  282,\n","  3934,\n","  128,\n","  1437,\n","  1437,\n","  1437,\n","  1437,\n","  5494,\n","  5457,\n","  910,\n","  330,\n","  1215,\n","  3194,\n","  7198,\n","  23687,\n","  1640,\n","  179,\n","  21710,\n","  6,\n","  359,\n","  642,\n","  6,\n","  359,\n","  29,\n","  329,\n","  4397,\n","  37457,\n","  282,\n","  3934,\n","  128,\n","  1437,\n","  1437,\n","  1437,\n","  1437,\n","  114,\n","  36,\n","  4903,\n","  49394,\n","  282,\n","  3934,\n","  128,\n","  37457,\n","  282,\n","  108,\n","  48392,\n","  2,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1],\n"," 'attention_mask': [1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0]}"]},"metadata":{},"execution_count":41}],"source":["dsTest[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZ8B5i8Zj-7c","outputId":"c51e05e7-191a-4552-f854-944853fcf3ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"]}],"source":["!pip install torchinfo\n","from torchinfo import summary"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_2rVmAXthwI","outputId":"bd618470-3e80-4177-898e-c0c96587eedf"},"outputs":[{"name":"stdout","output_type":"stream","text":["==============================================================================================================\n","Layer (type:depth-idx)                                       Output Shape              Param #\n","==============================================================================================================\n","RobertaForSequenceClassification                             [1, 2]                    --\n","├─RobertaModel: 1-1                                          [1, 6, 768]               --\n","│    └─RobertaEmbeddings: 2-1                                [1, 6, 768]               --\n","│    │    └─Embedding: 3-1                                   [1, 6, 768]               38,603,520\n","│    │    └─Embedding: 3-2                                   [1, 6, 768]               768\n","│    │    └─Embedding: 3-3                                   [1, 6, 768]               394,752\n","│    │    └─LayerNorm: 3-4                                   [1, 6, 768]               1,536\n","│    │    └─Dropout: 3-5                                     [1, 6, 768]               --\n","│    └─RobertaEncoder: 2-2                                   [1, 6, 768]               --\n","│    │    └─ModuleList: 3-6                                  --                        85,054,464\n","├─RobertaClassificationHead: 1-2                             [1, 2]                    --\n","│    └─Dropout: 2-3                                          [1, 768]                  --\n","│    └─Linear: 2-4                                           [1, 768]                  590,592\n","│    └─Dropout: 2-5                                          [1, 768]                  --\n","│    └─Linear: 2-6                                           [1, 2]                    1,538\n","==============================================================================================================\n","Total params: 124,647,170\n","Trainable params: 124,647,170\n","Non-trainable params: 0\n","Total mult-adds (M): 124.65\n","==============================================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 5.02\n","Params size (MB): 498.59\n","Estimated Total Size (MB): 503.61\n","============================================================================================================== RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]}],"source":["s = summary(model, input_size=(1, 6), dtypes=[torch.long])\n","print(s,model)  # Adjust input size to match model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxJVlIX3waMQ","outputId":"7915fd0a-8779-4499-e484-d9f121883d45"},"outputs":[{"data":{"text/plain":["('RobertaTrainedModel/tokenizer_config.json',\n"," 'RobertaTrainedModel/special_tokens_map.json',\n"," 'RobertaTrainedModel/vocab.json',\n"," 'RobertaTrainedModel/merges.txt',\n"," 'RobertaTrainedModel/added_tokens.json')"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["# Define save path\n","save_directory = \"RobertaTrainedModel\"\n","\n","# Save the model\n","trainer.model.save_pretrained(save_directory)\n","\n","# Save the tokenizer (important for reloading)\n","tokenizer.save_pretrained(save_directory)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nC5--RAMw0Tg"},"outputs":[],"source":["# Load model and tokenizer from saved directory\n","model = RobertaForSequenceClassification.from_pretrained(\"RobertaTrainedModel\")\n","tokenizer = RobertaTokenizer.from_pretrained(\"RobertaTrainedModel\")\n","\n","# Now you can use the model for predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"uFDkdRLvAUil","outputId":"ecb2d03c-db54-4457-dac6-38f5adc4bafc"},"outputs":[{"ename":"NameError","evalue":"name 'evaluate' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-e5bdbd78fd8a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m results = precision_metric.compute(references=torch.tensor([0.0, 1.0]),\n\u001b[1;32m      3\u001b[0m                                   predictions=torch.tensor([0.0, 1.0]))\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"]}],"source":["precision_metric = evaluate.load(\"f1\")\n","results = precision_metric.compute(references=torch.tensor([0.0, 1.0]),\n","                                  predictions=torch.tensor([0.0, 1.0]))\n","print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xnRpq1UAVYd"},"outputs":[],"source":["predictions = [0, 2, 1, 0, 0, 1]\n","references = [0, 1, 2, 0, 1, 2]\n","results = precision_metric.compute(predictions=predictions, references=references, average='macro')\n"]},{"cell_type":"code","source":["#@title Loading Model from checkpoint\n","from transformers import AutoModelForSequenceClassification\n","\n","# Path to your saved checkpoint directory\n","checkpoint_path = \"./results/checkpoint-4760\"\n","\n","# Load the model from the checkpoint\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    checkpoint_path, num_labels=2\n",")"],"metadata":{"id":"QBIBYoIw7-tp","executionInfo":{"status":"ok","timestamp":1750533718367,"user_tz":-120,"elapsed":66,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["example = dsTest[0]\n","text = example[\"text\"]\n","label = example[\"label\"]\n","print(text, label)  # optional\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JBGJh3P8pYa","executionInfo":{"status":"ok","timestamp":1750533840446,"user_tz":-120,"elapsed":26,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"734ffb5a-fc91-4235-b565-ac78e6417bf6"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["COMMIT: ['\\n', '    add --no-embedded-cert and --embed-leak-only\\n', '\\n'] CODE: [[' \\tif (ret)\\n', ' \\t    hx509_err(context, 1, ret, \"hx509_certs_find\");\\n', '     }\\n', '+    if (!opt->embedded_certs_flag)\\n', '+\\tflags |= HX509_CMS_SIGNATURE_NO_CERTS;\\n', '+    if (opt->embed_leaf_only_flag)\\n', '+\\tflags |= HX509_CMS_SIGNATURE_LEAF_ONLY;\\n', ' \\n', '     ret = rk_undumpdata(infile, &p, &sz);\\n', '     if (ret)\\n', '\\n']] 0\n"]}]},{"cell_type":"code","source":["inputs = tokenizer(\n","    text,\n","    return_tensors=\"pt\",\n","    truncation=True,\n","    padding=True\n",")\n","\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    logits = outputs.logits\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","    probs = torch.softmax(logits, dim=1).squeeze().tolist()\n","\n","print(\"Predicted class:\", predicted_class)\n","print(\"Probabilities per class:\", probs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54qC9wxk8ruE","executionInfo":{"status":"ok","timestamp":1750533849409,"user_tz":-120,"elapsed":612,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"ca22f600-eee5-4f07-b1fc-62b6aacfac26"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: 0\n","Probabilities per class: [0.9984032511711121, 0.00159674440510571]\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","preds = []\n","for example in tqdm(dsTest):\n","    inputs = tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","        preds.append(int(torch.argmax(logits, dim=1)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kiLfJkYS8uoZ","executionInfo":{"status":"ok","timestamp":1750536621775,"user_tz":-120,"elapsed":2689041,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"210cfc1c-e4a7-49f5-f93a-7e6a7dfd3c62"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 7608/7608 [44:49<00:00,  2.83it/s]\n"]}]},{"cell_type":"code","source":["def Predict(model,text):\n","  from tqdm import tqdm\n","  inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n","  with torch.no_grad():\n","      logits = model(**inputs).logits\n","      return (int(torch.argmax(logits, dim=1)))\n"],"metadata":{"id":"yx6c5LDGMzSX","executionInfo":{"status":"ok","timestamp":1750538126686,"user_tz":-120,"elapsed":20,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["OutputEval(np.array(preds).reshape(-1, 1),np.array(dsTest[:]['label']).reshape(-1, 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kWKknd8e81fD","executionInfo":{"status":"ok","timestamp":1750536968872,"user_tz":-120,"elapsed":2865,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"8047be8d-3139-4790-9ee8-fff853930977"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["       -------------------------------------------\n","       accuracy  (ACC)  :  90.300%\n","       precision (P)    :  84.815%\n","       recall    (R)    :  85.634%\n","       F1 score  (F1)   :  0.852\n","       fall-out  (FPR)  :  7.437%\n","       miss rate (FNR)  :  14.366%\n","       confusion matrix :      (actual)\n","                           Neg         Pos\n","       (predicted) Neg     4742 (TN)   357  (FN)\n","                   Pos     381  (FP)   2128 (TP)\n","       -------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.9029968454258676,\n"," array([[4742.,  357.],\n","        [ 381., 2128.]]))"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["## Integrated Gradients for Roberta"],"metadata":{"id":"aOVry1MX9Ptt"}},{"cell_type":"code","source":["!pip install captum\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oaYq5R759ItJ","executionInfo":{"status":"ok","timestamp":1750536981394,"user_tz":-120,"elapsed":2205,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"3a97bc10-e470-4103-b80f-d978f01bc677"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.8.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from captum) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from captum) (24.1)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from captum) (2.4.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (2024.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (10.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->captum) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->captum) (1.3.0)\n"]}]},{"cell_type":"code","source":["#@title Pick a sample\n","analyzedSampleIdx = 7\n","eX = dsTest[analyzedSampleIdx]['text']\n","eXlabel = dsTest[analyzedSampleIdx]['label']"],"metadata":{"id":"iqvhFgf2Lfl_","executionInfo":{"status":"ok","timestamp":1750538660676,"user_tz":-120,"elapsed":24,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":["text = eX\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","input_ids = inputs.input_ids\n","attention_mask = inputs.attention_mask\n","baseline_ids = torch.full_like(input_ids, tokenizer.pad_token_id)\n","\n","def forward_func(input_ids, attention_mask):\n","    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","    print(outputs.logits[:, 1].dtype)\n","    return outputs.logits[:, 1]  # float tensor\n","\n"],"metadata":{"id":"CApARGa2AH_F","executionInfo":{"status":"ok","timestamp":1750538661130,"user_tz":-120,"elapsed":22,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":135,"outputs":[]},{"cell_type":"code","source":["# 1. Get the embedding layer and the baseline embedding\n","embedding_layer = model.roberta.embeddings.word_embeddings\n","\n","# Get embeddings for inputs and baseline\n","input_embeds = embedding_layer(input_ids)                        # (1, seq_len, hidden_size)\n","baseline_embeds = embedding_layer(baseline_ids)                  # same shape\n","\n","# 2. Define a forward func that accepts *embeddings*, not token ids\n","def forward_func_embeds(inputs_embeds, attention_mask):\n","    outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n","    return outputs.logits[:, 1]\n","\n","# 3. Compute attributions on embeddings\n","from captum.attr import IntegratedGradients\n","\n","ig = IntegratedGradients(forward_func_embeds)\n","\n","attributions = ig.attribute(\n","    inputs=input_embeds,               # float embeddings\n","    baselines=baseline_embeds,         # float embeddings\n","    additional_forward_args=(attention_mask,),\n","    target=None,\n","    n_steps=50\n",")\n","# 4. Sum across embedding dimension\n","attributions = attributions.sum(dim=-1).squeeze(0)\n"],"metadata":{"id":"oKc09TpGAKjk","executionInfo":{"status":"ok","timestamp":1750538672665,"user_tz":-120,"elapsed":10640,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}}},"execution_count":136,"outputs":[]},{"cell_type":"code","source":["input_ids.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ueSW3n-jPB8h","executionInfo":{"status":"ok","timestamp":1750538700466,"user_tz":-120,"elapsed":12,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"f299ac39-1db6-4b4c-c8eb-ab0d1e134cf7"},"execution_count":140,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 165])"]},"metadata":{},"execution_count":140}]},{"cell_type":"code","source":["tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","attr_scores = attributions.detach().cpu().tolist()\n","\n","# Show each token's attribution\n","for token, score in zip(tokens, attr_scores):\n","    print(f\"{token:15} {score:.4f}\")\n","print(len(tokens), len(input_ids), len(attr_scores))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-PH0H-jATyc","executionInfo":{"status":"ok","timestamp":1750538678669,"user_tz":-120,"elapsed":61,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"97c1e665-20e7-495c-bd17-a6efafbd0caf"},"execution_count":138,"outputs":[{"output_type":"stream","name":"stdout","text":["<s>             -0.0006\n","COM             0.0050\n","MIT             -0.0040\n",":               -0.0145\n","Ġ['             -0.0110\n","\\               0.0082\n","n               0.0074\n","',              0.0080\n","Ġ'              0.0037\n","Ġ               -0.0700\n","Ġ               -0.0762\n","Ġ               -0.1755\n","ĠInitial        0.1541\n","ize             0.0069\n","Ġd              0.0040\n","_               -0.0035\n","result          -0.0250\n","Ġto             -0.0228\n","ĠNULL           -0.0053\n","Ġon             0.0191\n","Ġc              -0.0397\n","tor             -0.1404\n","\\               0.0536\n","n               0.0828\n","',              0.2127\n","Ġ'              0.4795\n","\\               -0.0009\n","n               0.0048\n","']              -0.0919\n","ĠCODE           0.0022\n",":               -0.0007\n","Ġ[[             -0.0267\n","'               0.0211\n","Ġ{\\             0.0081\n","n               0.0211\n","',              0.0347\n","Ġ'              0.0159\n","Ġ               -0.0133\n","Ġ               -0.0012\n","Ġ               0.0046\n","Ġ               0.0032\n","Ġ               -0.0024\n","Ġ               0.0032\n","Ġset            -0.0258\n","Arg             0.0022\n","Pref            -0.0060\n","ix              0.0018\n","(\"              0.0077\n","remote          -0.0204\n","\"               0.0238\n","+               -0.0195\n","suff            0.0194\n","ix              0.0120\n",");              -0.0242\n","\\               0.0222\n","n               0.0158\n","',              0.0206\n","Ġ'              0.0346\n","Ġ               -0.0119\n","Ġ               -0.0021\n","Ġ               -0.0015\n","Ġ               -0.0047\n","Ġ               -0.0085\n","Ġ               -0.0197\n","Ġbuild          -0.0086\n","(               -0.0415\n","get             -0.0097\n","Arg             -0.0289\n","(\"              -0.0363\n","connection      -0.0198\n","-               0.0264\n","string          0.0207\n","\")              0.0183\n",");              -0.0182\n","\\               0.0358\n","n               0.0482\n","',              0.1285\n","Ġ'              0.5217\n","+               -0.1911\n","Ġ               -0.0528\n","Ġ               0.0084\n","Ġ               0.0103\n","Ġ               -0.0090\n","Ġ               -0.0125\n","Ġthis           0.0143\n","->              0.0205\n","d               -0.0002\n","_               0.0067\n","result          -0.0040\n","Ġ=              -0.0325\n","ĠNULL           0.0801\n",";               0.0129\n","\\               0.0487\n","n               0.0297\n","',              0.0162\n","Ġ'              -0.0058\n","Ġ               -0.0063\n","Ġ               -0.0036\n","Ġ               -0.0044\n","Ġ               -0.0050\n","Ġ               -0.0071\n","Ġ               -0.0081\n","Ġthis           -0.0026\n","->              -0.0058\n","d               -0.0067\n","_               -0.0055\n","d               -0.0006\n","ns              0.0125\n","sec             0.0060\n","Ġ=              -0.0439\n","Ġmust           -0.0075\n","Do              0.0002\n","(\"              0.0275\n","d               0.0037\n","ns              0.0119\n","sec             -0.0028\n","\");             0.0166\n","\\               0.0250\n","n               0.0155\n","',              0.0122\n","Ġ'              0.0002\n","Ġ               0.0039\n","Ġ               0.0017\n","Ġ               0.0067\n","Ġ               -0.0011\n","Ġ               -0.0008\n","Ġ               0.0056\n","Ġthis           0.0135\n","->              0.0191\n","d               0.0044\n","_               0.0024\n","index           0.0305\n","Ġ=              0.0217\n","Ġ-              0.0305\n","1               -0.0044\n",";               0.0080\n","\\               0.0193\n","n               0.0267\n","',              0.0310\n","Ġ'              0.0142\n","Ġ               0.0181\n","Ġ               0.0136\n","Ġ               0.0038\n","Ġ               0.0043\n","Ġ               0.0031\n","Ġ               0.0040\n","Ġthis           -0.0034\n","->              0.0015\n","d               0.0082\n","_               0.0060\n","tr              0.0156\n","x               0.0139\n","id              0.0119\n","Ġ=              0.0012\n","Ġ0              0.0037\n",";               0.0146\n","\\               0.0405\n","n               0.0431\n","',              0.0553\n","Ġ'              0.1651\n","\\               0.0485\n","n               0.0224\n","'               -0.0109\n","]]              -0.0354\n","</s>            0.0700\n","165 13 165\n"]}]},{"cell_type":"code","source":["from termcolor import colored\n","\n","for token, score in zip(tokens, attr_scores):\n","    color = \"green\" if score > 0 else \"red\"\n","    print(colored(f\"{token}\", color=color), end=\" \")\n","print()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8ZDr8iaAVEU","executionInfo":{"status":"ok","timestamp":1750538577915,"user_tz":-120,"elapsed":16,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"4a463d3d-8e67-42e9-9803-6427dea64cbd"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> COM MIT : Ġ[' \\ n ', Ġ' Ġ Ġ Ġ ĠInitial ize Ġd _ result Ġto ĠNULL Ġon Ġc tor \\ n ', Ġ' \\ n '] ĠCODE : Ġ[[ ' Ġ{\\ n ', Ġ' Ġ Ġ Ġ Ġ Ġ Ġ Ġset Arg Pref ix (\" remote \" + suff ix ); \\ n ', Ġ' Ġ Ġ Ġ Ġ Ġ Ġ Ġbuild ( get Arg (\" connection - string \") ); \\ n ', Ġ' + Ġ Ġ Ġ Ġ Ġ Ġthis -> d _ result Ġ= ĠNULL ; \\ n ', Ġ' Ġ Ġ Ġ Ġ Ġ Ġ Ġthis -> d _ d ns sec Ġ= Ġmust Do (\" d ns sec \"); \\ n ', Ġ' Ġ Ġ Ġ Ġ Ġ Ġ Ġthis -> d _ index Ġ= Ġ- 1 ; \\ n ', Ġ' Ġ Ġ Ġ Ġ Ġ Ġ Ġthis -> d _ tr x id Ġ= Ġ0 ; \\ n ', Ġ' \\ n ' ]] </s> \n"]}]},{"cell_type":"code","source":["from IPython.display import display, HTML\n","import numpy as np\n","\n","# Decode the tokens\n","tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n","\n","# Normalize attributions for coloring\n","attr_scores = attributions.detach().cpu().numpy()\n","attr_scores = attr_scores / (np.max(np.abs(attr_scores)) + 1e-8)\n","\n","# Color map: red for negative, green for positive\n","def color_token(token, score):\n","    color = \"rgba({},{},0,0.6)\".format(\n","        int(255 * max(-score, 0)),   # Red intensity if score < 0\n","        int(255 * max(score, 0))     # Green intensity if score > 0\n","    )\n","    return f\"<span style='background-color: {color}; padding:2px; margin:1px;'>{token}</span>\"\n","\n","# Wrap all tokens\n","html = \" \".join(color_token(tok, score) for tok, score in zip(tokens, attr_scores))\n","\n","# Display in the notebook\n","display(HTML(html))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"qgoKiqgrKNAm","executionInfo":{"status":"ok","timestamp":1750538577979,"user_tz":-120,"elapsed":62,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"55869f1f-61c4-4c1b-f1bf-4997305c7847"},"execution_count":124,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'><s></span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>COM</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>MIT</span> <span style='background-color: rgba(7,0,0,0.6); padding:2px; margin:1px;'>:</span> <span style='background-color: rgba(5,0,0,0.6); padding:2px; margin:1px;'>Ġ['</span> <span style='background-color: rgba(0,4,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(34,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(37,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(85,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,75,0,0.6); padding:2px; margin:1px;'>ĠInitial</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>ize</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġd</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>_</span> <span style='background-color: rgba(12,0,0,0.6); padding:2px; margin:1px;'>result</span> <span style='background-color: rgba(11,0,0,0.6); padding:2px; margin:1px;'>Ġto</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>ĠNULL</span> <span style='background-color: rgba(0,9,0,0.6); padding:2px; margin:1px;'>Ġon</span> <span style='background-color: rgba(19,0,0,0.6); padding:2px; margin:1px;'>Ġc</span> <span style='background-color: rgba(68,0,0,0.6); padding:2px; margin:1px;'>tor</span> <span style='background-color: rgba(0,26,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,40,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,103,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,234,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(44,0,0,0.6); padding:2px; margin:1px;'>']</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>ĠCODE</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>:</span> <span style='background-color: rgba(13,0,0,0.6); padding:2px; margin:1px;'>Ġ[[</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>'</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>Ġ{\\</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,16,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,7,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(6,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(12,0,0,0.6); padding:2px; margin:1px;'>Ġset</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Arg</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>Pref</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>ix</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>(\"</span> <span style='background-color: rgba(9,0,0,0.6); padding:2px; margin:1px;'>remote</span> <span style='background-color: rgba(0,11,0,0.6); padding:2px; margin:1px;'>\"</span> <span style='background-color: rgba(9,0,0,0.6); padding:2px; margin:1px;'>+</span> <span style='background-color: rgba(0,9,0,0.6); padding:2px; margin:1px;'>suff</span> <span style='background-color: rgba(0,5,0,0.6); padding:2px; margin:1px;'>ix</span> <span style='background-color: rgba(11,0,0,0.6); padding:2px; margin:1px;'>);</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,7,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,16,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(5,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(4,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(9,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(4,0,0,0.6); padding:2px; margin:1px;'>Ġbuild</span> <span style='background-color: rgba(20,0,0,0.6); padding:2px; margin:1px;'>(</span> <span style='background-color: rgba(4,0,0,0.6); padding:2px; margin:1px;'>get</span> <span style='background-color: rgba(14,0,0,0.6); padding:2px; margin:1px;'>Arg</span> <span style='background-color: rgba(17,0,0,0.6); padding:2px; margin:1px;'>(\"</span> <span style='background-color: rgba(9,0,0,0.6); padding:2px; margin:1px;'>connection</span> <span style='background-color: rgba(0,12,0,0.6); padding:2px; margin:1px;'>-</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>string</span> <span style='background-color: rgba(0,8,0,0.6); padding:2px; margin:1px;'>\")</span> <span style='background-color: rgba(8,0,0,0.6); padding:2px; margin:1px;'>);</span> <span style='background-color: rgba(0,17,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,23,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,62,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,255,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(93,0,0,0.6); padding:2px; margin:1px;'>+</span> <span style='background-color: rgba(25,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,4,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,5,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(4,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(6,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,6,0,0.6); padding:2px; margin:1px;'>Ġthis</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>-></span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>d</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>_</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>result</span> <span style='background-color: rgba(15,0,0,0.6); padding:2px; margin:1px;'>Ġ=</span> <span style='background-color: rgba(0,39,0,0.6); padding:2px; margin:1px;'>ĠNULL</span> <span style='background-color: rgba(0,6,0,0.6); padding:2px; margin:1px;'>;</span> <span style='background-color: rgba(0,23,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,14,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,7,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(3,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(3,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(3,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>Ġthis</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>-></span> <span style='background-color: rgba(3,0,0,0.6); padding:2px; margin:1px;'>d</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>_</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>d</span> <span style='background-color: rgba(0,6,0,0.6); padding:2px; margin:1px;'>ns</span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>sec</span> <span style='background-color: rgba(21,0,0,0.6); padding:2px; margin:1px;'>Ġ=</span> <span style='background-color: rgba(3,0,0,0.6); padding:2px; margin:1px;'>Ġmust</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Do</span> <span style='background-color: rgba(0,13,0,0.6); padding:2px; margin:1px;'>(\"</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>d</span> <span style='background-color: rgba(0,5,0,0.6); padding:2px; margin:1px;'>ns</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>sec</span> <span style='background-color: rgba(0,8,0,0.6); padding:2px; margin:1px;'>\");</span> <span style='background-color: rgba(0,12,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,7,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,5,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,6,0,0.6); padding:2px; margin:1px;'>Ġthis</span> <span style='background-color: rgba(0,9,0,0.6); padding:2px; margin:1px;'>-></span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>d</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>_</span> <span style='background-color: rgba(0,14,0,0.6); padding:2px; margin:1px;'>index</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>Ġ=</span> <span style='background-color: rgba(0,14,0,0.6); padding:2px; margin:1px;'>Ġ-</span> <span style='background-color: rgba(2,0,0,0.6); padding:2px; margin:1px;'>1</span> <span style='background-color: rgba(0,3,0,0.6); padding:2px; margin:1px;'>;</span> <span style='background-color: rgba(0,9,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,13,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,15,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,6,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(0,8,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,6,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ</span> <span style='background-color: rgba(1,0,0,0.6); padding:2px; margin:1px;'>Ġthis</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>-></span> <span style='background-color: rgba(0,4,0,0.6); padding:2px; margin:1px;'>d</span> <span style='background-color: rgba(0,2,0,0.6); padding:2px; margin:1px;'>_</span> <span style='background-color: rgba(0,7,0,0.6); padding:2px; margin:1px;'>tr</span> <span style='background-color: rgba(0,6,0,0.6); padding:2px; margin:1px;'>x</span> <span style='background-color: rgba(0,5,0,0.6); padding:2px; margin:1px;'>id</span> <span style='background-color: rgba(0,0,0,0.6); padding:2px; margin:1px;'>Ġ=</span> <span style='background-color: rgba(0,1,0,0.6); padding:2px; margin:1px;'>Ġ0</span> <span style='background-color: rgba(0,7,0,0.6); padding:2px; margin:1px;'>;</span> <span style='background-color: rgba(0,19,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,21,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(0,27,0,0.6); padding:2px; margin:1px;'>',</span> <span style='background-color: rgba(0,80,0,0.6); padding:2px; margin:1px;'>Ġ'</span> <span style='background-color: rgba(0,23,0,0.6); padding:2px; margin:1px;'>\\</span> <span style='background-color: rgba(0,10,0,0.6); padding:2px; margin:1px;'>n</span> <span style='background-color: rgba(5,0,0,0.6); padding:2px; margin:1px;'>'</span> <span style='background-color: rgba(17,0,0,0.6); padding:2px; margin:1px;'>]]</span> <span style='background-color: rgba(0,34,0,0.6); padding:2px; margin:1px;'></s></span>"]},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Normalize scores to [-1, 1] range for coloring\n","attr_scores = np.array(attr_scores)\n","attr_scores /= np.max(np.abs(attr_scores))\n","\n","# Helper to get color intensity\n","def color_for_score(score):\n","    # Red for negative, green for positive\n","    intensity = int(255 * abs(score))\n","    if score > 0:\n","        return f\"rgba(0, 200, 0, {abs(score):.5f})\"\n","    else:\n","        return f\"rgba(200, 0, 0, {abs(score):.5f})\"\n","\n","# Build HTML string\n","html_content = \"<html><body style='font-family:sans-serif;line-height:2;'>\"\n","for token, score in zip(tokens, attr_scores):\n","    color = color_for_score(score)\n","    html_content += f\"<span style='background:{color};padding:2px;margin:1px;'>{token}</span> \"\n","html_content += \"</body></html>\"\n","\n","# Save to file\n","with open(\"attributions.html\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(html_content)\n","\n","print(\"✅ Saved to attributions.html\")\n","\n","import imgkit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9SVfXOLnAlDu","executionInfo":{"status":"ok","timestamp":1750538578002,"user_tz":-120,"elapsed":20,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"05701f3f-88d9-4471-a5de-81709e7c2c18"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Saved to attributions.html\n"]}]},{"cell_type":"code","source":["!pip install imgkit\n","!sudo apt-get install wkhtmltopdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtBaS3vMAnCl","executionInfo":{"status":"ok","timestamp":1750538582057,"user_tz":-120,"elapsed":4051,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"bdff5651-1ac9-45f0-f6ab-6b6b0043547b"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imgkit in /usr/local/lib/python3.10/dist-packages (1.2.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgkit) (1.16.0)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","wkhtmltopdf is already the newest version (0.12.6-2).\n","0 upgraded, 0 newly installed, 0 to remove and 301 not upgraded.\n"]}]},{"cell_type":"code","source":["\n","# Convert to PNG\n","imgkit.from_file(\"attributions.html\", \"attributions.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6KAI_mWLQKv","executionInfo":{"status":"ok","timestamp":1750538582357,"user_tz":-120,"elapsed":265,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"9f95dcd6-5a27-40af-b036-57a61f7588a7"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'\n","Loading page (1/2)\n","[>                                                           ] 0%\r[===================>                                        ] 33%\r[============================================================] 100%\rRendering (2/2)                                                    \n","[>                                                           ] 0%\r[===============>                                            ] 25%\r[============================================================] 100%\rDone                                                               \n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","source":["from IPython.display import display, HTML\n","import numpy as np\n","import re\n","\n","def generate_attr_table_html(header, tokens, token_ids, attr_scores, title=\"Attribution Scores\", threshold=0.05):\n","    # Normalize attribution scores to [-1, 1]\n","    max_abs = max(abs(min(attr_scores)), max(attr_scores)) or 1.0\n","    norm_scores = [s / max_abs for s in attr_scores]\n","\n","    def lerp_color(c1, c2, t):\n","        return tuple(int(c1[i] + (c2[i] - c1[i]) * t) for i in range(3))\n","\n","    red_rgb = (255, 77, 77)    # strong red\n","    green_rgb = (77, 255, 77)  # strong green\n","    white_rgb = (255, 255, 255) # white\n","\n","    def score_to_color(score):\n","        if score < 0:\n","            t = -score\n","            return lerp_color(white_rgb, red_rgb, t)\n","        else:\n","            t = score\n","            return lerp_color(white_rgb, green_rgb, t)\n","\n","    def rgb_to_css(rgb):\n","        return f'rgb{rgb}'\n","\n","    clean_tokens = [re.sub(r'^Ġ', ' ', t) for t in tokens]\n","\n","    # Mark tokens as important or excluded by threshold\n","    important_flags = [abs(s) >= threshold for s in attr_scores]\n","\n","    rows_html = \"\"\n","    excluded_run_count = 0\n","\n","    for i in range(len(tokens)):\n","        if important_flags[i]:\n","            # If we had an excluded run before this important token, add a note row\n","            if excluded_run_count > 0:\n","                rows_html += f\"\"\"\n","                <tr>\n","                    <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                        {excluded_run_count} token{'s' if excluded_run_count > 1 else ''} excluded by threshold (|score| < {threshold})\n","                    </td>\n","                </tr>\"\"\"\n","                excluded_run_count = 0\n","\n","            color = rgb_to_css(score_to_color(norm_scores[i]))\n","            rows_html += f\"\"\"\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">{clean_tokens[i]}</td>\n","                <td style=\"padding:6px; text-align:center;\">{token_ids[i]}</td>\n","                <td title=\"{attr_scores[i]:.5f}\"\n","                    style=\"padding:6px; background-color: {color}; text-align:center; font-family: monospace;\">\n","                    {attr_scores[i]:.5f}\n","                </td>\n","            </tr>\"\"\"\n","        else:\n","            excluded_run_count += 1\n","\n","    # If tokens ended with excluded run, add note row\n","    if excluded_run_count > 0:\n","        rows_html += f\"\"\"\n","        <tr>\n","            <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                {excluded_run_count} token{'s' if excluded_run_count > 1 else ''} excluded by threshold (|score| < {threshold})\n","            </td>\n","        </tr>\"\"\"\n","\n","    table_html = f\"\"\"\n","    <div style=\"margin-bottom: 12px; font-size: 18px; font-weight: bold; font-family: Arial, sans-serif;\">\n","        {title}\n","    </div>\n","    <table style=\"border-collapse: collapse; width: 100%; max-width: 600px; font-size: 14px;\">\n","      <thead>\n","        <tr>\n","          {''.join(f'<th style=\"border-bottom: 2px solid #555; padding:8px; text-align:left;\">{col}</th>' for col in header)}\n","        </tr>\n","      </thead>\n","      <tbody>\n","        {rows_html}\n","      </tbody>\n","    </table>\n","    \"\"\"\n","    return table_html\n","\n","# Example usage: replace these variables as you want\n","special_tokens = {\"<s>\", \"</s>\"}\n","filtered = [(t, tid, s) for t, tid, s in zip(tokens, input_ids[0], attr_scores) if t not in special_tokens]\n","tokens_f, token_ids_f, attr_scores_f = zip(*filtered)\n","\n","header = [\"Token\", \"Token ID\", \"Attribution\"]\n","\n","html_table = generate_attr_table_html(\n","    header, tokens_f, token_ids_f, attr_scores_f,\n","    title=f'Integrated Gradients | Sample {analyzedSampleIdx} (Real: {eXlabel}; Predicted: {Predict(model,eX)})',\n","    threshold=0.05\n",")\n","\n","display(HTML(html_table))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":819},"id":"5d6L_ZBuKUE_","executionInfo":{"status":"ok","timestamp":1750539389183,"user_tz":-120,"elapsed":311,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"d832cdf8-1647-4120-897f-02bbf6df4009"},"execution_count":147,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div style=\"margin-bottom: 12px; font-size: 18px; font-weight: bold; font-family: Arial, sans-serif;\">\n","        Integrated Gradients | Sample 7 (Real: 1; Predicted: 1)\n","    </div>\n","    <table style=\"border-collapse: collapse; width: 100%; max-width: 600px; font-size: 14px;\">\n","      <thead>\n","        <tr>\n","          <th style=\"border-bottom: 2px solid #555; padding:8px; text-align:left;\">Token</th><th style=\"border-bottom: 2px solid #555; padding:8px; text-align:left;\">Token ID</th><th style=\"border-bottom: 2px solid #555; padding:8px; text-align:left;\">Attribution</th>\n","        </tr>\n","      </thead>\n","      <tbody>\n","        \n","                <tr>\n","                    <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                        8 tokens excluded by threshold (|score| < 0.05)\n","                    </td>\n","                </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> </td>\n","                <td style=\"padding:6px; text-align:center;\">1437</td>\n","                <td title=\"-0.06999\" \n","                    style=\"padding:6px; background-color: rgb(255, 231, 231); text-align:center; font-family: monospace;\">\n","                    -0.06999\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> </td>\n","                <td style=\"padding:6px; text-align:center;\">1437</td>\n","                <td title=\"-0.07622\" \n","                    style=\"padding:6px; background-color: rgb(255, 228, 228); text-align:center; font-family: monospace;\">\n","                    -0.07622\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> </td>\n","                <td style=\"padding:6px; text-align:center;\">1437</td>\n","                <td title=\"-0.17546\" \n","                    style=\"padding:6px; background-color: rgb(255, 195, 195); text-align:center; font-family: monospace;\">\n","                    -0.17546\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> Initial</td>\n","                <td style=\"padding:6px; text-align:center;\">24685</td>\n","                <td title=\"0.15406\" \n","                    style=\"padding:6px; background-color: rgb(202, 255, 202); text-align:center; font-family: monospace;\">\n","                    0.15406\n","                </td>\n","            </tr>\n","                <tr>\n","                    <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                        8 tokens excluded by threshold (|score| < 0.05)\n","                    </td>\n","                </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">tor</td>\n","                <td style=\"padding:6px; text-align:center;\">10321</td>\n","                <td title=\"-0.14036\" \n","                    style=\"padding:6px; background-color: rgb(255, 207, 207); text-align:center; font-family: monospace;\">\n","                    -0.14036\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">\\</td>\n","                <td style=\"padding:6px; text-align:center;\">37457</td>\n","                <td title=\"0.05356\" \n","                    style=\"padding:6px; background-color: rgb(236, 255, 236); text-align:center; font-family: monospace;\">\n","                    0.05356\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">n</td>\n","                <td style=\"padding:6px; text-align:center;\">282</td>\n","                <td title=\"0.08279\" \n","                    style=\"padding:6px; background-color: rgb(226, 255, 226); text-align:center; font-family: monospace;\">\n","                    0.08279\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">',</td>\n","                <td style=\"padding:6px; text-align:center;\">3934</td>\n","                <td title=\"0.21271\" \n","                    style=\"padding:6px; background-color: rgb(182, 255, 182); text-align:center; font-family: monospace;\">\n","                    0.21271\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> '</td>\n","                <td style=\"padding:6px; text-align:center;\">128</td>\n","                <td title=\"0.47954\" \n","                    style=\"padding:6px; background-color: rgb(91, 255, 91); text-align:center; font-family: monospace;\">\n","                    0.47954\n","                </td>\n","            </tr>\n","                <tr>\n","                    <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                        2 tokens excluded by threshold (|score| < 0.05)\n","                    </td>\n","                </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">']</td>\n","                <td style=\"padding:6px; text-align:center;\">44403</td>\n","                <td title=\"-0.09190\" \n","                    style=\"padding:6px; background-color: rgb(255, 223, 223); text-align:center; font-family: monospace;\">\n","                    -0.09190\n","                </td>\n","            </tr>\n","                <tr>\n","                    <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                        47 tokens excluded by threshold (|score| < 0.05)\n","                    </td>\n","                </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">',</td>\n","                <td style=\"padding:6px; text-align:center;\">3934</td>\n","                <td title=\"0.12846\" \n","                    style=\"padding:6px; background-color: rgb(211, 255, 211); text-align:center; font-family: monospace;\">\n","                    0.12846\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> '</td>\n","                <td style=\"padding:6px; text-align:center;\">128</td>\n","                <td title=\"0.52167\" \n","                    style=\"padding:6px; background-color: rgb(77, 255, 77); text-align:center; font-family: monospace;\">\n","                    0.52167\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">+</td>\n","                <td style=\"padding:6px; text-align:center;\">2744</td>\n","                <td title=\"-0.19107\" \n","                    style=\"padding:6px; background-color: rgb(255, 189, 189); text-align:center; font-family: monospace;\">\n","                    -0.19107\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> </td>\n","                <td style=\"padding:6px; text-align:center;\">1437</td>\n","                <td title=\"-0.05281\" \n","                    style=\"padding:6px; background-color: rgb(255, 236, 236); text-align:center; font-family: monospace;\">\n","                    -0.05281\n","                </td>\n","            </tr>\n","                <tr>\n","                    <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                        10 tokens excluded by threshold (|score| < 0.05)\n","                    </td>\n","                </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> NULL</td>\n","                <td style=\"padding:6px; text-align:center;\">48955</td>\n","                <td title=\"0.08007\" \n","                    style=\"padding:6px; background-color: rgb(227, 255, 227); text-align:center; font-family: monospace;\">\n","                    0.08007\n","                </td>\n","            </tr>\n","                <tr>\n","                    <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                        67 tokens excluded by threshold (|score| < 0.05)\n","                    </td>\n","                </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\">',</td>\n","                <td style=\"padding:6px; text-align:center;\">3934</td>\n","                <td title=\"0.05526\" \n","                    style=\"padding:6px; background-color: rgb(236, 255, 236); text-align:center; font-family: monospace;\">\n","                    0.05526\n","                </td>\n","            </tr>\n","            <tr>\n","                <td style=\"padding:6px; font-family: monospace;\"> '</td>\n","                <td style=\"padding:6px; text-align:center;\">128</td>\n","                <td title=\"0.16510\" \n","                    style=\"padding:6px; background-color: rgb(198, 255, 198); text-align:center; font-family: monospace;\">\n","                    0.16510\n","                </td>\n","            </tr>\n","        <tr>\n","            <td colspan=\"3\" style=\"padding:8px; font-style: italic; color: #666; text-align: center;\">\n","                4 tokens excluded by threshold (|score| < 0.05)\n","            </td>\n","        </tr>\n","      </tbody>\n","    </table>\n","    "]},"metadata":{}}]},{"cell_type":"code","source":["with open(\"attributionsv2.html\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(html_table)\n","# Convert to PNG\n","options = {\n","    \"format\": \"png\",\n","    \"width\": 400,          # force the image width\n","    \"disable-smart-width\": \"\",\n","    \"encoding\": \"utf-8\",\n","    \"quiet\": \"\"\n","}\n","\n","imgkit.from_file(\"attributionsv2.html\", \"attributionsv2.png\",options)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P94pL7riMXIP","executionInfo":{"status":"ok","timestamp":1750541978881,"user_tz":-120,"elapsed":302,"user":{"displayName":"Tomasz Skowron","userId":"13919330185531057464"}},"outputId":"7deba76d-4ffa-4582-bb4a-50aee7bda3b9"},"execution_count":151,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":151}]},{"cell_type":"markdown","metadata":{"id":"tAZw01C7sHxK"},"source":["# Network Training - Transformer Proposal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0MYHGj_sP8X","outputId":"7ecbf9dc-39b9-46c1-f84e-e14d965275a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] <SplitData> Split data into 34237 REST dataset and 3804 TEST dataset. (Total: 38041, Rate: 10%)\n","[INFO] <demoTwin> Get 34237 TRAIN data, 3804 TEST data. (Total: 38041)\n"]}],"source":["# @title Loading Dataset (Test and Train)\n","dataTrain, labelTrain, dataTest, labelTest = SplitData(data, label, 'test', rate=0.2)\n","print('[INFO] <demoTwin> Get ' + str(len(dataTrain)) + ' TRAIN data, ' + str(len(dataTest))\n","      + ' TEST data. (Total: ' + str(len(dataTrain) + len(dataTest)) + ')')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqawiwI2r9Gw"},"outputs":[],"source":["# @title HyperParameters Overrides\n","# Overrides default hyperparameters\n","#Predefined variables\n","_TTransformerMaxEpoch_ = 20\n","_TTransformerPerEpoch_ = 1\n","_TTransformerJudEpoch_ = 5\n","_TTransformerBatchSz_ = 20\n","_TTransformerLearnRt_ = 0.0005\n","_TwinMaxLen_ = 800 # This parameter determines maximum sequence lenght\n","\n","_nHead_ = 8\n","# Device\n","device = 'cpu'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzFhYOzpviYX"},"outputs":[],"source":["# @title TwinTransformer (nn.model)\n","class TwinTransformer(nn.Module):\n","    '''\n","    TwinTransformer : convert a patch data into a predicted label.\n","    '''\n","\n","    def __init__(self, preWTwin=None, preWMsg=None, hidSizTwin=32, hidSizMsg=32, hidLayTwin=1, hidLayMsg=1, vSizTwin=None, vSizMsg=None, emDimTwin=128, emDimMsg=128):\n","        '''\n","        define each layer in the network model.\n","        :param preWTwin: tensor pre-trained weights for embedding layer for twin.\n","        :param preWMsg: tensor pre-trained weights for embedding layer for msg.\n","        :param hidSizTwin: node number in the hidden layer for twin.\n","        :param hidSizMsg: node number in the hidden layer for msg.\n","        :param hidLayTwin: number of hidden layer for twin.\n","        :param hidLayMsg: number of hidden layer for msg.\n","        '''\n","\n","        super(TwinTransformer, self).__init__()\n","        # parameters.\n","        class_num = 2\n","        # twin.\n","        #vSizTwin, emDimTwin = preWTwin.size()\n","        # Embedding Layer for twin.\n","        self.embedTwin = nn.Embedding(num_embeddings=vSizTwin, embedding_dim=emDimTwin)\n","        if(preWTwin is not None):\n","          self.embedTwin.load_state_dict({'weight': preWTwin})\n","        self.embedTwin.weight.requires_grad = True\n","\n","        self.embedPositionTwin  = nn.Parameter(data=torch.randn(1, _TwinMaxLen_, emDimTwin),requires_grad=True)\n","        self.embedPositionMsg  = nn.Parameter(data=torch.randn(1, _MsgMaxLen_, emDimMsg),requires_grad=True)\n","\n","        #vSizMsg, emDimMsg = preWMsg.size()\n","        # Embedding Layer for msg.\n","        self.embedMsg = nn.Embedding(num_embeddings=vSizMsg, embedding_dim=emDimMsg)\n","        if(preWMsg is not None):\n","          self.embedMsg.load_state_dict({'weight': preWMsg})\n","        self.embedMsg.weight.requires_grad = True\n","\n","        #lerneable parameters\n","        self.embedClassB = nn.Parameter(data=torch.randn(1, 1, emDimTwin),requires_grad=True)\n","        self.embedClassA = nn.Parameter(data=torch.randn(1, 1, emDimTwin),requires_grad=True)\n","        self.embedClassM = nn.Parameter(data=torch.randn(1, 1, emDimMsg),requires_grad=True)\n","\n","        self.transformerTwinB = nn.TransformerEncoder(encoder_layer = nn.TransformerEncoderLayer(\n","            d_model = emDimTwin,\n","            nhead = _nHead_,\n","            dim_feedforward=512,\n","            dropout=0.2,\n","            activation=\"gelu\",\n","            device=device,\n","            dtype=torch.float,\n","            batch_first=True,\n","        ), num_layers = 1, norm=nn.LayerNorm(emDimTwin), enable_nested_tensor=True, mask_check=False)\n","\n","        self.transformerTwinA = nn.TransformerEncoder(encoder_layer = nn.TransformerEncoderLayer(\n","            d_model = emDimTwin,\n","            nhead = _nHead_,\n","            dim_feedforward=512,\n","            dropout=0.2,\n","            activation=\"gelu\",\n","            device=device,\n","            dtype=torch.float,\n","            batch_first=True,\n","        ), num_layers = 1, norm=None, enable_nested_tensor=True, mask_check=False)\n","\n","        self.transformerMsg = nn.TransformerEncoder(encoder_layer = nn.TransformerEncoderLayer(\n","            d_model = emDimMsg,\n","            nhead = _nHead_,\n","            dim_feedforward=512,\n","            dropout=0.2,\n","            activation=\"gelu\",\n","            device=device,\n","            dtype=torch.float,\n","            batch_first=True,\n","        ), num_layers = 1, norm=nn.LayerNorm(emDimTwin), enable_nested_tensor=True, mask_check=False)\n","\n","        self.classifier = nn.Sequential(\n","            nn.LayerNorm(normalized_shape=emDimTwin+emDimTwin+emDimMsg),\n","            nn.Dropout(p=0.5),\n","            nn.Linear(in_features=emDimTwin+emDimTwin+emDimMsg,\n","                      out_features=emDimTwin+emDimTwin+emDimMsg),\n","            nn.LayerNorm(normalized_shape=emDimTwin+emDimTwin+emDimMsg),\n","            nn.Linear(in_features= emDimTwin+emDimTwin+emDimMsg,\n","                      out_features=class_num)\n","        )\n","\n","    def forward(self, x):\n","        '''\n","        convert inputs to predictions.\n","        :param x: input tensor. dimension: batch_size * twin_length * feature_dim.\n","        :return: self.softmax(final_out) - predictions.\n","        [[0.3, 0.7], [0.2, 0.8], ...]\n","        '''\n","        batch_size = x.shape[0]\n","    # twin 1.\n","        #print(\"begin\")\n","        xTwinB = x[:, :_TwinMaxLen_-1, :6]\n","        embedsTwinB = self.embedTwin(xTwinB [:, :, 0]) # b,799,128\n","        featuresB = xTwinB[:, :, 1:] # b,799,5\n","        classTokenB = self.embedClassB.expand(batch_size,-1,-1).to(device)\n","        xb = torch.cat((embedsTwinB[:,:,:-5],featuresB),dim=2) # b,799,128\n","        xb = torch.cat((classTokenB,xb),dim=1) # b,800,128\n","        xb = self.embedPositionTwin + xb\n","        xb = self.transformerTwinB(xb) # b,800,128\n","\n","    # twin 2.\n","        xTwinA = x[:, :_TwinMaxLen_-1, 6:-1]\n","        embedsTwinA = self.embedTwin(xTwinA[:, :, 0]) # b,799,128\n","        featuresA = xTwinA[:, :, 1:] # b,799,5\n","        classTokenA = self.embedClassA.expand(batch_size,-1,-1).to(device)\n","        xa = torch.cat((embedsTwinA[:,:,:-5],featuresA),dim=2) # b,799,128\n","        xa = torch.cat((classTokenA,xa),dim=1) # b,800,128\n","        xa = self.embedPositionTwin + xa\n","        xa = self.transformerTwinA(xa) # b,800,128\n","\n","    # msg.\n","        xMsg = x[:, :_MsgMaxLen_-1, -1]\n","        embedsMsg = self.embedMsg(xMsg) # b,199,128\n","        classTokenM = self.embedClassM.expand(batch_size,-1,-1).to(device)\n","        xm = torch.cat((classTokenM,embedsMsg[:,:,:]),dim=1) # b,200,128\n","        xm = self.embedPositionMsg + xm\n","        xm = self.transformerMsg(xm) # b,200,128\n","\n","    # common.\n","        concatenated = torch.cat((xb[:,0], xa[:,0],xm[:,0]), dim=1) # b,128+128+128\n","\n","        x = self.classifier(concatenated)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"FbAIBPQiue-4"},"source":["### Test/Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMfvpANxue-5"},"outputs":[],"source":["import time\n","# @title TwinTransformerTrain Definition\n","def TwinTransformerTrain(dTrain, lTrain, dValid, lValid, preWTwin=None, preWMsg=None, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n","    '''\n","    Train the TwinRNN model.\n","    :param dTrain: training data. [[n, ...], ...]\n","    :param lTrain: training label. [[n, ...], ...]\n","    :param dValid: validation data. [[n, ...], ...]\n","    :param lValid: validation label. [[n, ...], ...]\n","    :param preWDiff: pre-trained weights for diff embedding layer.\n","    :param preWMsg: pre-trained weights for msg embedding layer.\n","    :param batchsize: number of samples in a batch.\n","    :param learnRate: learning rate.\n","    :param dTest: test data. [[n, ...], ...]\n","    :param lTest: test label. [[n, ...], ...]\n","    :return: model - the TwinTransformer model.\n","    '''\n","\n","    # get the mark of the test dataset.\n","    if dTest is None: dTest = []\n","    if lTest is None: lTest = []\n","    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n","\n","    # tensor data processing.\n","    try:\n","      xTrain\n","    except NameError:\n","      xTrain = torch.from_numpy(dTrain).long().to(device)\n","    try:\n","      yTrain\n","    except NameError:\n","      yTrain = torch.from_numpy(lTrain).long().to(device)\n","    try:\n","      xValid\n","    except NameError:\n","      xValid = torch.from_numpy(dValid).long().to(device)\n","    try:\n","      yValid\n","    except NameError:\n","      yValid = torch.from_numpy(lValid).long().to(device)\n","    if (markTest):\n","        xTest = torch.from_numpy(dTest).long().to(device)\n","        yTest = torch.from_numpy(lTest).long().to(device)\n","\n","    # batch size processing.\n","    train = torchdata.TensorDataset(xTrain, yTrain)\n","    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n","    valid = torchdata.TensorDataset(xValid, yValid)\n","    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n","    if (markTest):\n","        test = torchdata.TensorDataset(xTest, yTest)\n","        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n","\n","    # get training weights.\n","    lbTrain = [item for sublist in lTrain.tolist() for item in sublist]\n","    weights = []\n","    for lb in range(2):\n","        weights.append(1 - lbTrain.count(lb) / len(lbTrain))\n","    lbWeights = torch.FloatTensor(weights).to(device)\n","\n","    if(preWTwin is not None):\n","      preWTwin = torch.from_numpy(preWTwin)\n","    if(preWMsg is not None):\n","      preWMsg = torch.from_numpy(preWMsg)\n","    model = TwinTransformer(preWTwin, preWMsg,vSizTwin=35576,vSizMsg=80831)\n","    model.to(device)\n","    print('[INFO] <TwinTransformer> ModelType: TwinTransformer.')\n","    # print('[INFO] <TwinTransformer> Code Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_TwinEmbedDim_, _TwinMaxLen_, _TRnnHidSiz_, _TRnnHidLay_))\n","    # print('[INFO] <TwinTransformer> Msg  Part: EmbedDim: %d, MaxLen: %d, HidNodes: %d, HidLayers: %d.' % (_MsgEmbedDim_, _MsgMaxLen_, _MRnnHidSiz_, _MRnnHidLay_))\n","    # print('[INFO] <TwinTransformer> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d, JudEpoch: %d.' % (batchsize, learnRate, _TRnnMaxEpoch_, _TRnnPerEpoch_, _TRnnJudEpoch_))\n","    # optimizing with stochastic gradient descent.\n","    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n","    # seting loss function as mean squared error.\n","    criterion = nn.CrossEntropyLoss(weight=lbWeights)\n","    # memory\n","    torch.backends.cudnn.benchmark = True\n","    torch.backends.cudnn.enabled = True\n","\n","    # run on each epoch.\n","    accList = [0]\n","    lossL = []\n","    accL = []\n","    validAccL = []\n","    execution_times = []\n","\n","    for epoch in range(_TTransformerMaxEpoch_):\n","        # training phase.\n","        model.train()\n","        lossTrain = 0\n","        predictions = []\n","        labels = []\n","        prti = 0\n","        for iter, (data, label) in enumerate(trainloader):\n","            prti = prti + 1\n","            if prti % 100 == 0:\n","              print(f\"Iteration {iter} out of {len(trainloader)}\")\n","            # data conversion.\n","            data = data.to(device)\n","            label = label.contiguous().view(-1)\n","            label = label.to(device)\n","            # back propagation.\n","            start_time = time.perf_counter()\n","            optimizer.zero_grad()  # set the gradients to zero.\n","            yhat = model.forward(data)  # get output\n","            loss = criterion(yhat, label)\n","            #print(\"Yhat\",yhat)\n","            #print(\"Label\",label)\n","            loss.backward()\n","            optimizer.step()\n","            # statistic\n","            end_time = time.perf_counter()\n","            elapsed_time = end_time - start_time\n","            execution_times.append(elapsed_time)\n","            lossTrain += loss.item() * len(label)\n","            preds = torch.argmax(torch.softmax(yhat,dim=1),dim=1)\n","            predictions.extend(preds.int().tolist())\n","            labels.extend(label.int().tolist())\n","            torch.cuda.empty_cache()\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        lossTrain /= len(dTrain)\n","        # train accuracy.\n","        accTrain = accuracy_score(labels, predictions) * 100\n","\n","        # validation phase.\n","        model.eval()\n","        predictions = []\n","        labels = []\n","        with torch.no_grad():\n","            for iter, (data, label) in enumerate(validloader):\n","                # data conversion.\n","                data = data.to(device)\n","                label = label.contiguous().view(-1)\n","                label = label.to(device)\n","                # forward propagation.\n","                yhat = model.forward(data)  # get output\n","                # statistic\n","                preds = torch.argmax(torch.softmax(yhat,dim=1),dim=1)\n","                predictions.extend(preds.int().tolist())\n","                labels.extend(label.int().tolist())\n","                torch.cuda.empty_cache()\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # valid accuracy.\n","        accValid = accuracy_score(labels, predictions) * 100\n","        accList.append(accValid)\n","\n","        # testing phase.\n","        if (markTest):\n","            model.eval()\n","            predictions = []\n","            labels = []\n","            with torch.no_grad():\n","                for iter, (data, label) in enumerate(testloader):\n","                    # data conversion.\n","                    data = data.to(device)\n","                    label = label.contiguous().view(-1)\n","                    label = label.to(device)\n","                    # forward propagation.\n","                    yhat = model.forward(data)  # get output\n","                    # statistic\n","                    preds = torch.argmax(torch.softmax(yhat,dim=1),dim=1)\n","                    predictions.extend(preds.int().tolist())\n","                    labels.extend(label.int().tolist())\n","                    torch.cuda.empty_cache()\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            # test accuracy.\n","            accTest = accuracy_score(labels, predictions) * 100\n","\n","        # output information.\n","        if (0 == (epoch + 1) % _TTransformerPerEpoch_):\n","            strAcc = '[Epoch {:03}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n","            if (markTest):\n","                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n","            print(strAcc)\n","            lossL.append(lossTrain)\n","            accL.append(accTrain)\n","            validAccL.append(accValid)\n","        # save the best model.\n","        if (accList[-1] > max(accList[0:-1])):\n","            torch.save(model.state_dict(), tempPath + '/model_TwinTransformer.pth')\n","        # stop judgement.\n","        if (epoch >= _TTransformerJudEpoch_) and (accList[-1] < min(accList[-1-_TTransformerJudEpoch_:-1])):\n","            break\n","\n","    print(f\"Total execution time {sum(execution_times):.6f}\")\n","\n","    # load best model.\n","    model.load_state_dict(torch.load(tempPath + '/model_TwinTransformer.pth'))\n","    print('[INFO] <TwinRNNTrain> Finish training TwinTransformer model. (Best model: ' + tempPath + '/model_TwinTransformer.pth)')\n","\n","    return model, accList, execution_times, accL, validAccL, lossL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rr71CD1ue-5"},"outputs":[],"source":["# @title TwinTransformerTest Definition\n","def TwinTransformerTest(model, dTest, lTest, batchsize=64):\n","    '''\n","    Test the TwinRNN model.\n","    :param model: deep learning model.\n","    :param dTest: test data.\n","    :param lTest: test label.\n","    :param batchsize: number of samples in a batch\n","    :return: predictions - predicted labels. [[0], [1], ...]\n","             accuracy - the total test accuracy. numeric\n","    '''\n","\n","    # tensor data processing.\n","    xTest = torch.from_numpy(dTest).long().to(device)\n","    yTest = torch.from_numpy(lTest).long().to(device)\n","\n","    # batch size processing.\n","    test = torchdata.TensorDataset(xTest, yTest)\n","    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n","\n","    # load the model of recurrent neural network.\n","    model.to(device)\n","    execution_times = []\n","    # testing phase.\n","    model.eval()\n","    predictions = []\n","    labels = []\n","    with torch.no_grad():\n","        for iter, (data, label) in enumerate(testloader):\n","            # data conversion.\n","            data = data.to(device)\n","            label = label.contiguous().view(-1)\n","            label = label.to(device)\n","            # forward propagation.\n","            start_time = time.perf_counter()\n","            yhat = model.forward(data)  # get output\n","            # statistic\n","            end_time = time.perf_counter()\n","            elapsed_time = end_time - start_time\n","            execution_times.append(elapsed_time)\n","            preds = torch.argmax(torch.softmax(yhat,dim=1),dim=1)\n","            predictions.extend(preds.int().tolist())\n","            labels.extend(label.int().tolist())\n","            torch.cuda.empty_cache()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # testing accuracy.\n","    accuracy = accuracy_score(labels, predictions) * 100\n","    predictions = [[item] for item in predictions]\n","    print(f\"Execution Time = {sum(execution_times)} seconds\")\n","    return predictions, accuracy"]},{"cell_type":"markdown","metadata":{"id":"8JJKh4ZCu8QI"},"source":["## Run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCTlc2ZGue-6"},"outputs":[],"source":["# @title RunTransformer\n","\n","def RunTransformer():\n","  acclst = []\n","  # TwinTransformerTrain\n","  if (_MODEL_) & (os.path.exists(tempPath + '/model_TwinTransformer.pth')):\n","      preWTwin = torch.from_numpy(twinPreWeights)\n","      preWMsg = torch.from_numpy(msgPreWeights)\n","      model = TwinTransformer(preWTwin, preWMsg, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_)\n","      model.load_state_dict(torch.load(tempPath + '/model_TwinTransformer.pth'))\n","      return model\n","  else:\n","      model, acc,execution_times,accL, validAccL, lossL = TwinTransformerTrain(dataTrain, labelTrain, dataTest, labelTest, preWTwin=twinPreWeights, preWMsg=msgPreWeights,\n","                            batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n","  # TwinRNNTest\n","  predictions, accuracy = TwinTransformerTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n","  _, confusion = OutputEval(predictions, labelTest, 'TwinTwinTransformer')\n","\n","  return model, acc,execution_times,accL, validAccL, lossL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cC1OdowOzrw-"},"outputs":[],"source":["# @title RunTransformerFromFile\n","def RunTransformerFromFile():\n","  # TwinTransformerTrain\n","  #preWTwin = torch.from_numpy(twinPreWeights)\n","  #preWMsg = torch.from_numpy(msgPreWeights)\n","  model = TwinTransformer(None, None, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_,vSizTwin=35576,vSizMsg=80831, emDimTwin=128, emDimMsg=128)\n","  model.load_state_dict(torch.load(tempPath + '/model_TwinTransformer.pth'))\n","  return model\n","  model,acc = TwinTransformerTrain(dataTrain, labelTrain, dataTest, labelTest, preWTwin=None, preWMsg=None,batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n","\n","  # TwinRNNTest\n","  predictions, accuracy = TwinTransformerTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n","  _, confusion = OutputEval(predictions, labelTest, 'TwinTwinTransformer')\n","\n","  return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwgIeGQ8wXoZ","outputId":"2c1e20a4-7bcc-4300-ab03-f15e78befb25"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] <TwinTransformer> ModelType: TwinTransformer.\n"]}],"source":["#preWTwin = torch.from_numpy(twinPreWeights)\n","#print(preWTwin)\n","#print(type(twinPreWeights))\n","#twinPreWeights.shape\n","model,_,execution_times,accL, validAccL, lossL=RunTransformer()\n","#print(torch.from_numpy(msgPreWeights).size())\n","#acclst=RunTransformerFromFile()\n","#model = TwinTransformer(None, None, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_,vSizTwin=35576,vSizMsg=80831, emDimTwin=128, emDimMsg=128)\n","#model.load_state_dict(torch.load(tempPath + '/model_TwinTransformer.pth'))\n","#predictions, accuracy = TwinTransformerTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n","#_, confusion = OutputEval(predictions, labelTest, 'TwinTwinTransformer')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqkBW_mYDKs9","outputId":"b1ec4046-498a-4fef-8418-0edb4268595a"},"outputs":[{"name":"stdout","output_type":"stream","text":["TrainLoss\n","0.6836903764916263\n","0.5756226209647523\n","0.5202310553697183\n","0.4846651273868169\n","0.46042444149592765\n","0.4300165000826266\n","0.40777710488920066\n","0.3920866292165109\n","0.37296869444503095\n","0.35574744187351365\n","0.3367733137949913\n","0.3214805730009941\n","0.3035258052278548\n","0.2894542025524653\n","0.2729290025587007\n","0.2654637241017426\n","0.2519384859180981\n","Accuracy\n","59.69565090399276\n","71.75862371118966\n","76.54876303414436\n","78.4472938633642\n","79.5396792943307\n","81.12276192423401\n","82.0457399889009\n","83.10599643660368\n","83.97347898472412\n","84.72120804977071\n","85.62958203113591\n","86.37146946286181\n","87.3178140608114\n","87.84648187633263\n","88.57960685807753\n","89.06446242369367\n","89.60189268919589\n","ValidationAccuracy\n","68.21766561514195\n","74.65825446898002\n","76.76130389064143\n","79.41640378548895\n","80.57308096740273\n","80.5993690851735\n","79.57413249211356\n","80.09989484752892\n","80.3627760252366\n","79.57413249211356\n","81.12513144058885\n","80.5205047318612\n","80.73080967402734\n","81.38801261829653\n","80.31019978969506\n","80.5205047318612\n","80.28391167192429\n"]}],"source":["print(\"TrainLoss\")\n","for i in lossL:\n","  print(i)\n","print(\"Accuracy\")\n","for i in accL:\n","  print(i)\n","print(\"ValidationAccuracy\")\n","for i in validAccL:\n","  print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iKcSsoW5Gzdi","outputId":"9eaf6894-1373-471e-bd72-925a718e4941"},"outputs":[{"name":"stdout","output_type":"stream","text":["Execution Time = 40.32994986139238 seconds\n","       -------------------------------------------\n","       method           :  TwinTransformer\n","       accuracy  (ACC)  :  81.388%\n","       precision (P)    :  69.203%\n","       recall    (R)    :  77.554%\n","       F1 score  (F1)   :  0.731\n","       fall-out  (FPR)  :  16.751%\n","       miss rate (FNR)  :  22.446%\n","       confusion matrix :      (actual)\n","                           Neg         Pos\n","       (predicted) Neg     2132 (TN)   279  (FN)\n","                   Pos     429  (FP)   964  (TP)\n","       -------------------------------------------\n"]}],"source":["predictions, accuracy = TwinTransformerTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n","_, confusion = OutputEval(predictions, labelTest, 'TwinTransformer')\n","np.savetxt(\"reals.txt\", labelTest, fmt=\"%s\", delimiter=\"\\n\")\n","np.savetxt(\"predicted.txt\", predictions, fmt=\"%s\", delimiter=\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h8Q1IwTV36Nj","outputId":"d1da68af-512a-4ab9-9019-b44f0d111c72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"]}],"source":["!pip install torchinfo\n","from torchinfo import summary\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbxDy23AGN5r","outputId":"33b5df8d-8d87-4e56-f741-60e815f0ba3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["=================================================================================================================================================\n","Layer (type:depth-idx)                        Kernel Shape              Output Shape              Param #                   Mult-Adds\n","=================================================================================================================================================\n","TwinTransformer                               --                        [189, 2]                  128,384                   --\n","├─Embedding: 1-1                              --                        [189, 799, 128]           4,553,728                 860,654,592\n","├─TransformerEncoder: 1-2                     --                        [189, 800, 128]           --                        --\n","│    └─ModuleList: 2-1                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-1      --                        [189, 800, 128]           198,272                   --\n","│    └─LayerNorm: 2-2                         --                        [189, 800, 128]           256                       48,384\n","├─Embedding: 1-3                              --                        [189, 799, 128]           (recursive)               860,654,592\n","├─TransformerEncoder: 1-4                     --                        [189, 800, 128]           --                        --\n","│    └─ModuleList: 2-3                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-2      --                        [189, 800, 128]           198,272                   --\n","├─Embedding: 1-5                              --                        [189, 199, 128]           10,346,368                1,955,463,552\n","├─TransformerEncoder: 1-6                     --                        [189, 200, 128]           --                        --\n","│    └─ModuleList: 2-4                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-3      --                        [189, 200, 128]           198,272                   --\n","│    └─LayerNorm: 2-5                         --                        [189, 200, 128]           256                       48,384\n","├─Sequential: 1-7                             --                        [189, 2]                  --                        --\n","│    └─LayerNorm: 2-6                         --                        [189, 384]                768                       145,152\n","│    └─Dropout: 2-7                           --                        [189, 384]                --                        --\n","│    └─Linear: 2-8                            --                        [189, 384]                147,840                   27,941,760\n","│    └─LayerNorm: 2-9                         --                        [189, 384]                768                       145,152\n","│    └─Linear: 2-10                           --                        [189, 2]                  770                       145,530\n","=================================================================================================================================================\n","Total params: 15,773,954\n","Trainable params: 15,773,954\n","Non-trainable params: 0\n","Total mult-adds (G): 3.71\n","=================================================================================================================================================\n","Input size (MB): 15.72\n","Forward/backward pass size (MB): 543.07\n","Params size (MB): 60.20\n","Estimated Total Size (MB): 618.99\n","=================================================================================================================================================\n","=================================================================================================================================================\n","Layer (type:depth-idx)                        Kernel Shape              Output Shape              Param #                   Mult-Adds\n","=================================================================================================================================================\n","TwinTransformer                               --                        [189, 2]                  128,384                   --\n","├─Embedding: 1-1                              --                        [189, 799, 128]           4,553,728                 860,654,592\n","├─TransformerEncoder: 1-2                     --                        [189, 800, 128]           --                        --\n","│    └─ModuleList: 2-1                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-1      --                        [189, 800, 128]           198,272                   --\n","│    └─LayerNorm: 2-2                         --                        [189, 800, 128]           256                       48,384\n","├─Embedding: 1-3                              --                        [189, 799, 128]           (recursive)               860,654,592\n","├─TransformerEncoder: 1-4                     --                        [189, 800, 128]           --                        --\n","│    └─ModuleList: 2-3                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-2      --                        [189, 800, 128]           198,272                   --\n","├─Embedding: 1-5                              --                        [189, 199, 128]           10,346,368                1,955,463,552\n","├─TransformerEncoder: 1-6                     --                        [189, 200, 128]           --                        --\n","│    └─ModuleList: 2-4                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-3      --                        [189, 200, 128]           198,272                   --\n","│    └─LayerNorm: 2-5                         --                        [189, 200, 128]           256                       48,384\n","├─Sequential: 1-7                             --                        [189, 2]                  --                        --\n","│    └─LayerNorm: 2-6                         --                        [189, 384]                768                       145,152\n","│    └─Dropout: 2-7                           --                        [189, 384]                --                        --\n","│    └─Linear: 2-8                            --                        [189, 384]                147,840                   27,941,760\n","│    └─LayerNorm: 2-9                         --                        [189, 384]                768                       145,152\n","│    └─Linear: 2-10                           --                        [189, 2]                  770                       145,530\n","=================================================================================================================================================\n","Total params: 15,773,954\n","Trainable params: 15,773,954\n","Non-trainable params: 0\n","Total mult-adds (G): 3.71\n","=================================================================================================================================================\n","Input size (MB): 15.72\n","Forward/backward pass size (MB): 543.07\n","Params size (MB): 60.20\n","Estimated Total Size (MB): 618.99\n","=================================================================================================================================================\n","TwinTransformer(\n","  (embedTwin): Embedding(35576, 128)\n","  (embedMsg): Embedding(80831, 128)\n","  (transformerTwinB): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","        (linear2): Linear(in_features=512, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformerTwinA): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","        (linear2): Linear(in_features=512, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","  )\n","  (transformerMsg): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","        (linear2): Linear(in_features=512, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (classifier): Sequential(\n","    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","    (1): Dropout(p=0.5, inplace=False)\n","    (2): Linear(in_features=384, out_features=384, bias=True)\n","    (3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","    (4): Linear(in_features=384, out_features=2, bias=True)\n","  )\n",")\n"]}],"source":["xTrain = torch.from_numpy(dataTrain).to(device)\n","yTrain = torch.from_numpy(labelTrain).to(device)\n","train = torchdata.TensorDataset(xTrain, yTrain)\n","trainloader = torchdata.DataLoader(train, batch_size=_TRnnBatchSz_, shuffle=False)\n","b1 = None\n","for iter, (data, label) in enumerate(trainloader):\n","  b1 = data\n","b1.shape\n","\n","model_stats = summary(model, input_data=b1,batch_dim=0, verbose=1,dtypes=[torch.long],col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"], depth=100)\n","\n","summary_str = str(model_stats)\n","# summary_str contains the string representation of the summary!\n","print(summary_str)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsH_t3ORlOQO","outputId":"46c646a9-a520-497d-bd34-9b70ec78fd1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer: transformerTwinB.layers.0.self_attn, Number of Heads: 2\n","Layer: transformerTwinA.layers.0.self_attn, Number of Heads: 2\n","Layer: transformerMsg.layers.0.self_attn, Number of Heads: 2\n"]}],"source":["for name, module in model.named_modules():\n","    if isinstance(module, torch.nn.MultiheadAttention):\n","        print(f\"Layer: {name}, Number of Heads: {module.num_heads}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Br8V0Np375AR","outputId":"150c3aed-9713-49f3-ca3b-90ce24503d8e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","<ipython-input-101-7b822e4054f0>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(tempPath + '/model_TwinTransformer.pth'))\n"]}],"source":["model = RunTransformerFromFile()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3bhTUQ2-ACa","outputId":"df10c65d-0895-4779-f92a-83a1c2b40673"},"outputs":[{"data":{"text/plain":["torch.Size([189, 800, 13])"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["xTrain = torch.from_numpy(dataTrain).long().to(device)\n","yTrain = torch.from_numpy(labelTrain).long().to(device)\n","train = torchdata.TensorDataset(xTrain, yTrain)\n","trainloader = torchdata.DataLoader(train, batch_size=256, shuffle=False)\n","b1 = None\n","for iter, (data, label) in enumerate(trainloader):\n","  b1 = data\n","b1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DqyB19-76KcA","outputId":"3f1998f2-8f12-4de9-e47b-3de7385c3fae"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([189, 800, 128])\n","=================================================================================================================================================\n","Layer (type:depth-idx)                        Kernel Shape              Output Shape              Param #                   Mult-Adds\n","=================================================================================================================================================\n","TwinTransformer                               --                        [189, 2]                  128,384                   --\n","├─Embedding: 1-1                              --                        [189, 799, 128]           4,553,728                 860,654,592\n","├─TransformerEncoder: 1-2                     --                        [189, 800, 128]           --                        --\n","│    └─ModuleList: 2-1                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-1      --                        [189, 800, 128]           198,272                   24,990,336\n","│    └─LayerNorm: 2-2                         --                        [189, 800, 128]           256                       48,384\n","├─Embedding: 1-3                              --                        [189, 799, 128]           (recursive)               860,654,592\n","├─TransformerEncoder: 1-4                     --                        [189, 800, 128]           --                        --\n","│    └─ModuleList: 2-3                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-2      --                        [189, 800, 128]           198,272                   24,990,336\n","├─Embedding: 1-5                              --                        [189, 199, 128]           10,346,368                1,955,463,552\n","├─TransformerEncoder: 1-6                     --                        [189, 200, 128]           --                        --\n","│    └─ModuleList: 2-4                        --                        --                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-3      --                        [189, 200, 128]           198,272                   24,990,336\n","│    └─LayerNorm: 2-5                         --                        [189, 200, 128]           256                       48,384\n","├─Sequential: 1-7                             --                        [189, 2]                  --                        --\n","│    └─LayerNorm: 2-6                         --                        [189, 384]                768                       145,152\n","│    └─Dropout: 2-7                           --                        [189, 384]                --                        --\n","│    └─Linear: 2-8                            --                        [189, 384]                147,840                   27,941,760\n","│    └─LayerNorm: 2-9                         --                        [189, 384]                768                       145,152\n","│    └─Linear: 2-10                           --                        [189, 2]                  770                       145,530\n","=================================================================================================================================================\n","Total params: 15,773,954\n","Trainable params: 15,773,954\n","Non-trainable params: 0\n","Total mult-adds (G): 3.78\n","=================================================================================================================================================\n","Input size (MB): 15.72\n","Forward/backward pass size (MB): 2981.62\n","Params size (MB): 61.79\n","Estimated Total Size (MB): 3059.13\n","=================================================================================================================================================\n"]}],"source":["model_stats = summary(model, input_data=b1,batch_dim=0, verbose=0,dtypes=[torch.long],col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n","\n","summary_str = str(model_stats)\n","# summary_str contains the string representation of the summary!\n","print(summary_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKT7Q-KEA9NR"},"outputs":[],"source":["f = open(\"/persistent/article/models/1.txt\", \"w\")\n","f.write(summary_str)\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"MRvhgYRSAnCW","outputId":"6136c658-9429-4798-b395-80a6dd84ff42"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.7033533508519889]\n"]},{"data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7c9cb2893280>]"]},"execution_count":57,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","print(loss)\n","plt.plot(list(range(1)), loss[1:],linestyle=\"\",marker=\"o\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bWPMQ628lxQ"},"outputs":[],"source":["accs20241028 = acclst"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxL_ggccuUWY"},"outputs":[],"source":["# @title Cleanup\n","# Due to Python memory management this might be needed to clean gpu cache (even a few times)\n","# Please ignore error 'model' is not defined\n","\n","torch.cuda.empty_cache()\n","gc.collect()\n","#!nvidia-smi #prints out stats\n","\n","del model"]},{"cell_type":"markdown","metadata":{"id":"nvn10jhueP0c"},"source":["## Result Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4W_m7LKSf6Jz"},"outputs":[],"source":["# @title TwinTransformerGetBadResults Definition\n","def TwinTransformerGetBadResults(model, dTest, lTest, batchsize=64):\n","    '''\n","    Test the TwinRNN model.\n","    :param model: deep learning model.\n","    :param dTest: test data.\n","    :param lTest: test label.\n","    :param batchsize: number of samples in a batch\n","    :return: predictions - predicted labels. [[0], [1], ...]\n","             accuracy - the total test accuracy. numeric\n","    '''\n","\n","    # tensor data processing.\n","    xTest = torch.from_numpy(dTest).long().to(device)\n","    yTest = torch.from_numpy(lTest).long().to(device)\n","\n","    # batch size processing.\n","    test = torchdata.TensorDataset(xTest, yTest)\n","    testloader = torchdata.DataLoader(test, batch_size=1, shuffle=False)\n","\n","    # load the model of recurrent neural network.\n","    model.to(device)\n","\n","    # testing phase.\n","    model.eval()\n","    predictions = []\n","    labels = []\n","    falsePositives = []\n","    falseNegatives = []\n","    with torch.no_grad():\n","        for iter, (data, label) in enumerate(testloader):\n","            # data conversion.\n","            data = data.to(device)\n","            label = label.contiguous().view(-1)\n","            label = label.to(device)\n","            # forward propagation.\n","            yhat = model.forward(data)  # get output\n","            # statistic\n","            preds = torch.argmax(torch.softmax(yhat,dim=1),dim=1)\n","            if preds[0].int() != label.int():\n","              if preds[0].int() == 1:\n","                falsePositives.append(data)\n","              if preds[0].int() == 0:\n","                falseNegatives.append(data)\n","\n","            predictions.extend(preds.int().tolist())\n","            labels.extend(label.int().tolist())\n","            torch.cuda.empty_cache()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # testing accuracy.\n","    accuracy = accuracy_score(labels, predictions) * 100\n","    predictions = [[item] for item in predictions]\n","\n","    return predictions, accuracy, falsePositives, falseNegatives"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3YmY8cqePHq","outputId":"3a770bed-97a0-46e2-a5da-e28f7b200a06"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-49-a8faf42b97f2>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(tempPath + '/model_TwinTransformer.pth'))\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model = TwinTransformer(None, None, hidSizTwin=_TRnnHidSiz_, hidSizMsg=_MRnnHidSiz_, hidLayTwin=_TRnnHidLay_, hidLayMsg=_MRnnHidLay_,vSizTwin=35576,vSizMsg=80831, emDimTwin=128, emDimMsg=128)\n","model.load_state_dict(torch.load(tempPath + '/model_TwinTransformer.pth'))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yB2w16dWhS7x"},"outputs":[],"source":["p,a,fp,fn = TwinTransformerGetBadResults(model,dataTest,labelTest)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVt7HIMui8x4","outputId":"a19dc026-55ac-4eb9-b7b4-d37ae14dd262"},"outputs":[{"name":"stdout","output_type":"stream","text":["       -------------------------------------------\n","       method           :  TwinTwinTransformer\n","       accuracy  (ACC)  :  83.333%\n","       precision (P)    :  76.322%\n","       recall    (R)    :  70.037%\n","       F1 score  (F1)   :  0.730\n","       fall-out  (FPR)  :  10.339%\n","       miss rate (FNR)  :  29.963%\n","       confusion matrix :      (actual)\n","                           Neg         Pos\n","       (predicted) Neg     4622 (TN)   735  (FN)\n","                   Pos     533  (FP)   1718 (TP)\n","       -------------------------------------------\n"]}],"source":["_, confusion = OutputEval(p, labelTest, 'TwinTwinTransformer')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lq4lCTPsj7Pb","outputId":"e3802218-1986-4c3c-9a0a-42c3175a3aa9"},"outputs":[{"data":{"text/plain":["(1268, 800, 13)"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["fdata = np.concatenate((fp,fn)).squeeze()\n","flabels = np.concatenate((np.zeros((len(fp),1)),np.ones((len(fn),1))))\n","fdata.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"Ij1svtMrjoPt","outputId":"a44c5403-cc4c-4fd5-8a32-9cfa00c305b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] <TwinTransformer> ModelType: TwinTransformer.\n","[Epoch 001] loss: 3.32, train acc: 28.391%, valid acc: 67.758%, test acc: 67.758%.\n","[Epoch 002] loss: 0.324, train acc: 91.719%, valid acc: 32.242%, test acc: 32.242%.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-92-008e5b5acf10>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m modelV, acc = TwinTransformerTrain(fdata, flabels, dataTest, labelTest, preWTwin=twinPreWeights, preWMsg=msgPreWeights,\n\u001b[0m\u001b[1;32m      2\u001b[0m                           batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n\u001b[1;32m      3\u001b[0m \u001b[0macclst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TwinRNNTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwinTransformerTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_TRnnBatchSz_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-3ae8757286b5>\u001b[0m in \u001b[0;36mTwinTransformerTrain\u001b[0;34m(dTrain, lTrain, dValid, lValid, preWTwin, preWMsg, batchsize, learnRate, dTest, lTest)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;31m# forward propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;31m# statistic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-ce2be6dfc6f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mxa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassTokenA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# b,800,128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mxa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedPositionTwin\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mxa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mxa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformerTwinA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# b,800,128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# msg.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwhy_not_sparsity_fast_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                 \u001b[0mmerged_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 return torch._transformer_encoder_layer_fwd(\n\u001b[0m\u001b[1;32m    721\u001b[0m                     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["  modelV, acc = TwinTransformerTrain(fdata, flabels, dataTest, labelTest, preWTwin=twinPreWeights, preWMsg=msgPreWeights,\n","                            batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n","  acclst = acc\n","  # TwinRNNTest\n","  predictions, accuracy = TwinTransformerTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n","  _, confusion = OutputEval(predictions, labelTest, 'TwinTwinTransformerV')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVZx72yzjIw6"},"outputs":[],"source":["def TransformPatch2Imgs(sequence): #sequence 800x5\n","  bf = sequence[:,0:5]\n","  b = np.zeros((800,5))\n","  for i in range(len(bf)):\n","    itemValue = bf[i][0]\n","    itemChannel = np.argmax(bf[i][1:6])\n","    b[i][itemChannel] = itemValue\n","  b = b.reshape((20,40,5))\n","  af = sequence[:,6:]\n","  a = np.zeros((800,5))\n","  for i in range(len(af)):\n","    itemValue = af[i][0]\n","    itemChannel = np.argmax(af[i][1:6])\n","    a[i][itemChannel] = itemValue\n","  a = a.reshape((20,40,5))\n","\n","  c = np.zeros((40,40,5))\n","  iter = 0\n","  for i in range(0,20):\n","    c[iter]=b[i]\n","    c[iter+1]=a[i]\n","    iter = iter +2\n","\n","  return b,a,c #20x40x5 #h,w,c\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqd6lLaajgOV"},"outputs":[],"source":["before, after, concatenated = TransformPatch2Imgs(fp[0])"]},{"cell_type":"markdown","metadata":{"id":"7Wru6LwfiMH9"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["5wxnat0TUADv","NARLlgMQHPpw","0p1f0cF1UhD0","3tY0VEerWlxT","iyixzxKZns3s"],"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1b443fecd83f478e9de52cc4ae744393":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61c069436b5a44b1aadc950210c7f174":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ae91f73e6443bca1c55bc4ee559ad0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f4844186243477b8c39942d4839c51f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61c069436b5a44b1aadc950210c7f174","placeholder":"​","style":"IPY_MODEL_1b443fecd83f478e9de52cc4ae744393","value":" 38041/38041 [04:00&lt;00:00, 250.19 examples/s]"}},"6fab03833cb84f179a5317d4481b86c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae6488596b2a42639a5d4d833533dc75","placeholder":"​","style":"IPY_MODEL_aae749477f6f40b9938a1b9cdab166e7","value":"Map: 100%"}},"73abd90a326a4ff692d8a39eb57e595c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aae749477f6f40b9938a1b9cdab166e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae6488596b2a42639a5d4d833533dc75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b28dc90e3b8d4b76b4ccd5a67d7cbb5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b595bdcd677d4f0ba91e15e30f6e1e6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fab03833cb84f179a5317d4481b86c4","IPY_MODEL_e390001c9e374619b77fe15bb158d84a","IPY_MODEL_6f4844186243477b8c39942d4839c51f"],"layout":"IPY_MODEL_67ae91f73e6443bca1c55bc4ee559ad0"}},"e390001c9e374619b77fe15bb158d84a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b28dc90e3b8d4b76b4ccd5a67d7cbb5e","max":38041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73abd90a326a4ff692d8a39eb57e595c","value":38041}},"91cb984d41f84ccf9dd429e596cc6a9e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_040b5326b88748ea94e1547c72eac420","IPY_MODEL_1b6d5271171f4b7d9e3116991a043f42","IPY_MODEL_11ce2ddb93d94ec38cb4b3f4fb4ab229"],"layout":"IPY_MODEL_951ab643f28b4c499e0c7dfc775b7d12"}},"040b5326b88748ea94e1547c72eac420":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e89ece786e674efba80944e084107e26","placeholder":"​","style":"IPY_MODEL_2670b1bf75834664a5ac6bf3a03649ff","value":"Map: 100%"}},"1b6d5271171f4b7d9e3116991a043f42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3eaac7222d940518a74f9d2db4d8bc0","max":30433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df33a24b77a444d39d00db2fd53afb4e","value":30433}},"11ce2ddb93d94ec38cb4b3f4fb4ab229":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3da7c741cc1743edb75911d43d842993","placeholder":"​","style":"IPY_MODEL_fe522c7d515d4bd5a8f15c70fda477a4","value":" 30433/30433 [02:49&lt;00:00, 301.81 examples/s]"}},"951ab643f28b4c499e0c7dfc775b7d12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e89ece786e674efba80944e084107e26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2670b1bf75834664a5ac6bf3a03649ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3eaac7222d940518a74f9d2db4d8bc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df33a24b77a444d39d00db2fd53afb4e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3da7c741cc1743edb75911d43d842993":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe522c7d515d4bd5a8f15c70fda477a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aacad80d2e0b4e43b76553008ef90f2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cdef000b56ba45f98ec4598201dd2d70","IPY_MODEL_41a11a6bc8ce4df8a1584d00a50942e2","IPY_MODEL_7f86ecfde6c24e64adac6a42e7d442a4"],"layout":"IPY_MODEL_622d1b43cd544a1db413338be9cb3c72"}},"cdef000b56ba45f98ec4598201dd2d70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c181606614b45a2b9246dc71acc9d34","placeholder":"​","style":"IPY_MODEL_4d92bff7c5004d33bee84a5e63e3edf5","value":"Map: 100%"}},"41a11a6bc8ce4df8a1584d00a50942e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb7a133f84fd46a88394ac329efaf023","max":7608,"min":0,"orientation":"horizontal","style":"IPY_MODEL_240bc7a2b3b041bbb1541d1c671a7a5e","value":7608}},"7f86ecfde6c24e64adac6a42e7d442a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1468c2c529e749418c163fd02f9d4f70","placeholder":"​","style":"IPY_MODEL_f5584363917840b795f599a772fd12aa","value":" 7608/7608 [00:29&lt;00:00, 337.66 examples/s]"}},"622d1b43cd544a1db413338be9cb3c72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c181606614b45a2b9246dc71acc9d34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d92bff7c5004d33bee84a5e63e3edf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb7a133f84fd46a88394ac329efaf023":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"240bc7a2b3b041bbb1541d1c671a7a5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1468c2c529e749418c163fd02f9d4f70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5584363917840b795f599a772fd12aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"896fa4fccc0142768abca36302fa4169":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_943cda20adae48dc949a72b48d1a4b45","IPY_MODEL_3e9812f750db4a37b1746ecd0279bf98","IPY_MODEL_d0dacf90928244ea81750a4978c1e769"],"layout":"IPY_MODEL_6887caaea9eb4f9b83d90c9e3db597c8"}},"943cda20adae48dc949a72b48d1a4b45":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2424dc68b4004341afdafcf59d462103","placeholder":"​","style":"IPY_MODEL_b3ba6359ad984b2b8c294f720fddc577","value":"config.json: 100%"}},"3e9812f750db4a37b1746ecd0279bf98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56d17bf1a43d48b2b53807046603a1d6","max":1522,"min":0,"orientation":"horizontal","style":"IPY_MODEL_821fa077fb684f979d386c57d9bd318c","value":1522}},"d0dacf90928244ea81750a4978c1e769":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b23ac6979d240eaa548086e80c2b3c4","placeholder":"​","style":"IPY_MODEL_412ba5ac39ce49fbad8c613de49bc17b","value":" 1.52k/1.52k [00:00&lt;00:00, 112kB/s]"}},"6887caaea9eb4f9b83d90c9e3db597c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2424dc68b4004341afdafcf59d462103":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3ba6359ad984b2b8c294f720fddc577":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56d17bf1a43d48b2b53807046603a1d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"821fa077fb684f979d386c57d9bd318c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b23ac6979d240eaa548086e80c2b3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"412ba5ac39ce49fbad8c613de49bc17b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"647c573096564b45a29530de28e93925":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a94c72811c4c4b09b32fab230599e012","IPY_MODEL_1a69e3e765a3494db3dd9fbcf21e55b9","IPY_MODEL_4bd61096e7b045d7ac299bf13d230129"],"layout":"IPY_MODEL_6c64834ca0914387b34c38dfd87c71dc"}},"a94c72811c4c4b09b32fab230599e012":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2213a6e586784ffd9b925f3e62302ea9","placeholder":"​","style":"IPY_MODEL_b0ce189bd063496a934377524ca6897a","value":"configuration_deepseek.py: 100%"}},"1a69e3e765a3494db3dd9fbcf21e55b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_556e3f5d20b74b65bd4601c03e15a034","max":10338,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa49bbfbedcc47a99a5293b50211eedf","value":10338}},"4bd61096e7b045d7ac299bf13d230129":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaa247bb790048b2b7401117c3d99b38","placeholder":"​","style":"IPY_MODEL_1e496485cdc048df862fd50b130bef0e","value":" 10.3k/10.3k [00:00&lt;00:00, 740kB/s]"}},"6c64834ca0914387b34c38dfd87c71dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2213a6e586784ffd9b925f3e62302ea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0ce189bd063496a934377524ca6897a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"556e3f5d20b74b65bd4601c03e15a034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa49bbfbedcc47a99a5293b50211eedf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aaa247bb790048b2b7401117c3d99b38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e496485cdc048df862fd50b130bef0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e52625c876d423fbb2a081a67dc4f90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4cae2638ace422a91f814389dfbc2a8","IPY_MODEL_8f5014f012654d29aa47b4983e4e09e0","IPY_MODEL_5df681f467544406900146c830ffe038"],"layout":"IPY_MODEL_d13893ad015d4be5a82a2065844fe875"}},"e4cae2638ace422a91f814389dfbc2a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d995ac22361d4ec9a09b430dd52fcb9e","placeholder":"​","style":"IPY_MODEL_11c44a07fae449fba3a73a4e72747f30","value":"modeling_deepseek.py: 100%"}},"8f5014f012654d29aa47b4983e4e09e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0429f68beba4b2cb3cf352ce32d68b8","max":78672,"min":0,"orientation":"horizontal","style":"IPY_MODEL_870aebdd5f674ab8a744c134fa259819","value":78672}},"5df681f467544406900146c830ffe038":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5bb0a58a6047d0a93cfa89d4e486d8","placeholder":"​","style":"IPY_MODEL_4e0235b2f8084137adcd41f914e27348","value":" 78.7k/78.7k [00:00&lt;00:00, 745kB/s]"}},"d13893ad015d4be5a82a2065844fe875":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d995ac22361d4ec9a09b430dd52fcb9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11c44a07fae449fba3a73a4e72747f30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0429f68beba4b2cb3cf352ce32d68b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"870aebdd5f674ab8a744c134fa259819":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5bb0a58a6047d0a93cfa89d4e486d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e0235b2f8084137adcd41f914e27348":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"422308152f334ccca1c5b561001ee1f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da9512195cd842d596066637e6fbfa71","IPY_MODEL_850811e03d574f8ea15cd0d6167d8a32","IPY_MODEL_ea92a5cacecf451b8a16485716f17f46"],"layout":"IPY_MODEL_c5ca54eebcdf4bda8c01b608ac93b66a"}},"da9512195cd842d596066637e6fbfa71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e32142035c6741e4b9aa60075c726753","placeholder":"​","style":"IPY_MODEL_0bd16ce6239c4130a62fa8a458d7ec37","value":"pytorch_model.bin: 100%"}},"850811e03d574f8ea15cd0d6167d8a32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d6182a638bd4d84b34efac2052cce5e","max":498627950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09c728f001624016b5e415ae5484e998","value":498627950}},"ea92a5cacecf451b8a16485716f17f46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bac6211d7534f23bf549e1e9c5591d8","placeholder":"​","style":"IPY_MODEL_255dd84f7f9f4b35844636840e8ec342","value":" 499M/499M [00:04&lt;00:00, 115MB/s]"}},"c5ca54eebcdf4bda8c01b608ac93b66a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e32142035c6741e4b9aa60075c726753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd16ce6239c4130a62fa8a458d7ec37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d6182a638bd4d84b34efac2052cce5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09c728f001624016b5e415ae5484e998":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4bac6211d7534f23bf549e1e9c5591d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"255dd84f7f9f4b35844636840e8ec342":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9f74d74999f48c2ab263d59a7a6c015":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_241ea13971f644279c89e552099e4b36","IPY_MODEL_e06e003c078e4d04b2307ad0673d07ae","IPY_MODEL_c3de7d5fbb0a4c3e9dbe61a0df5e769b"],"layout":"IPY_MODEL_5c363d8085dd48c1a58304154ba98302"}},"241ea13971f644279c89e552099e4b36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf9cf381f9634f158e9cc44219e79628","placeholder":"​","style":"IPY_MODEL_a4b287fbab8941b6bd342a6a657c0f16","value":"Map: 100%"}},"e06e003c078e4d04b2307ad0673d07ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92be6250d59d4245b55eaeaa16ac283c","max":30433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9413285dfb2b43c4b3525edfa8d73beb","value":30433}},"c3de7d5fbb0a4c3e9dbe61a0df5e769b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b19aef4e4a2f468aacf8ec97c8129d3c","placeholder":"​","style":"IPY_MODEL_3f1fd2cd4daf49c4ae22a5da5153e9c7","value":" 30433/30433 [02:40&lt;00:00, 320.62 examples/s]"}},"5c363d8085dd48c1a58304154ba98302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf9cf381f9634f158e9cc44219e79628":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4b287fbab8941b6bd342a6a657c0f16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92be6250d59d4245b55eaeaa16ac283c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9413285dfb2b43c4b3525edfa8d73beb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b19aef4e4a2f468aacf8ec97c8129d3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f1fd2cd4daf49c4ae22a5da5153e9c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b99bbb54f9da43238c6132e803d16e47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5388668109f40b0b8ab9ef3e869e579","IPY_MODEL_d9138abbae9d4d11abdd4ee10410f8d1","IPY_MODEL_bd0d2fd645e247b2a8923e4c5fa0b441"],"layout":"IPY_MODEL_a1ead30ee2b34740801f5014a69f8690"}},"b5388668109f40b0b8ab9ef3e869e579":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_172a52ad37cc4f43995ccd8dbdb74332","placeholder":"​","style":"IPY_MODEL_b8fbacce4b9443a0bec6626b477a0b7b","value":"Map: 100%"}},"d9138abbae9d4d11abdd4ee10410f8d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0a76983ccf9420d8d820331da7c98ed","max":7608,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9df10f746c2142eab01f804df3717c67","value":7608}},"bd0d2fd645e247b2a8923e4c5fa0b441":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d10160f4e1b9418b8f9e87f13523add5","placeholder":"​","style":"IPY_MODEL_8a05563af6f24b7c80f69a29b5895cfd","value":" 7608/7608 [00:28&lt;00:00, 289.50 examples/s]"}},"a1ead30ee2b34740801f5014a69f8690":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"172a52ad37cc4f43995ccd8dbdb74332":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8fbacce4b9443a0bec6626b477a0b7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0a76983ccf9420d8d820331da7c98ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9df10f746c2142eab01f804df3717c67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d10160f4e1b9418b8f9e87f13523add5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a05563af6f24b7c80f69a29b5895cfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}